{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bG_dSRHovsP"
      },
      "source": [
        "**Datathon Cajamar UniversityHack 2022**   \n",
        "**Universidad de Granada (UGR)**   \n",
        "Salvador Corts Sánchez   \n",
        "Blanca Domene López"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRp7gN9vGE6Q",
        "outputId": "da08bd12-f0da-4340-b4ad-c1f0dcb066fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Montar directorio Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S_nlHKKhcZA",
        "outputId": "af16713e-0b5d-4e60-ead6-ca5777521a0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Recolector de basura de python\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a utilizar una variable que apunte donde tenemos los archivos del problema."
      ],
      "metadata": {
        "id": "Gw6gHEbFkkth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypmBmjgjGknF"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH=\"/content/drive/MyDrive/Cajamar\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos importando el dataset y visualizando la información relativa al mismo, así como visualizar las primeras filas:"
      ],
      "metadata": {
        "id": "yw2M-Nzzk2si"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ICdCbT7lPZy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import dataset\n",
        "data = pd.read_csv(f\"{ROOT_PATH}/Modelar_UH2022.txt\",\n",
        "                   sep=\"|\",\n",
        "                   dtype={\n",
        "                       \"ID\": 'Int32',\n",
        "                       \"SAMPLETIME\": 'str',\n",
        "                       \"READINGINTEGER\": 'Int32',\n",
        "                       \"READINGTHOUSANDTH\": 'Int32',\n",
        "                       \"DELTAINTEGER\": 'Int32',\n",
        "                       \"DELTATHOUSANDTH\": 'Int32',\n",
        "                   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JZ-HrxkGcZ2s",
        "outputId": "99ff609c-f89e-4bb8-956a-4a84c51e858d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>READINGINTEGER</th>\n",
              "      <th>READINGTHOUSANDTH</th>\n",
              "      <th>DELTAINTEGER</th>\n",
              "      <th>DELTATHOUSANDTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 08:34:09</td>\n",
              "      <td>369320</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 17:34:10</td>\n",
              "      <td>369403</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 18:34:10</td>\n",
              "      <td>369403</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 04:34:10</td>\n",
              "      <td>369284</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 14:34:10</td>\n",
              "      <td>369356</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID           SAMPLETIME  READINGINTEGER  READINGTHOUSANDTH  DELTAINTEGER  \\\n",
              "0   0  2019-06-13 08:34:09          369320                  0            17   \n",
              "1   0  2019-06-13 17:34:10          369403                  0             2   \n",
              "2   0  2019-06-13 18:34:10          369403                  0             0   \n",
              "3   0  2019-06-13 04:34:10          369284                  0             1   \n",
              "4   0  2019-06-13 14:34:10          369356                  0            28   \n",
              "\n",
              "   DELTATHOUSANDTH  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4PMmcNccVUK",
        "outputId": "e8296ff9-23b8-4489-9020-9b562a8bbf38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21404828 entries, 0 to 21404827\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Dtype \n",
            "---  ------             ----- \n",
            " 0   ID                 Int32 \n",
            " 1   SAMPLETIME         object\n",
            " 2   READINGINTEGER     Int32 \n",
            " 3   READINGTHOUSANDTH  Int32 \n",
            " 4   DELTAINTEGER       Int32 \n",
            " 5   DELTATHOUSANDTH    Int32 \n",
            "dtypes: Int32(5), object(1)\n",
            "memory usage: 673.6+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N8zVTUWcXd1"
      },
      "source": [
        "Como podemos ver, la variable *SAMPLETIME* es de tipo *object*, en lugar de tipo *date*. Utilizamos pandas para parsearlo correctamente y darle el formato que queremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpHK8L3VNCfp"
      },
      "outputs": [],
      "source": [
        "data[\"SAMPLETIME\"] = pd.to_datetime(data[\"SAMPLETIME\"], format=\"%Y-%m-%d %H:%M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "h0aVILiSHeUu",
        "outputId": "552729c8-7a0a-4392-dd0c-43a0e83876d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>READINGINTEGER</th>\n",
              "      <th>READINGTHOUSANDTH</th>\n",
              "      <th>DELTAINTEGER</th>\n",
              "      <th>DELTATHOUSANDTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 08:34:09</td>\n",
              "      <td>369320</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 17:34:10</td>\n",
              "      <td>369403</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 18:34:10</td>\n",
              "      <td>369403</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 04:34:10</td>\n",
              "      <td>369284</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-06-13 14:34:10</td>\n",
              "      <td>369356</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID          SAMPLETIME  READINGINTEGER  READINGTHOUSANDTH  DELTAINTEGER  \\\n",
              "0   0 2019-06-13 08:34:09          369320                  0            17   \n",
              "1   0 2019-06-13 17:34:10          369403                  0             2   \n",
              "2   0 2019-06-13 18:34:10          369403                  0             0   \n",
              "3   0 2019-06-13 04:34:10          369284                  0             1   \n",
              "4   0 2019-06-13 14:34:10          369356                  0            28   \n",
              "\n",
              "   DELTATHOUSANDTH  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EVE1mTcH2yR",
        "outputId": "3405b9fe-ff56-4aa4-e464-2eecbefb699a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21404828 entries, 0 to 21404827\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Dtype         \n",
            "---  ------             -----         \n",
            " 0   ID                 Int32         \n",
            " 1   SAMPLETIME         datetime64[ns]\n",
            " 2   READINGINTEGER     Int32         \n",
            " 3   READINGTHOUSANDTH  Int32         \n",
            " 4   DELTAINTEGER       Int32         \n",
            " 5   DELTATHOUSANDTH    Int32         \n",
            "dtypes: Int32(5), datetime64[ns](1)\n",
            "memory usage: 673.6 MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "B1nahf5vH47N",
        "outputId": "b5465663-7eba-4485-cc7b-29a160e378bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>READINGINTEGER</th>\n",
              "      <th>READINGTHOUSANDTH</th>\n",
              "      <th>DELTAINTEGER</th>\n",
              "      <th>DELTATHOUSANDTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.140483e+07</td>\n",
              "      <td>2.140483e+07</td>\n",
              "      <td>2.126477e+07</td>\n",
              "      <td>2.140483e+07</td>\n",
              "      <td>2.126477e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.259029e+03</td>\n",
              "      <td>6.052178e+05</td>\n",
              "      <td>3.930737e+00</td>\n",
              "      <td>2.277017e+01</td>\n",
              "      <td>4.228367e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.334956e+02</td>\n",
              "      <td>2.857464e+06</td>\n",
              "      <td>1.496893e+01</td>\n",
              "      <td>1.516794e+03</td>\n",
              "      <td>1.541059e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-6.407800e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-5.307340e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.230000e+02</td>\n",
              "      <td>6.368800e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.255000e+03</td>\n",
              "      <td>1.831570e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.887000e+03</td>\n",
              "      <td>3.520500e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.756000e+03</td>\n",
              "      <td>5.108976e+07</td>\n",
              "      <td>9.900000e+01</td>\n",
              "      <td>9.499810e+05</td>\n",
              "      <td>9.900000e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID  READINGINTEGER  READINGTHOUSANDTH  DELTAINTEGER  \\\n",
              "count  2.140483e+07    2.140483e+07       2.126477e+07  2.140483e+07   \n",
              "mean   1.259029e+03    6.052178e+05       3.930737e+00  2.277017e+01   \n",
              "std    7.334956e+02    2.857464e+06       1.496893e+01  1.516794e+03   \n",
              "min    0.000000e+00   -6.407800e+04       0.000000e+00 -5.307340e+05   \n",
              "25%    6.230000e+02    6.368800e+04       0.000000e+00  0.000000e+00   \n",
              "50%    1.255000e+03    1.831570e+05       0.000000e+00  0.000000e+00   \n",
              "75%    1.887000e+03    3.520500e+05       0.000000e+00  9.000000e+00   \n",
              "max    2.756000e+03    5.108976e+07       9.900000e+01  9.499810e+05   \n",
              "\n",
              "       DELTATHOUSANDTH  \n",
              "count     2.126477e+07  \n",
              "mean      4.228367e+00  \n",
              "std       1.541059e+01  \n",
              "min       0.000000e+00  \n",
              "25%       0.000000e+00  \n",
              "50%       0.000000e+00  \n",
              "75%       0.000000e+00  \n",
              "max       9.900000e+01  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSkxlRygdsvy"
      },
      "source": [
        "A continuación, comprobamos la cantidad de valores NA por columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_d-HIbjdx3R",
        "outputId": "d0cc1cd6-5470-482b-97dc-b3662b056f44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                        0\n",
              "SAMPLETIME                0\n",
              "READINGINTEGER            0\n",
              "READINGTHOUSANDTH    140056\n",
              "DELTAINTEGER              0\n",
              "DELTATHOUSANDTH      140056\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUy9hPZelkK"
      },
      "source": [
        "Todos los valores NA se encuentran en las dos columnas de *thousandth*. Nuestro objetivo es predecir el consumo, por lo que dichas columnas no son *muy* significativas; además de que más del 75% de estas columnas es 0. Dicho esto, imputaremos estos valores como 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNGgC5Xfr_0",
        "outputId": "16083101-097c-46d1-b59d-63f53df9c3aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                   0\n",
              "SAMPLETIME           0\n",
              "READINGINTEGER       0\n",
              "READINGTHOUSANDTH    0\n",
              "DELTAINTEGER         0\n",
              "DELTATHOUSANDTH      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"READINGTHOUSANDTH\"].fillna(0, inplace=True)\n",
        "data[\"DELTATHOUSANDTH\"].fillna(0, inplace=True)\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJrF6eMtdyHr"
      },
      "source": [
        "Nuestra idea inicial es la de usar redes neuronales, que pueden manejar floats. Además, el significado de *integer* y *thousandth* está claramente ligado. Por tanto, crearemos dos nuevas columnas, que combinen dichos valores de enteros y milésimas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwz9gM5QOSo3"
      },
      "outputs": [],
      "source": [
        "def combine(a, b):\n",
        "    return np.float64(float(f\"{a}.{b}\"))\n",
        "\n",
        "data[\"READ\"] = data.apply(lambda row: combine(row[\"READINGINTEGER\"], row[\"READINGTHOUSANDTH\"]) , axis=1)\n",
        "data[\"DELTA\"] = data.apply(lambda row: combine(row[\"DELTAINTEGER\"], row[\"DELTATHOUSANDTH\"]) , axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TldeMtRHdSxC"
      },
      "source": [
        "Puesto que estamos ante un problema de series temporales, necesitamos que nuestras secuencias estén ordenadsa en el tiempo. Una secuencia se identifica por la columna *ID*, por lo que ordenaremos por dicho *ID*, así como por *SAMPLETIME*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgej199rLeIn"
      },
      "outputs": [],
      "source": [
        "data = data.sort_values([\"ID\", \"SAMPLETIME\"], ascending = (True, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xU5Wj7YNbyk",
        "outputId": "c36c0335-75f4-4a9b-be8c-ba6beb4e6c00"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>READINGINTEGER</th>\n",
              "      <th>READINGTHOUSANDTH</th>\n",
              "      <th>DELTAINTEGER</th>\n",
              "      <th>DELTATHOUSANDTH</th>\n",
              "      <th>READ</th>\n",
              "      <th>DELTA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 00:39:36</td>\n",
              "      <td>331710</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 01:39:36</td>\n",
              "      <td>331710</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 02:39:35</td>\n",
              "      <td>331710</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 03:39:35</td>\n",
              "      <td>331710</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 04:39:35</td>\n",
              "      <td>331710</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ID          SAMPLETIME  READINGINTEGER  READINGTHOUSANDTH  DELTAINTEGER  \\\n",
              "160   0 2019-02-01 00:39:36          331710                  0             0   \n",
              "152   0 2019-02-01 01:39:36          331710                  0             0   \n",
              "150   0 2019-02-01 02:39:35          331710                  0             0   \n",
              "156   0 2019-02-01 03:39:35          331710                  0             0   \n",
              "161   0 2019-02-01 04:39:35          331710                  0             0   \n",
              "\n",
              "     DELTATHOUSANDTH      READ  DELTA  \n",
              "160                0  331710.0    0.0  \n",
              "152                0  331710.0    0.0  \n",
              "150                0  331710.0    0.0  \n",
              "156                0  331710.0    0.0  \n",
              "161                0  331710.0    0.0  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hecho esto, procedemos a guardar nuestro progreso. Hemos realizado este paso varias veces a lo largo del script con la idea de dividir las diferentes partes del trabajo, muchas de las cuales necesitan varios minutos de cómputo, y no requerir volver a ejecutarlas al realizar modificaciones."
      ],
      "metadata": {
        "id": "f6TKssxbnZRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qIJVlecMoYR"
      },
      "outputs": [],
      "source": [
        "data.to_csv(f\"{ROOT_PATH}/Modelar_UH2022_preprocess.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeImm9ugRt8c",
        "outputId": "4b5a2f6a-fea3-4985-ccd8-45dd3919f407"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>READ</th>\n",
              "      <th>DELTA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 00:39:36</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 01:39:36</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 02:39:35</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 03:39:35</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-02-01 04:39:35</td>\n",
              "      <td>331710.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ID          SAMPLETIME      READ  DELTA\n",
              "160   0 2019-02-01 00:39:36  331710.0    0.0\n",
              "152   0 2019-02-01 01:39:36  331710.0    0.0\n",
              "150   0 2019-02-01 02:39:35  331710.0    0.0\n",
              "156   0 2019-02-01 03:39:35  331710.0    0.0\n",
              "161   0 2019-02-01 04:39:35  331710.0    0.0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.drop(['READINGINTEGER', 'READINGTHOUSANDTH', 'DELTAINTEGER', 'DELTATHOUSANDTH'], axis=1, inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ4m7qDSRwFW"
      },
      "outputs": [],
      "source": [
        "data.to_csv(f\"{ROOT_PATH}/Modelar_UH2022_preprocess_dropped.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez exportados los datos preprocesados, habiéndonos deshecho de las columnas que no nos sirven, procedemos a cargar dichos datos y darles el formato adecuado."
      ],
      "metadata": {
        "id": "PENg3QdUorbL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij6slGSKouVy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import dataset\n",
        "data = pd.read_csv(f\"{ROOT_PATH}/Modelar_UH2022_preprocess_dropped.txt\",\n",
        "                   sep=\",\",\n",
        "                   dtype={\n",
        "                       \"ID\": 'Int32',\n",
        "                       \"SAMPLETIME\": 'str',\n",
        "                       \"READ\": 'Float64',\n",
        "                       \"DELTA\": 'Float64',\n",
        "                   })\n",
        "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "data[\"SAMPLETIME\"] = pd.to_datetime(data[\"SAMPLETIME\"], format=\"%Y-%m-%d %H:%M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch4MIdO1FiDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "58586e01-e57d-441c-8fad-a10474b6bc77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 ID          READ         DELTA\n",
              "count  2.140483e+07  2.140483e+07  2.140483e+07\n",
              "mean   1.259029e+03  6.052179e+05  2.283618e+01\n",
              "std    7.334956e+02  2.857464e+06  1.516795e+03\n",
              "min    0.000000e+00 -6.407800e+04 -5.307340e+05\n",
              "25%    6.230000e+02  6.368800e+04  0.000000e+00\n",
              "50%    1.255000e+03  1.831570e+05  0.000000e+00\n",
              "75%    1.887000e+03  3.520500e+05  9.600000e+00\n",
              "max    2.756000e+03  5.108976e+07  9.499812e+05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2659e5ff-a86f-4b19-8f3b-b1fe95aba660\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>READ</th>\n",
              "      <th>DELTA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.140483e+07</td>\n",
              "      <td>2.140483e+07</td>\n",
              "      <td>2.140483e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.259029e+03</td>\n",
              "      <td>6.052179e+05</td>\n",
              "      <td>2.283618e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.334956e+02</td>\n",
              "      <td>2.857464e+06</td>\n",
              "      <td>1.516795e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-6.407800e+04</td>\n",
              "      <td>-5.307340e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.230000e+02</td>\n",
              "      <td>6.368800e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.255000e+03</td>\n",
              "      <td>1.831570e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.887000e+03</td>\n",
              "      <td>3.520500e+05</td>\n",
              "      <td>9.600000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.756000e+03</td>\n",
              "      <td>5.108976e+07</td>\n",
              "      <td>9.499812e+05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2659e5ff-a86f-4b19-8f3b-b1fe95aba660')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2659e5ff-a86f-4b19-8f3b-b1fe95aba660 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2659e5ff-a86f-4b19-8f3b-b1fe95aba660');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data.head()\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PTp15yyflarR",
        "outputId": "a32c2c19-4cc5-4a47-a4fd-c7f18c6c7174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ee7ebfca-613d-4c1b-b981-b8831b9297fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>READ</th>\n",
              "      <th>DELTA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21404823</th>\n",
              "      <td>2749</td>\n",
              "      <td>2019-10-11 07:27:14</td>\n",
              "      <td>164507.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21404824</th>\n",
              "      <td>2756</td>\n",
              "      <td>2019-04-06 04:50:17</td>\n",
              "      <td>349758.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21404825</th>\n",
              "      <td>2756</td>\n",
              "      <td>2019-04-06 05:50:17</td>\n",
              "      <td>349758.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21404826</th>\n",
              "      <td>2756</td>\n",
              "      <td>2019-04-06 06:50:17</td>\n",
              "      <td>349758.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21404827</th>\n",
              "      <td>2756</td>\n",
              "      <td>2019-04-06 07:50:17</td>\n",
              "      <td>349770.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee7ebfca-613d-4c1b-b981-b8831b9297fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee7ebfca-613d-4c1b-b981-b8831b9297fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee7ebfca-613d-4c1b-b981-b8831b9297fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            ID          SAMPLETIME      READ  DELTA\n",
              "21404823  2749 2019-10-11 07:27:14  164507.0    2.0\n",
              "21404824  2756 2019-04-06 04:50:17  349758.0    0.0\n",
              "21404825  2756 2019-04-06 05:50:17  349758.0    0.0\n",
              "21404826  2756 2019-04-06 06:50:17  349758.0    0.0\n",
              "21404827  2756 2019-04-06 07:50:17  349770.0   12.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sb\n",
        "\n",
        "sb.histplot(data, x=\"ID\")"
      ],
      "metadata": {
        "id": "F747nQ_kQee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1c2f1fd0-7b0a-4db8-f49a-3dd84830b890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f533cd7c090>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este formato comienza por la columna *SAMPLETIME*. Nuestras instancias representan medidas de los contadores en un año aproximadamente, en varias horas dentro de cada día. Procedemos a agregar el consumo por *ID* y por *SAMPLETIME*, para conseguir, por cada *ID*, el consumo diario."
      ],
      "metadata": {
        "id": "TQ1uc7Teo2Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate by day\n",
        "data_by_day = data.groupby([\"ID\", data[\"SAMPLETIME\"].dt.date]).agg({\n",
        "    \"READ\": 'last',\n",
        "    \"DELTA\": 'sum'\n",
        "})\n",
        "data_by_day"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4TZ3t4KFsglM",
        "outputId": "b08935c4-89e6-4a78-acee-1450c178a949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     READ  DELTA\n",
              "ID   SAMPLETIME                 \n",
              "0    2019-02-01  331953.0  243.0\n",
              "     2019-02-02  332189.0  236.0\n",
              "     2019-02-03  332524.0  335.0\n",
              "     2019-02-04  332776.0  252.0\n",
              "     2019-02-05  332996.0  220.0\n",
              "...                   ...    ...\n",
              "2748 2019-08-03  406953.0  313.0\n",
              "     2019-09-14  422515.0    8.0\n",
              "2749 2019-08-06  156799.0    0.0\n",
              "     2019-10-11  164507.0    2.0\n",
              "2756 2019-04-06  349770.0   12.0\n",
              "\n",
              "[890837 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7349a31d-43e2-44cd-b073-03e6676315bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>READ</th>\n",
              "      <th>DELTA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>2019-02-01</th>\n",
              "      <td>331953.0</td>\n",
              "      <td>243.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-02</th>\n",
              "      <td>332189.0</td>\n",
              "      <td>236.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-03</th>\n",
              "      <td>332524.0</td>\n",
              "      <td>335.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-04</th>\n",
              "      <td>332776.0</td>\n",
              "      <td>252.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-05</th>\n",
              "      <td>332996.0</td>\n",
              "      <td>220.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2748</th>\n",
              "      <th>2019-08-03</th>\n",
              "      <td>406953.0</td>\n",
              "      <td>313.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-14</th>\n",
              "      <td>422515.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2749</th>\n",
              "      <th>2019-08-06</th>\n",
              "      <td>156799.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-11</th>\n",
              "      <td>164507.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2756</th>\n",
              "      <th>2019-04-06</th>\n",
              "      <td>349770.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>890837 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7349a31d-43e2-44cd-b073-03e6676315bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7349a31d-43e2-44cd-b073-03e6676315bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7349a31d-43e2-44cd-b073-03e6676315bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hecho esto, realizamos una tabla, con *SAMPLETIME* como identificador de las filas, e *ID* de contador como nombres de las columnas. El contenido de esta tabla será el valor *DELTA*.\n",
        "\n",
        "Hemos elegido trabajar sólo con el valor *DELTA*, pues *DELTA* y *READ* no son variables independientes. "
      ],
      "metadata": {
        "id": "KUzCw_5zpky-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_by_day_pivot = data_by_day.pivot_table(index=\"SAMPLETIME\", columns=[\"ID\"], values=[\"DELTA\"], fill_value=0)"
      ],
      "metadata": {
        "id": "EKSgodVuvCkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_by_day_pivot.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JuVlma65_WEK",
        "outputId": "f551b3c4-e588-42c0-f237-1e52f7bafa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            DELTA                                                        \\\n",
              "ID           0     1    2      3       4       5       6      7    8      \n",
              "SAMPLETIME                                                                \n",
              "2019-02-01  243.0   8.0  0.0  492.0  247.56  442.98    80.0   45.0  4.0   \n",
              "2019-02-02  236.0  47.0  0.0  381.0  235.68     0.0    11.0   36.0  0.0   \n",
              "2019-02-03  335.0   6.0  0.0  313.0  254.35     0.0    69.0  426.0  0.0   \n",
              "2019-02-04  252.0  12.0  0.0  362.0   412.0     0.0  269.98  433.0  7.0   \n",
              "2019-02-05  220.0  44.0  0.0  380.0   269.0     0.0  226.96   63.0  0.0   \n",
              "\n",
              "                   ...                                                    \n",
              "ID           9     ... 2739 2742 2743 2744 2745 2746 2747 2748 2749 2756  \n",
              "SAMPLETIME         ...                                                    \n",
              "2019-02-01  109.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2019-02-02  305.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2019-02-03  205.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2019-02-04  287.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2019-02-05  150.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 2747 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad43d160-aa68-4c69-bc3a-d70cc9594f5a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"21\" halign=\"left\">DELTA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>2739</th>\n",
              "      <th>2742</th>\n",
              "      <th>2743</th>\n",
              "      <th>2744</th>\n",
              "      <th>2745</th>\n",
              "      <th>2746</th>\n",
              "      <th>2747</th>\n",
              "      <th>2748</th>\n",
              "      <th>2749</th>\n",
              "      <th>2756</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-02-01</th>\n",
              "      <td>243.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>492.0</td>\n",
              "      <td>247.56</td>\n",
              "      <td>442.98</td>\n",
              "      <td>80.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-02</th>\n",
              "      <td>236.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>381.0</td>\n",
              "      <td>235.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>305.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-03</th>\n",
              "      <td>335.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>313.0</td>\n",
              "      <td>254.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-04</th>\n",
              "      <td>252.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>362.0</td>\n",
              "      <td>412.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>269.98</td>\n",
              "      <td>433.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-05</th>\n",
              "      <td>220.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>226.96</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2747 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad43d160-aa68-4c69-bc3a-d70cc9594f5a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad43d160-aa68-4c69-bc3a-d70cc9594f5a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad43d160-aa68-4c69-bc3a-d70cc9594f5a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_by_day_pivot.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z20SG4xkC6P_",
        "outputId": "22aa8ec8-ab69-4ae2-8537-e1966b0d7fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiIndex([('DELTA',    0),\n",
              "            ('DELTA',    1),\n",
              "            ('DELTA',    2),\n",
              "            ('DELTA',    3),\n",
              "            ('DELTA',    4),\n",
              "            ('DELTA',    5),\n",
              "            ('DELTA',    6),\n",
              "            ('DELTA',    7),\n",
              "            ('DELTA',    8),\n",
              "            ('DELTA',    9),\n",
              "            ...\n",
              "            ('DELTA', 2739),\n",
              "            ('DELTA', 2742),\n",
              "            ('DELTA', 2743),\n",
              "            ('DELTA', 2744),\n",
              "            ('DELTA', 2745),\n",
              "            ('DELTA', 2746),\n",
              "            ('DELTA', 2747),\n",
              "            ('DELTA', 2748),\n",
              "            ('DELTA', 2749),\n",
              "            ('DELTA', 2756)],\n",
              "           names=[None, 'ID'], length=2747)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_by_day_pivot.columns = [str(s2) for (s1,s2) in data_by_day_pivot.columns.tolist()]\n",
        "data_by_day_pivot.reset_index(inplace=True)\n",
        "data_by_day_pivot.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "GKML04OSBk-t",
        "outputId": "18ecfc03-673a-40bb-e427-295825fcc2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SAMPLETIME      0     1    2      3       4       5       6      7    8  \\\n",
              "0  2019-02-01  243.0   8.0  0.0  492.0  247.56  442.98    80.0   45.0  4.0   \n",
              "1  2019-02-02  236.0  47.0  0.0  381.0  235.68     0.0    11.0   36.0  0.0   \n",
              "2  2019-02-03  335.0   6.0  0.0  313.0  254.35     0.0    69.0  426.0  0.0   \n",
              "3  2019-02-04  252.0  12.0  0.0  362.0   412.0     0.0  269.98  433.0  7.0   \n",
              "4  2019-02-05  220.0  44.0  0.0  380.0   269.0     0.0  226.96   63.0  0.0   \n",
              "\n",
              "   ...  2739  2742  2743  2744  2745  2746  2747  2748  2749  2756  \n",
              "0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "1  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "2  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "3  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "4  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 2748 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3da66242-7489-477d-a093-85a4d08ce160\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2739</th>\n",
              "      <th>2742</th>\n",
              "      <th>2743</th>\n",
              "      <th>2744</th>\n",
              "      <th>2745</th>\n",
              "      <th>2746</th>\n",
              "      <th>2747</th>\n",
              "      <th>2748</th>\n",
              "      <th>2749</th>\n",
              "      <th>2756</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-02-01</td>\n",
              "      <td>243.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>492.0</td>\n",
              "      <td>247.56</td>\n",
              "      <td>442.98</td>\n",
              "      <td>80.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-02-02</td>\n",
              "      <td>236.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>381.0</td>\n",
              "      <td>235.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-02-03</td>\n",
              "      <td>335.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>313.0</td>\n",
              "      <td>254.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-02-04</td>\n",
              "      <td>252.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>362.0</td>\n",
              "      <td>412.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>269.98</td>\n",
              "      <td>433.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>220.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>226.96</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2748 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3da66242-7489-477d-a093-85a4d08ce160')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3da66242-7489-477d-a093-85a4d08ce160 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3da66242-7489-477d-a093-85a4d08ce160');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_by_day_pivot.to_csv(f\"{ROOT_PATH}/Modelar_UH2022_preprocess_pivot.txt\", index=False)"
      ],
      "metadata": {
        "id": "IVkJNLyyv_lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head $ROOT_PATH/Modelar_UH2022_preprocess_pivot.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op7sRXBo7Y-5",
        "outputId": "8cb66873-78b4-45f5-e336-bc08d0a2088e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLETIME,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2739,2742,2743,2744,2745,2746,2747,2748,2749,2756\n",
            "2019-02-01,243.0,8.0,0.0,492.0,247.56,442.98,80.0,45.0,4.0,109.0,5.0,0.0,27.0,1.0,101.0,219.8,288.4,578.0,1054.0,377.0,5.0,0.0,123.0,0.0,129.0,351.2,0.0,1.0,47.0,2409.0,259.0,2.24,0.0,4137.04,0.0,0.0,4316.0,61.0,204.0,0.0,304.0,20.0,10892.970000000001,172.85,78.37,182.0,322.0,3571.0,159.31,4.0,180.0,377.0,223.0,8.98,0.0,297.0,216.0,442.0,270.63,61.0,275.0,298.56,0.0,0.0,207.6,59.0,4.2,0.0,568.0,172.0,52.0,1129.03,0.0,64.8,0.0,157.0,0.0,152.0,0.0,0.0,0.0,14.0,165.0,52.0,317.0,0.0,48.0,216.0,177.0,352.0,0.0,224.0,603.0,408.96000000000004,279.0,371.0,530.2,213.0,192.0,380.0,621.0,40.0,352.0,168.0,258.0,400.0,142.01,220.0,229.0,206.0,216.0,16490.0,133.99,0.0,141.0,126.0,0.0,0.0,488.0,67.97,1255.0,222.0,2355.0,0.0,457.0,566.0,113.0,227.0,284.0,362.0,141.08,61.0,398.0,777.0,636.77,0.0,0.0,86.0,0.0,196.0,52.0,146.0,25.099999999999998,48.0,57.0,0.0,328.0,258.0,1057.0,0.0,26.0,0.0,128.0,275.0,143.0,0.0,115.0,79.0,142.2,0.0,358.0,283.0,235.0,109.2,61.0,31.0,261.01,0.0,395.0,570.0,291.0,101.01,498.48,0.0,187.0,369.68,0.0,126.0,137.0,507.01,121.0,216.0,1237.0,472.0,272.2,0.0,401.0,291.0,0.0,299.5,335.95,590.0,0.0,0.0,359.98,413.0,268.0,24.0,120.0,0.0,213.0,143.0,74.66,0.0,111.29,389.0,0.0,115.0,390.0,0.0,365.96,203.0,211.98000000000002,288.0,241.0,51.0,120.02,115.0,22420.0,0.0,74.41,167.0,225.0,134.95,407.0,403.0,0.0,326.0,357.77,58.0,84.0,0.0,363.0,0.0,0.0,105.0,0.0,0.0,16.0,169.0,332.2,196.68,0.0,0.0,42.0,279.0,0.0,253.0,109320.0,100.0,91.24,226.0,168.0,24.0,733.04,203.6,53.0,407.0,450.0,134.01,113.17,114.01,72.0,93.0,163.34,0.0,0.0,0.0,675.0,74.6,160.0,138.2,0.0,69.0,112.97,190.05,173.01,65.0,565.02,0.0,249.0,60.45,364.74,0.0,0.0,46.0,1.0,0.0,167.0,226.0,98.0,835.0,61800.0,283.44,12.02,260.0,64.97999999999999,43.0,45.0,0.0,179.0,139.0,232.0,467.0,153.0,329.0,0.0,109.0,29.0,7.0,241.4,0.0,0.0,56.88,130.0,163.0,0.0,65.89,333.2,559.0,0.0,160.51999999999998,0.0,0.0,126.0,44.08,0.0,37.0,6.0,267.2,4333.0,355.6,160.92000000000002,0.0,1814.0,5241.0,46.0,180.96,102.0,307.0,159.0,0.0,375.01,0.0,87.0,0.0,188.6,0.0,192.0,332.0,0.0,231.2,324.88,325.3,0.0,4731.0,0.0,231.0,7.0,1011.0,0.0,0.0,43.14,148.0,144.0,692.0,33.66,97.35,122.0,227.0,0.0,0.0,46.0,0.0,22620.0,4430.0,0.0,42.0,131.0,21.0,0.0,132.0,0.0,186.2,186.0,182.0,179.0,125.0,0.0,361.4,224.0,79.0,523.0,103.0,0.0,399.0,158.0,0.0,29.0,457.0,695.0,290.0,623.0,0.0,0.0,325.0,192.99,393.8,288.0,211.0,134.04,0.0,109.0,335.0,0.0,250.67000000000002,27.0,349.08,0.0,254.0,0.0,623.0,192.0,242.0,31.200000000000003,6.0,0.0,0.0,288.4,0.0,560.0,140.0,183.0,23.0,29.0,77.0,1019.0,794.0,0.0,205.0,612.0,150.0,584.0,63.0,498.01,160.0,92.0,176.01,11.0,0.0,0.0,114.02,192.0,227.2,563.76,0.0,123.0,0.0,121.0,147.6,293.40999999999997,0.0,0.0,0.0,268.0,444.0,10.68,744.0,207.0,252.0,3.84,89.0,0.0,0.0,790.0,303.0,0.0,0.0,0.0,1.0,19.01,1.0,5.0,104.0,72.0,65.0,9.0,307.0,30.0,0.0,105.0,77.0,205.0,0.0,7.0,195.0,245.4,114.0,844.96,155.0,207.8,216.19,112.0,147.0,77.0,396.0,145.03,283.0,235.0,318.0,73.94999999999999,175.0,269.0,402.0,0.0,213.0,410.0,440.0,220.0,0.0,65.88,206.2,148.0,93.04,43.19,1302.7,0.0,507.0,305.0,0.0,241.0,479.0,429.38,392.0,32.0,335.0,138.0,1767.0,51.0,221.0,654.0,274.0,375.08,21.060000000000002,184.0,0.0,163.0,0.0,105.0,0.0,267.0,3.0,395.0,0.0,111.0,237.0,188.99,0.0,66.0,277.0,183.03,174.0,394.4,472.0,1089.0,0.0,1171.0,22.32,294.0,4.0,161.0,0.0,201.2,157.0,11.0,448.0,393.4,0.0,47.599999999999994,53.0,499.15999999999997,0.0,1283.2,267.19,0.0,502.0,79.0,192.0,501.43,80.0,62.36,461.0,4.0,137.0,129.0,0.0,131.0,128.0,53.0,324.0,248.0,148.0,105.0,85.0,383.0,0.0,282.0,334.0,265.0,165.4,128.64,0.0,22.0,300.0,447.4,340.46,120.0,133.0,214.04,51.0,516.0,357.0,0.0,238.57,0.0,0.0,0.0,0.0,1942.0,213.4,0.0,66.0,4.0,195.0,267.0,557.0,0.0,500.6,0.0,181.99,541.44,340.0,11.24,173.0,219.67000000000002,237.16,0.0,193.0,63.0,387.0,55.0,237.24,294.0,56.0,264.59999999999997,0.0,233.0,362.0,43889.98,0.0,333.0,0.0,346.0,294.0,0.0,342.0,50.72,188.0,203.0,211.0,13.0,0.0,194.0,11.0,0.0,87.0,0.0,131.0,210.97,16.0,220.0,4.0,0.0,308.0,0.0,547.0,318.0,245.0,179.0,211.0,102.0,526.0,350.8,256.0,502.0,1416.0,165.0,12.0,457.48,123.0,0.0,25.6,2935.0,0.0,0.0,166.0,103.6,238.0,0.0,0.0,231.0,283.0,0.0,436.0,128.0,47.0,14.08,323.0,62.0,265.0,289.0,1.0,6.0,97.44,222.0,0.0,0.0,0.0,359.0,0.0,230.02,117.0,268.0,296.0,213.0,421.0,47.0,0.0,220.0,424.0,322.0,644.0,27.0,838.0,149.0,252.35,0.0,87.0,170.0,29.990000000000002,603.0,141.0,186.54,427.83000000000004,0.0,280.0,303.0,548.0,527.0,372.0,218.0,427.6,0.0,364.0,217.0,0.0,332.0,651.02,388.0,540.0,10949.0,285.0,1.0,132.0,618.0,0.0,48.4,0.0,370.0,101.0,199.0,416.0,0.0,164.0,3016.0,384.0,361.0,191.0,154.0,163.34,172.0,78.0,-3.0,554.0,138.61,0.0,2451.0,0.0,192.0,579.0,295.0,52.0,512.0,20.0,93.0,834.0,52.0,0.0,0.0,474.0,177.6,380.0,27311.68,0.0,188.76,0.0,213.4,166.0,124.0,139.76,225.0,6.0,126.0,0.0,0.0,0.0,54.0,337.0,580.0,7.0,764.0,46.6,46.53,0.0,182.0,121.0,325.0,181.4,285.0,432.14,171.0,565.0,34.0,158.05,189.0,298.0,0.0,15.0,157.04000000000002,79.0,240.03000000000003,265.0,189.0,319.0,169.0,5.04,1631.99,325.05,13.0,364.0,382.83,65.0,214.20000000000002,140.03,299.0,318.0,314.0,184.0,0.0,165.0,409.01,84.08,0.0,165.0,195.0,181.44,94.0,0.0,308.08000000000004,261.0,0.0,158.0,0.0,328.0,0.0,0.0,73.0,622.4,265.0,1398.0,0.0,140.0,43.0,82.0,124.0,201.01999999999998,1.0,183.4,117.0,0.0,189.4,13.0,20.0,206.0,319.0,175.0,204.0,16.0,31.0,0.0,0.0,514.0,45.6,0.0,0.0,134.0,477.22,0.0,0.0,50.980000000000004,33.0,121.0,83.0,273.0,254.0,175.0,42.6,130.0,8.0,0.0,455.0,0.0,125.0,1.0,119.0,88.10000000000001,191.34,0.0,0.0,158.0,147.2,387.0,4.0,56.0,92.0,204.60000000000002,740.0,130.8,238.0,450.99,0.0,91.0,146.0,0.0,432.0,339.0,402.8,271.68,0.0,0.0,125.0,483.0,245.0,364.0,674.5500000000001,142.0,0.0,0.0,2769.0,393.59999999999997,310.0,101.0,117.0,0.0,111.0,129.0,153.02,0.0,0.0,14.0,16.0,0.0,168.0,161.16,167.0,168.48000000000002,44.0,942.0,210.0,0.0,438.0,0.0,206.0,5.0,222.0,549.0,0.0,0.0,107.0,7.0,47.0,333.0,114.0,311.0,90.0,589.0,0.0,197.0,68.0,16.0,81.0,102.99,37.0,0.0,0.0,268.0,189.0,227.0,0.0,471.33000000000004,0.0,176.0,354.15999999999997,0.0,77.0,0.0,523.04,165.0,9.0,45.0,52.0,176.0,295.6,7.0,5020.0,293.0,1201.6,390.0,5.04,161.0,233.01,0.0,578.4,0.0,51.12,241.0,290.0,0.0,0.0,229.0,240.0,69.4,70.0,41.0,75.0,151.0,321.0,800.0,44.4,188.0,174.0,214.0,34.0,0.0,0.0,413.0,18.02,30.439999999999998,29.92,0.0,146.0,583.0,0.0,37.0,486.0,185.98,110.0,50.0,99.0,4026.0,49.0,394.0,224.0,284.0,162.0,124.0,163.05,623.0,571.99,385.0,111.23,95.0,113.0,189.0,2.0,394.0,318.0,0.0,436.0,327.0,297.0,207.0,91.5,522.0,137.0,221.8,63.0,200.06,0.0,123.0,103.0,76.0,150.54,82.0,40.0,351.0,69.0,277.0,16.0,0.0,0.0,201.0,95.82,128.0,57.0,2498.0,277.0,131.0,77.99000000000001,17.0,1730.0,340.0,44.4,0.0,338.0,0.0,0.0,159.2,0.0,511.0,18000.0,0.0,88.0,217.0,235.0,0.0,258.0,0.0,458.8,0.0,36.0,269.98,0.0,322.08,1586.0,480.0,88.0,54.0,873.35,21.0,143.0,103.19999999999999,357.0,247.0,119.0,49.0,130.0,206.0,45.0,0.0,0.0,74.0,43.78,1424.0,0.0,145.0,169.0,70.0,0.0,22.2,0.0,68.0,0.0,81.0,298.0,104.0,535.58,0.0,207.78,140.0,197.0,267.0,173.16,0.0,227.0,11.0,154.0,221.0,97.0,649.0,148.0,365.0,24702.56,154.03,215.0,569.0,569.0,96.0,151.0,227.0,0.0,159.0,151.0,341.37,0.0,129.0,93.35,14619.94,924.46,400.0,4.0,299.0,0.0,477.55999999999995,78.64,0.0,372.0,384.0,212.0,351.0,113.0,367.0,1.0,0.0,220.0,1036.89,0.0,1223.39,28.0,146.0,243.06,57.0,104.0,305.05,0.0,198.0,180.0,6960.0,302.0,289.56,44.519999999999996,21.0,133.0,0.0,83.0,254.01,379.0,80.0,0.0,207.0,331.0,0.0,248.0,419.0,345.0,226.0,0.0,358.65999999999997,227.0,108.2,89.0,184.0,11.0,220.0,182.95,414.0,27.0,78.0,0.0,96.0,239.0,272.0,400.9,122.0,0.0,0.0,0.0,216.55,100.0,0.0,284.4,17.0,189.0,449.0,181.11,302.4,0.0,151.0,20.0,174.0,137.0,153.4,35.0,330.0,16.53,148.0,129.03,89.76,260.0,0.0,239.0,321.0,16.0,0.0,0.0,469.0,13.0,158.99,117.6,23.0,114.0,316.52,92.83999999999999,106.0,0.0,0.0,0.0,0.0,130.0,131.0,251.0,120.6,0.0,0.0,124.0,49.0,0.0,553.0,4572.96,22730.05,0.0,330.0,0.0,113.0,168.0,334.0,394.0,0.0,75.0,106.0,37.0,82.56,451.0,213.0,526.0,297.94,77.0,152.0,225.0,136.0,0.0,0.0,85.0,420.0,226.0,247.0,0.0,0.0,394.0,123.0,48.26,90.0,0.0,347.0,11.0,156.0,318.02,276.4,21.0,198.0,261.0,299.0,272.0,205.88,62.0,44.0,19.0,21.0,55.32,0.0,265.99,371.23,321.0,170.0,88.0,480.0,53.0,165.0,73.0,0.0,161.39,0.0,387.0,81.0,236.22000000000003,110.0,498.0,145.0,108.47,0.0,0.0,160.0,457.0,24.0,370.36,204.0,36020.0,0.0,971.0,206.0,454.94,322.0,591.0,0.0,181.94,0.0,240.0,0.0,3.0,0.0,0.0,0.0,14.0,166.0,1921.0,101.0,236.0,130.0,503.0,146.0,0.0,933.0,319.0,0.0,11340.0,0.0,0.0,0.0,15.0,0.0,354.0,373.0,75.67,0.0,292.2,217.0,272.06,23.0,217.0,272.0,230.0,36.0,209.0,89.4,504.0,0.0,300.65,0.0,604.0,271.0,331.85,209.0,189.0,0.0,399.0,327.0,237.0,425.6,149.03,7.64,204.0,21.0,129.0,98.0,691.66,66.0,176.8,241.49,189.0,149.0,447.0,104.0,0.0,0.0,144.04,188.0,114.0,193.0,169.0,56.39,377.0,398.38,727.03,0.0,27.0,238.96999999999997,163.21,0.0,427.86,779.0,329.0,23.0,83.0,509.56,0.0,486.0,213.0,146.0,117.0,79.0,96.0,968.2,104.0,138.0,337.0,91.0,312.0,622.82,120.0,112.0,0.0,117.0,383.0,120.0,261.92,0.0,146.6,0.0,192.01,118.67,176.0,113.0,293.0,258.0,10126.0,198.0,0.0,248.8,598.14,231.0,328.0,310.0,263.0,339.0,237.76999999999998,336.0,0.0,410.0,123.0,63.04,26.0,415.0,114.03999999999999,4.0,262.0,146.07999999999998,388.0,0.0,223.0,171.0,123.0,192.0,273.2,483.0,248.95,353.0,115.0,234.0,5.0,0.0,4.4,135.0,123.0,45.16,233.0,0.0,241.0,2106.0,263.0,45.8,155.0,11.0,116.0,25.0,72.0,65.97,247.0,533.6,382.8,92.8,54.0,39.0,153.0,240.0,33.0,0.0,146.0,196.0,0.0,271.0,208.36,18.95,944.0,463.0,542.8100000000001,181.0,159.88,46.019999999999996,15.94,323.0,0.0,54.0,0.0,218.2,302.21999999999997,0.0,0.0,550.0,44.010000000000005,150.8,348.0,356.0,0.0,89.0,80.0,461.8,175.79999999999998,46.0,214.0,152.6,0.0,217.0,1526.0,0.0,70.0,286.39,0.0,103.0,187.0,10.0,1.0,198.0,180.4,51.0,140.0,0.0,392.96,44.0,128.13,463.0,153.0,115.41999999999999,43.0,154.96,29540.0,304.0,226.0,0.0,270.0,0.0,545.0,427.0,0.0,0.0,0.0,453.0,290.0,3.44,0.0,0.0,425.0,677.0,115.0,0.0,115.0,297.77,130.0,0.0,1.0,88.0,104.6,44.0,0.0,0.0,70.0,354.0,0.0,49.0,0.0,102.97,0.0,0.0,190.0,21.0,304.0,0.0,135.0,254.0,275.36,170.0,0.0,44.6,260.0,187.0,224.04,121.0,428.0,0.0,0.0,173.0,59.99,89.0,280.8,0.0,0.0,107.0,568.0,45189.98,10.0,31.0,16.4,688.02,131.0,174.61,147.0,0.0,124.0,0.0,465.4,140.0,72.65,141.0,119.0,82.0,187.97,0.0,378.0,74.0,0.0,93.0,13730.0,44.0,56.0,0.0,0.0,464.56,33.4,77.0,464.72999999999996,0.0,212.0,419.0,3.0,61.0,132.47,0.0,0.0,0.0,0.0,165.14,350.0,7212.0,458.68,254.0,21.0,195.0,109.98,0.0,0.0,167.0,161.81,0.0,225.0,53.39,0.0,31084.0,497.0,40.0,4619.0,373.0,972.51,139.8,181.0,316.0,358.0,536.0,0.0,185.0,265.0,185.76,23.8,472.0,0.0,0.0,509.0,657.0,0.0,228.56,256.0,115.0,222.0,96.0,109.0,122.0,0.0,0.0,966.0,170.0,199.0,29.0,144.0,391.0,181.0,80.0,499.03,452.0,66.67,208.2,19.25,221.0,0.0,67.0,220.0,503.0,0.0,0.0,180.03,0.0,225.0,246.0,305.0,0.0,109.0,0.0,0.0,482.0,101.0,0.0,0.0,99.01,215.0,277.0,0.0,53.0,63.0,314.76,56.0,97.6,0.0,464.64,243.74,111.0,519.02,171.0,156.0,10.0,67.0,85.0,0.0,51.0,379.96000000000004,8.0,735.4,0.0,178.0,0.0,709.54,0.0,516.0,3236.34,272.0,347.0,4.0,32.0,70.10000000000001,0.0,0.0,107.0,171.0,0.0,0.0,13.0,11.4,0.0,217.2,221.0,0.0,195.0,317.0,5.0,335.0,34.0,184.0,359.0,418.0,109.0,350.21,112.0,165.0,474.0,39.0,0.99,76.0,434.0,372.0,106.35,0.0,166.0,134.8,64.0,101.0,133.0,113.0,114.97,511.0,119.0,133.0,99.0,18.0,135.0,160.35999999999999,1690.0,140.0,1941.0,393.52000000000004,76.0,172.0,321.0,320.0,0.0,172.0,86.2,193.0,270.0,4.0,244.0,13790.0,0.0,135.0,1.0,129.0,319.0,149.0,220.0,134.0,1337.0,0.0,145.0,87.0,52.0,0.0,0.0,363.0,315.0,73.0,0.0,0.0,27.0,336.98,98.0,2.0,45.0,282.04,219.34,0.0,0.0,0.0,0.0,27.0,154.0,0.0,19.200000000000003,83.76,361.0,40.0,317.0,3.0,234.0,121.0,0.0,115.0,195.0,8.0,0.0,0.0,82.0,0.0,27.09,63.0,85.0,595.0,0.0,171.0,100.0,189.99,50.0,6.0,32.0,128.8,60.0,171.0,20.0,772.0,223.0,2.0,430.0,309.23,435.72,230.0,251.0,83.0,1.0,123.0,0.0,547.0,203.0,0.0,98.0,214.0,0.0,7.0,367.0,152.64000000000001,308.0,46.0,0.0,2.0,0.0,446.0,143.01,61.0,295.40000000000003,0.0,89.0,59.08,5.0,228.4,59.0,136.0,361.0,1215.0,187.0,67.14,496.0,66.0,257.0,223.0,0.0,242.0,156.4,0.0,64.0,115.0,142.0,1739.96,0.0,0.0,315.68,3118.0,0.0,4.0,203.0,2.0,51.0,196.0,0.0,120.0,31.0,53.0,157.0,461.23,273.0,276.0,83.0,210.0,211.6,180.0,5920.0,163.0,0.0,100.0,155.6,97.0,656.96,101.0,8.0,395.0,221.0,0.0,0.0,196.0,238.0,84.2,384.63,117.0,259.0,183.0,95.75999999999999,93.0,499.0,316.0,252.0,0.0,248.0,187.0,144.0,0.0,40.0,373.0,0.0,262.0,920.0,263.0,0.0,403.6,162.0,0.0,462.03000000000003,308.96,288.52,91.0,376.32,37.0,12.0,17.0,0.0,1345.0,0.0,0.0,135.0,5.76,198.0,273.56,21350.0,0.0,287.0,504.0,169.2,145.98,330.52,69.0,0.0,207.0,425.0,15.0,160.0,196.0,506.0,148.0,131.03,64.0,151.84,603.0,0.0,231.72,26.0,408.0,3339.0,187.0,335.0,202.0,0.0,118.0,192.0,0.0,0.0,5.0,168.52,0.0,82.89,28.0,114.0,158.0,532.42,0.0,0.0,70.38,0.0,129.0,201.0,0.0,97.0,244.0,78.0,947.0,0.0,184.0,13.0,27.0,217.0,129.0,0.0,0.0,175.0,2.0,248.0,261.76,0.0,204.0,279.0,11.440000000000001,102.0,190.0,14.0,304.45,1120.0,345.0,158.98000000000002,21.0,139.0,34.0,0.0,516.0,87.0,107.0,956.0,220.0,135.35,39.0,81.0,240.0,0.0,276.0,391.9,122.0,6.59,566.0,309.0,130.16,359.0,0.0,250.66,45.0,258.0,304.08,229.0,203.0,186.0,0.0,0.0,0.0,83.0,354.0,84.0,0.0,172.0,0.0,173.0,152.0,170.0,0.0,95.0,197.0,81.84,374.0,294.37,0.0,32.04,74.0,434.0,428.0,0.0,0.0,58.0,0.0,0.0,273.0,257.0,38.0,457.0,0.0,149.0,0.0,261.2,177.0,248.0,218.60000000000002,167.0,455.0,352.9,1099.0,177.0,525.4,546.0,0.0,27.0,2572.0,31.0,382.0,0.0,163.48,569.0,193.0,305.28,184.0,0.0,291.0,0.0,103.0,42.42,429.0,124.0,574.0,361.0,75.0,0.0,0.0,0.0,104.0,66.0,449.0,54.0,219.81,216.0,444.44,190.0,484.0,0.0,147.73,110.0,0.0,372.3,0.0,266.36,559.0,256.0,157.4,379.16,309.0,238.0,334.0,133.0,16.02,43.36,44.0,311.99,15.0,173.0,375.0,380.0,5.0,146.0,354.02,314.87,546.99,136.0,0.0,183.06,292.0,34.0,595.73,100.0,228.0,0.0,80.0,46.03,259.0,72.0,122.93,0.0,0.0,63.0,145.0,0.0,22.0,145.59,166.0,438.0,374.0,79.0,0.0,617.0,612.0,0.0,0.0,540.0,232.0,132.88,548.0,332.0,250.0,62.0,518.0,55.0,188.0,58.0,0.0,135.0,341.0,40.0,67.60000000000001,701.0,495.03000000000003,0.0,1028.4,343.0,325.96000000000004,8.0,378.34000000000003,0.0,0.0,0.0,136.56,234.71999999999997,0.0,171.98000000000002,0.0,270.0,0.0,17.52,355.92,80.01,0.0,0.0,84.0,0.0,173.0,0.0,0.0,58.0,272.0,102.96000000000001,143.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,74.2,129.91,402.0,97.0,890.56,401.0,3.96,0.0,181.68,364.08,3.0,235.0,440.2,475.08,6.0,302.8,2147.0,8373.0,336.01,220.0,580.98,14390.45,0.0,2794.0,111.0,756.0,421.0,210.0,1149.0,101.52000000000001,46042.0,423.4,87.96000000000001,351.0,552.49,322.0,0.0,329.0,13685.56,328.79999999999995,9427.44,292.08,727.0,252.0,1331.0,194.0,2126.0,617.0,29.97,766.39,19.74,4049.0,282.0,405.48,0.0,378.0,0.0,369.0,0.0,4749.4,355.0,0.0,882.0,100.0,0.0,9.600000000000001,138.2,461.39,37320.0,2637.0,0.0,56.839999999999996,102.4,338.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-02,236.0,47.0,0.0,381.0,235.68,0.0,11.0,36.0,0.0,305.0,0.0,0.0,78.0,1.0,54.6,379.0,0.0,479.0,608.0,123.0,6.0,0.0,194.0,0.0,263.0,338.03,0.0,0.0,171.0,2.0,228.0,0.0,0.0,5359.0,0.0,0.0,765.0,0.0,172.0,0.0,340.0,16.0,953.0,180.56,73.28999999999999,308.0,70.0,5826.0,214.88,0.0,95.0,0.0,225.0,1.0,0.0,301.0,245.0,479.0,347.0,136.0,209.0,277.08,0.0,0.0,119.04,31.0,8.36,0.0,559.0,107.0,138.0,1507.07,0.0,86.4,0.0,191.0,0.0,319.0,0.0,0.0,7.0,83.12,193.0,94.0,277.0,0.0,3.0,379.0,164.0,491.0,0.0,195.0,453.0,512.72,236.8,388.0,11.0,84.0,233.0,192.0,683.0,57.0,711.0,457.0,269.0,129.0,150.36999999999998,206.0,434.0,1.0,0.0,7040.0,533.36,0.0,329.0,139.0,0.0,0.0,473.0,237.0,1079.2,257.0,0.0,0.0,49.0,516.83,28.0,433.0,237.0,309.0,445.59000000000003,155.0,797.0,138.0,577.44,0.0,0.0,112.0,0.0,364.0,120.0,196.0,92.39,75.0,74.0,0.0,441.0,226.0,928.0,0.0,63.6,1.0,166.0,227.0,117.0,0.0,301.0,205.0,359.6,0.0,255.0,216.0,146.0,76.0,61.0,14.0,242.97,0.0,464.0,388.0,79.0,92.66,828.53,0.0,158.0,519.5600000000001,0.0,105.0,192.0,4.0,103.0,241.0,122.0,442.0,104.0,0.0,192.0,357.0,0.0,634.69,539.1,531.0,0.0,0.0,153.0,406.0,220.0,15.0,100.0,0.0,259.0,1.0,0.33,0.0,27.69,178.0,0.0,122.0,75.0,0.0,341.0,75.59,43.0,252.0,292.0,81.67,322.0,57.0,18429.98,0.0,13.100000000000001,47.0,249.0,105.0,192.0,291.0,0.0,248.0,96.22,176.0,162.0,0.0,373.0,0.0,0.0,196.0,53.0,0.0,12.0,252.0,191.76,209.28000000000003,0.0,0.0,104.0,237.0,0.0,293.0,660.0,0.0,184.76999999999998,589.0600000000001,285.0,39.43,694.04,64.0,340.0,300.02,359.0,539.0,146.69,340.0,85.0,2.0,220.68,0.0,0.0,0.0,685.0,3.35,157.0,165.5,0.0,154.0,37.019999999999996,21.0,252.0,141.0,501.0,0.0,257.4,92.31,371.19,0.0,0.0,33.0,2.0,45.0,234.0,190.0,25.0,1174.0,32050.0,433.67,0.0,319.0,28.0,0.0,505.0,0.0,137.6,96.0,240.0,58.0,306.0,208.0,0.0,78.0,87.0,85.0,125.6,0.0,0.0,56.88,127.0,513.0,0.0,174.0,602.8,520.0,0.0,404.44,0.0,0.0,351.0,324.96,0.0,75.0,4.0,246.8,2.0,78.8,245.99,0.0,704.0,857.0,22.0,197.0,173.0,234.0,224.0,0.0,721.05,0.0,128.0,0.0,171.4,0.0,102.0,505.99,0.0,182.39999999999998,290.29,2187.0,0.0,433.0,0.0,48.8,0.0,996.0,0.0,0.0,38.44,334.0,99.0,369.0,22.68,154.99,134.0,486.0,0.0,0.0,35.0,7.0,20010.0,1930.0,0.0,0.0,42.0,34.0,0.0,182.0,1.0,296.4,415.0,70.0,571.21,43.0,0.0,354.0,221.0,152.0,643.0,207.98000000000002,1.0,819.0,147.0,0.0,2.0,223.0,365.0,225.0,570.0,0.0,0.0,90.0,536.04,289.0,209.0,77.0,4.94,0.0,359.0,423.0,0.0,293.52,4158.0,449.93,0.0,337.0,0.0,627.0,179.0,174.0,31.200000000000003,362.0,0.0,0.0,232.6,0.0,392.0,120.0,86.02,139.0,42.0,1.0,135.0,597.0,0.0,526.0,362.0,35.25,687.0,106.0,361.2,179.0,0.0,90.0,0.0,0.0,0.0,186.48,202.0,348.6,563.76,0.0,94.0,0.0,456.0,365.6,351.96,0.0,0.0,0.0,253.0,310.0,10.12,1.0,425.0,201.0,78.54,80.0,0.0,0.0,1910.0,115.0,0.0,0.0,0.0,0.0,22.0,9.0,0.0,250.0,43.0,1.0,31.0,245.98000000000002,323.0,0.0,75.0,69.0,208.0,0.0,29.64,224.0,334.6,155.0,326.0,290.01,425.09999999999997,119.03,215.0,233.0,98.0,320.0,353.0,304.0,272.98,236.0,4.8,397.0,566.0,345.0,0.0,316.08,0.0,749.0,205.0,0.0,161.01,194.8,328.02,362.0,173.0,1191.87,0.0,452.0,330.0,0.0,422.0,283.0,270.88,306.0,1.0,0.0,7.0,0.0,262.0,179.0,439.0,226.2,358.8,0.0,0.0,0.0,346.0,0.0,165.0,0.0,376.0,18.0,404.0,0.0,247.0,148.0,426.06,0.0,130.04000000000002,31.0,184.52,45.0,379.6,440.0,1202.0,0.0,1.0,3.7199999999999998,368.0,10.0,323.0,0.0,382.97,304.0,0.0,122.0,494.0,0.0,33.0,168.0,164.0,0.0,1571.8,195.39,0.0,549.0,5.0,133.0,829.6,258.0,48.589999999999996,355.2,0.0,214.0,0.0,0.0,245.0,165.0,39.0,399.2,35.0,216.0,88.0,223.0,135.0,0.0,214.02,361.0,378.0,271.20000000000005,107.99000000000001,139.0,20.0,393.0,339.6,90.96000000000001,47.0,72.0,226.01,29.0,375.68,151.0,0.0,47.97,0.0,0.0,0.0,0.0,0.0,267.0,0.0,144.0,1.0,263.0,176.4,353.0,0.0,551.88,0.0,0.0,300.99,392.0,79.74,30.0,250.0,109.44,0.0,330.0,0.0,150.0,3.0,313.56,145.0,51.0,211.96,0.0,432.0,334.0,3720.0,0.0,388.0,0.0,246.0,245.0,0.0,151.0,36.050000000000004,50.0,274.0,349.0,0.0,0.0,205.4,0.0,0.0,183.0,0.0,372.0,105.35000000000001,0.0,369.4,58.0,0.0,80.0,0.0,454.0,266.0,243.0,236.0,0.0,191.0,604.0,558.2,190.0,216.0,1385.0,312.0,12.0,448.56,266.0,0.0,65.0,100.0,21.0,0.0,246.0,277.4,182.0,16.0,0.0,354.0,80.0,0.0,369.0,210.0,70.0,0.88,197.0,168.0,132.0,337.02,0.0,0.0,109.95,551.0,0.0,0.0,0.0,1343.0,0.0,313.85,123.0,229.0,195.0,129.0,357.0,110.4,0.0,0.0,608.0,129.88,1.0,593.0,385.0,67.0,248.79999999999998,0.0,88.0,351.6,19.0,369.0,167.35,196.0,415.0,0.0,101.0,392.0,605.0,482.0,239.8,64.0,371.0,0.0,70.0,418.0,0.0,408.0,1412.2,214.0,303.0,2331.0,191.0,19.0,422.0,897.0,0.0,59.6,0.0,93.0,7.0,250.0,0.0,0.0,427.0,2031.0,964.0,249.0,85.0,73.0,115.0,278.0,0.0,0.0,203.0,47.28,0.0,223.0,0.0,621.0,530.0,237.0,37.0,754.0,19.0,124.0,5.0,0.0,0.0,0.0,486.0,211.4,431.48,33.410000000000004,0.0,207.2,0.0,327.98,234.0,60.0,397.18,117.0,0.0,92.0,120.0,0.0,0.0,193.0,264.0,456.0,6.0,621.0,5.4,26.46,0.0,170.0,118.0,276.0,213.60000000000002,67.0,240.4,116.0,313.0,12.0,8.0,241.0,383.0,0.0,0.0,272.0,125.0,415.0,360.0,288.0,296.0,88.0,3.97,1524.0,141.0,6.0,261.4,274.22,68.0,277.0,216.0,189.94,246.0,87.0,310.03000000000003,0.0,27.0,481.0,142.0,0.0,271.0,69.0,96.73,180.0,0.0,251.9,152.0,0.0,118.0,0.0,303.0,0.0,0.0,169.0,640.99,240.6,1176.0,0.0,79.0,46.0,0.0,192.0,333.03,11.0,184.0,135.66,0.0,380.6,0.0,302.02,214.0,240.0,437.0,325.0,0.0,63.0,1.0,85.0,499.0,1.4,0.0,0.0,89.0,444.8,0.0,0.0,72.0,246.0,78.0,30.0,145.0,100.0,179.0,10.0,0.0,24.0,0.0,520.0,0.0,156.0,9.0,191.0,339.90999999999997,306.8,0.0,0.0,92.0,190.0,318.02,0.0,72.0,0.0,245.52,445.0,40.2,197.0,473.58,0.0,2.0,330.01,0.0,644.0,234.0,59.8,247.04000000000002,0.0,0.0,140.0,185.0,540.0,302.0,833.8199999999999,206.97,0.0,0.0,780.0,393.59999999999997,293.0,164.0,86.0,0.0,2.0,542.96,93.89999999999999,0.0,4.0,63.0,0.0,0.0,346.0,216.51,294.0,0.53,160.0,43.0,150.0,0.0,182.0,0.0,527.0,4.99,172.0,0.0,0.0,0.0,80.0,9.0,52.0,252.0,218.0,331.0,191.0,379.02,0.0,422.0,0.0,26.0,98.0,0.0,24.8,0.0,0.0,179.0,360.0,116.0,0.0,495.27,0.0,211.0,240.79999999999998,62.0,55.0,0.0,471.0,419.0,41.0,83.23,21.0,244.0,51.0,7.0,250.0,467.0,214.8,69.0,4.0,162.0,360.0,0.0,2971.6,0.0,48.989999999999995,363.0,132.0,0.0,0.0,218.0,181.0,24.01,69.0,0.0,119.0,114.0,196.0,888.0,2.0,64.52,260.0,232.0,161.0,0.0,0.0,697.6,295.0,23.04,36.33,0.0,396.0,692.0,0.0,23.0,581.0,529.93,231.0,1.0,147.0,20.0,436.0,529.0,241.0,320.0,266.0,104.0,115.0,653.0,389.24,350.97,105.23,125.0,171.0,63.0,5.0,218.01,440.0,0.0,420.0,202.0,97.0,223.0,630.97,308.0,147.0,93.0,149.8,62.12,0.0,170.0,289.0,197.2,251.96,221.0,0.0,361.0,152.0,365.0,0.0,0.0,0.0,367.0,27.0,0.0,196.98000000000002,2646.0,173.0,340.0,125.76999999999998,36.0,2.0,457.0,134.98000000000002,0.0,326.0,0.0,0.0,191.8,0.0,360.0,11600.0,0.0,4.0,160.0,315.0,0.0,346.0,0.0,542.0,0.0,360.0,264.24,0.0,483.12,1347.0,95.0,52.0,74.0,745.69,26.0,343.0,98.89999999999999,89.0,255.0,135.0,131.0,54.0,133.0,0.0,0.0,0.0,83.03,6.0,592.0,0.0,166.2,270.0,41.0,0.0,51.2,0.0,40.0,0.0,92.0,277.66,183.0,234.58,0.0,0.0,196.0,280.0,232.0,155.21,0.0,272.0,74.0,84.0,777.4300000000001,0.0,554.0,341.0,285.0,22114.56,325.0,272.0,469.0,309.0,168.0,136.0,463.6,0.0,188.0,0.0,455.6,0.0,128.0,99.0,11840.01,2345.0,368.0,3.0,0.0,0.0,415.71,5.33,0.0,346.0,294.0,174.95000000000002,354.0,0.0,280.0,61.0,0.0,350.0,570.12,0.0,1154.58,342.0,219.0,170.0,117.0,14.0,349.96000000000004,0.0,434.0,227.0,681.67,544.0,259.66,43.51,304.0,88.0,0.0,92.0,1.0,199.0,69.0,0.0,212.0,298.2,0.0,194.0,500.0,296.99,31.0,0.0,257.33,299.0,101.0,18.0,275.0,-1.01,168.0,125.34,393.0,131.0,0.0,0.0,22.0,430.0,274.2,477.0,74.0,0.0,0.0,0.0,263.0,0.0,0.0,197.25,0.0,243.0,417.0,443.88,105.6,0.0,186.0,19.0,183.2,132.0,346.6,138.0,268.0,6.4399999999999995,15.0,28.0,12.58,124.0,0.0,196.0,309.0,6.0,9.0,0.0,3.0,8.0,40.019999999999996,67.4,94.0,128.4,217.70999999999998,157.84,304.0,0.0,0.0,0.0,0.0,304.0,204.0,256.0,232.4,0.0,0.0,75.0,78.0,0.0,146.98000000000002,4572.96,5420.0,1.0,291.0,0.0,184.0,110.0,445.6,193.0,254.07000000000002,69.84,336.0,70.0,57.8,94.0,239.0,540.4,274.0,194.0,225.0,268.0,302.0,0.0,0.0,30.0,277.62,6514.0,198.0,0.0,0.0,137.0,87.0,25.48,0.0,0.0,212.0,0.0,118.0,264.0,499.6,133.0,0.0,251.0,411.0,385.0,190.98,37.0,84.0,90.0,0.0,0.0,0.0,281.0,387.78000000000003,343.0,151.0,140.0,504.23,83.0,0.0,45.0,0.0,178.0,0.0,180.0,142.0,321.13,38.0,500.0,165.0,207.34,0.0,47.0,0.0,299.0,136.0,458.44,156.20000000000002,22730.0,0.0,975.0,423.0,207.36,593.0,764.0,0.0,107.00999999999999,0.0,161.0,0.0,2.0,0.0,0.0,0.0,64.0,209.0,1621.89,322.0,84.0,201.0,132.0,132.0,0.0,482.0,726.0,0.0,10772.0,0.0,0.0,0.0,0.0,0.0,410.0,347.0,34.8,0.0,157.0,343.0,439.18,26.0,1.66,410.0,380.4,86.0,100.0,354.0,0.0,0.0,333.84000000000003,0.0,172.0,208.0,212.0,274.0,283.0,3.0,349.0,377.0,281.55,416.99,244.8,12.72,192.0,0.0,12.0,212.0,251.32,11.0,139.62,303.39,3.0,416.0,394.0,0.0,0.0,0.0,159.0,318.0,51.0,398.0,0.0,9.76,0.0,360.96000000000004,443.0,0.0,46.0,239.8,139.8,0.0,231.0,715.0,418.01,25.0,132.99,523.0,0.0,506.96999999999997,110.64,186.04000000000002,180.0,128.0,0.0,1087.8,128.0,93.0,420.0,76.0,355.0,1010.94,0.0,303.0,0.0,100.6,486.0,15.0,117.44,0.0,163.4,403.69,123.0,189.57,167.4,132.0,388.0,369.0,0.0,334.0,0.0,71.67999999999999,689.3399999999999,159.92000000000002,157.0,477.52,156.0,370.0,174.69,253.0,0.0,304.0,213.0,0.0,70.0,175.0,248.01,0.0,198.0,103.88,393.0,0.0,220.0,196.0,223.0,0.0,217.14,340.0,478.86,174.0,324.0,177.0,0.0,0.0,5.6000000000000005,221.0,20.0,61.72,177.0,0.0,218.2,2053.4,323.0,68.2,173.0,0.0,86.0,107.0,1.0,9.98,307.03999999999996,408.4,570.0,348.15,2.0,0.0,166.0,416.0,71.0,0.0,201.0,35.0,0.0,324.0,227.27,17.56,1268.0,43.0,6.0,90.0,203.66,117.0,10.0,315.0,0.0,30.270000000000003,0.0,289.8,18.47,0.0,2.0,271.0,1.0,172.0,299.0,329.08,0.0,104.0,44.0,451.20000000000005,216.89999999999998,0.0,265.0,391.4,0.0,255.0,1715.0,0.0,56.0,213.60000000000002,0.0,99.0,147.0,193.66,21.0,255.0,105.6,83.0,322.0,0.0,287.15,30.0,184.51000000000002,373.85,165.0,292.99,0.0,186.96,51880.0,235.04,232.0,0.0,117.0,0.0,302.0,358.0,0.0,0.0,0.0,378.0,354.0,2.2,0.0,0.0,165.0,726.0,205.0,0.0,107.0,99.24,294.0,0.0,1.0,99.0,122.4,9.0,0.0,1.0,52.0,249.0,0.0,0.0,23.0,219.8,0.0,0.0,113.6,0.0,358.0,0.0,110.0,311.0,333.6,95.0,0.0,173.64,0.0,352.0,152.0,164.0,251.0,0.0,0.0,203.0,27.1,72.0,429.0,0.0,0.0,109.0,838.0,31770.0,11.11,155.0,2.6,489.0,323.0,198.8,208.0,0.0,62.0,0.0,124.6,401.0,45.17999999999999,0.0,82.0,56.0,234.0,0.0,362.0,44.0,0.0,187.0,14040.0,107.0,47.02,0.0,0.0,282.9,58.0,32.0,610.99,0.0,50.0,503.0,1.0,40.65,160.32,0.0,0.0,0.0,45.0,164.72,377.0,3940.0,402.39,161.0,25.0,-3.0,90.65,0.0,0.0,0.0,214.32,0.0,163.8,64.94,0.0,31494.0,838.0,60.0,4731.0,554.0,282.09999999999997,241.0,350.8,357.0,311.0,72.0,0.0,396.0,445.0,308.55,36.06,453.0,0.0,0.0,406.0,579.0,0.0,55.0,105.0,158.0,113.00999999999999,145.0,120.0,193.0,0.0,0.0,0.0,35.0,261.0,0.0,131.0,247.0,246.0,100.0,561.04,179.0,67.0,232.79000000000002,5.38,320.0,0.0,117.0,592.0,189.99,0.0,0.0,250.0,0.0,157.0,145.0,323.0,0.0,220.0,0.0,0.0,186.0,115.0,0.0,0.0,245.28000000000003,292.0,225.0,0.0,0.0,201.0,160.21,23.0,168.64,0.0,252.28,96.08000000000001,85.0,0.0,367.0,195.0,3.5,163.0,15.0,0.0,0.0,304.24,0.0,589.0,0.0,259.0,0.0,282.52,0.0,640.2,898.78,288.0,483.0,3.0,306.0,93.05000000000001,0.0,0.0,0.0,181.0,0.0,0.0,57.0,2.0,0.0,11.86,31.0,0.0,377.0,153.0,0.0,459.0,147.0,299.0,271.3,358.0,223.0,676.45,69.0,32.0,378.0,64.0,0.0,107.0,252.0,224.0,205.0,0.0,194.8,319.2,219.0,144.0,36.0,83.0,216.0,637.0,293.0,111.0,73.0,9.0,89.0,217.6,1162.0,92.0,3.4,303.64,0.0,81.0,341.0,673.8,0.0,0.0,107.48,430.0,362.0,0.0,228.4,16870.0,0.0,176.0,1.0,60.0,230.0,3.0,355.0,316.0,760.0,0.0,79.0,481.4,144.8,0.0,0.0,282.0,294.0,55.0,0.0,0.0,170.0,324.6,489.0,1.0,58.0,152.0,128.96,0.0,0.0,0.0,0.0,66.6,83.0,0.0,6.54,2.0,69.0,135.0,538.0,2.9499999999999997,0.0,11.0,0.0,64.0,128.0,7.4,0.0,0.0,0.0,0.0,37.0,242.0,0.0,394.0,0.0,238.0,182.0,242.0,56.0,0.0,27.0,4.18,128.0,86.0,110.0,693.0,386.0,0.0,0.0,215.0,256.59,398.0,99.0,164.0,2.0,348.0,0.0,421.0,262.0,0.0,145.0,256.0,0.0,0.0,459.0,131.39000000000001,183.0,1.0,0.0,0.0,0.0,37.0,192.0,94.0,894.97,0.0,80.0,30.73,159.0,567.6,0.0,261.0,0.0,1151.04,361.0,99.0,395.68,91.0,558.0,103.0,0.0,87.0,182.60000000000002,0.0,124.0,268.0,211.0,1527.2,0.0,15.0,371.73,3776.0,0.0,1.0,186.60000000000002,2.0,49.0,266.2,0.0,152.0,35.0,53.0,226.0,305.0,273.2,298.0,38.0,276.0,214.75,268.0,2780.0,185.0,0.0,326.0,358.0,101.0,689.96,53.0,36.0,198.0,134.0,0.0,0.0,199.0,182.0,106.0,282.0,125.0,337.0,168.0,218.24,21.02,349.0,427.0,286.0,0.0,52.0,126.0,187.0,0.0,29.0,262.0,63.839999999999996,237.0,433.0,280.36,0.0,492.8,213.0,0.0,318.0,331.01,447.36,144.0,334.95,7.0,0.0,52.0,0.0,362.0,0.0,0.0,171.0,2.16,80.0,305.76,2850.0,0.0,364.0,669.97,162.8,0.0,596.6800000000001,0.0,0.0,356.0,161.0,49.0,225.0,78.0,879.0,100.0,274.0,131.0,398.95000000000005,536.0,0.0,0.0,31.0,460.0,2543.0,24.0,397.0,254.0,0.0,328.0,222.0,0.0,0.0,6.0,144.44,0.0,47.0,90.0,275.0,240.0,309.3,0.0,0.0,141.60999999999999,0.0,226.0,1121.0,53.0,136.0,355.0,104.0,111.0,0.0,181.0,0.0,55.0,415.0,197.0,0.0,0.0,117.2,954.02,298.6,171.0,0.0,186.0,290.0,9.6,35.0,203.97,69.0,1027.51,1087.0,818.0,208.42,63.0,116.0,32.0,0.0,521.0,0.0,144.0,1822.0,367.0,0.0,64.0,53.0,159.0,0.0,139.99,313.08,66.0,8.2,748.0,460.0,207.05,396.0,0.0,391.31,7.0,299.0,148.53,269.0,222.8,172.0,0.0,0.0,0.0,87.0,545.6,232.0,0.0,147.0,0.0,118.0,294.0,93.0,0.0,31.0,593.54,51.550000000000004,229.0,339.21,0.0,0.0,199.0,495.0,527.0,0.0,0.0,284.42,0.0,0.0,283.0,0.0,94.0,442.0,0.0,56.0,16.4,89.8,353.0,435.0,1246.0,487.0,522.0,507.12,2149.96,270.0,276.6,392.0,0.0,22.0,1466.6399999999999,67.0,429.0,0.0,154.6,528.0,338.0,118.2,159.0,0.0,53.0,0.0,179.0,290.58,352.03,169.0,81.0,267.0,0.0,0.0,0.0,64.0,72.0,155.0,555.96,17.0,378.2,186.0,215.37,293.0,345.0,0.0,98.88,246.0,1.0,371.33,0.0,145.0,263.0,450.0,63.0,270.94,211.8,220.0,358.0,445.0,0.0,41.0,121.0,264.4,32.0,7.0200000000000005,610.0,367.0,4.0,0.0,624.0,259.68,125.0,407.0,0.0,175.28,225.0,0.0,355.0,80.0,165.0,0.0,211.0,112.99000000000001,220.0,186.0,74.12,0.0,0.0,17.0,471.0,0.0,10.2,151.92000000000002,186.0,355.0,483.98,216.0,0.0,370.0,1014.0,0.0,317.99,0.0,264.0,172.62,7.0,584.0,267.0,33.0,85.0,89.0,219.84,220.0,0.0,1.0,459.0,8.0,99.42,808.0,304.8,0.0,942.7,511.18,511.0,4.0,443.17,0.0,0.0,0.0,136.56,222.92,0.0,0.0,0.0,0.0,0.0,17.84,253.92,287.0,0.0,0.0,156.82,0.0,304.09999999999997,0.0,0.0,206.0,262.0,121.27,359.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,111.80000000000001,70.98,714.0,238.0,511.46,395.0,1.22,0.0,30.28,278.42,18.84,269.0,554.0,91.0,119.0,497.14,14.6,0.0,667.0,105.4,446.0,6333.58,0.0,3771.0,196.0,725.0,725.0,261.0,1841.2,99.29,11900.0,547.96,24.05,486.0,775.92,328.0,1073.0,344.0,9352.2,426.85,9737.39,292.08,458.0,190.0,461.0,266.0,3312.0,423.0,0.0,830.2600000000001,27.36,10777.0,453.0,425.51,0.0,358.0,0.0,4.0,272.0,4931.0,294.0,0.0,150.0,58.0,0.0,15.4,139.2,112.0,0.0,2624.0,0.0,138.12,493.6,0.0,0.0,52.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-03,335.0,6.0,0.0,313.0,254.35,0.0,69.0,426.0,0.0,205.0,0.0,0.0,106.0,12.0,189.4,354.2,26.64,301.0,503.0,181.0,11.0,0.0,90.0,0.0,116.0,471.0,0.0,95.0,236.0,2.0,188.0,142.99,0.0,5575.04,0.0,0.0,452.99,0.0,317.0,0.0,337.0,81.0,674.64,443.11999999999995,40.33,158.0,69.84,5730.0,407.11,0.0,0.0,0.0,122.0,0.0,0.0,226.0,243.0,513.0,159.51999999999998,129.0,206.0,272.68,0.0,0.0,259.5,0.0,9.44,0.0,661.0,465.0,112.0,1207.01,0.0,86.4,0.0,204.0,0.0,440.0,0.0,0.0,0.0,83.92,175.02,37.0,243.0,0.0,0.0,293.0,122.0,494.0,0.0,315.0,439.0,467.34,59.2,403.0,8.0,258.0,320.0,230.0,482.0,0.0,383.0,67.0,249.0,214.0,239.66,239.0,317.0,45.0,35.0,5760.0,180.99,0.0,135.0,163.0,0.0,0.0,467.0,320.0,490.8,437.0,0.0,0.0,0.0,787.01,354.0,197.0,272.0,203.0,245.67,93.0,393.0,289.0,567.99,0.0,0.0,69.0,0.0,397.0,486.0,318.0,45.480000000000004,47.0,322.0,0.0,341.02,201.0,802.0,0.0,64.4,0.0,235.12,407.0,21.0,0.0,90.0,175.0,403.8,0.0,251.0,567.0,257.0,123.98,159.0,362.0,163.20999999999998,0.0,422.0,425.0,136.0,340.62,868.04,0.0,136.0,665.52,0.0,283.89,155.0,3.0,357.0,270.0,166.0,941.0,393.0,0.0,364.0,333.0,0.0,167.34,500.0,467.0,0.0,0.0,187.0,3.0,116.0,17.0,38.06,0.0,429.0,7.81,0.0,0.0,17.0,379.0,0.0,227.0,291.0,0.0,370.85,7.38,12.99,319.0,270.0,50.36,204.0,365.0,26580.0,0.0,0.0,47.0,220.0,377.96000000000004,343.0,320.0,0.0,113.0,136.0,129.0,134.0,0.0,397.0,0.0,0.0,154.0,0.0,0.0,11.0,10.0,154.99,209.28000000000003,0.0,0.0,99.0,250.0,0.0,243.0,710.0,0.0,193.0,232.0,213.0,29.59,658.9200000000001,228.0,295.0,389.8,18.0,156.0,75.24,147.8,51.0,524.0,191.2,0.0,0.0,0.0,663.0,27.28,119.0,175.24,0.0,72.0,9.0,6.97,159.33,18.97,492.0,0.0,357.6,177.35,304.0,0.0,0.0,29.400000000000002,1.0,19.0,123.0,187.0,118.0,922.0,34030.0,444.0,0.0,222.0,0.0,0.0,495.0,0.0,105.6,101.0,211.0,25.0,230.0,302.0,0.0,209.0,52.0,6.0,256.02,0.0,0.0,53.89,59.0,496.0,0.0,115.0,555.0,636.0,0.0,190.0,0.0,0.0,287.0,309.09999999999997,7.0,113.0,3.0,441.0,3.0,68.64,200.0,7.0,617.0,820.0,733.0,469.0,230.0,245.0,129.0,0.0,539.0,0.0,115.0,0.0,196.89,0.0,14.0,320.22,0.0,182.39999999999998,343.92,431.0,0.0,40.0,0.0,99.2,0.0,964.0,0.0,0.0,7.5,453.0,70.0,357.0,4.6,188.85,207.0,716.0,0.0,0.0,72.0,1.0,6320.0,430.0,0.0,55.0,102.0,31.0,0.0,351.0,8.0,245.6,491.0,137.0,164.4,124.45,0.0,424.0,326.0,186.0,732.0,34.0,1.0,447.0,239.0,0.0,3.0,236.0,0.0,203.0,558.0,0.0,0.0,249.0,0.0,348.0,241.0,0.0,6.62,0.0,168.0,371.0,0.0,293.52,6667.0,318.0,0.0,313.0,0.0,658.0,236.0,135.0,3.9000000000000004,0.0,0.0,0.0,178.0,0.0,624.0,161.0,60.0,474.0,0.0,6.8,66.03,558.0,0.0,545.0,172.0,0.0,458.0,107.0,341.83,109.0,0.0,316.0,0.0,0.0,0.0,207.51999999999998,279.0,223.83,611.0699999999999,0.0,146.0,0.0,340.0,136.8,579.98,0.0,0.0,0.0,249.0,412.0,3.42,2.0,300.0,273.0,84.72,117.0,0.0,0.0,880.0,0.0,0.0,0.0,0.0,0.0,21.0,0.0,1.0,225.0,142.0,0.0,26.0,251.8,344.0,0.0,109.0,284.0,209.0,0.0,9.36,115.0,277.0,249.0,438.0,367.03999999999996,552.0,143.0,415.0,104.0,94.0,305.0,165.0,302.0,218.98000000000002,173.0,1.24,439.0,297.0,287.0,0.0,103.54,0.0,507.0,256.0,0.0,212.82,292.40000000000003,270.44,206.0,204.0,837.31,0.0,506.0,548.0,0.0,249.0,448.0,195.32999999999998,362.0,1.0,0.0,84.0,2.0,232.0,155.0,510.04999999999995,322.0,332.77,0.0,3.0,0.0,102.0,0.0,129.0,0.0,210.0,17.0,309.0,0.0,284.0,146.0,223.0,0.0,170.95,181.0,159.67000000000002,121.0,174.01999999999998,420.0,1236.0,0.0,2.0,0.0,305.0,10.0,226.0,0.0,371.0,395.0,0.0,42.0,505.0,0.0,425.0,98.0,343.0,0.0,1774.0,245.0,0.0,657.0,2.98,207.0,911.0,211.0,85.38,595.8,0.0,317.0,40.0,0.0,148.0,192.0,8.0,320.6,94.0,122.0,144.98,38.4,251.0,0.0,2.0,271.0,414.0,271.20000000000005,0.0,175.0,0.0,268.99,394.0,290.99,151.0,273.0,186.99,652.0,343.6,128.0,0.0,2.0,0.0,11.0,0.0,0.0,7.0,370.0,0.0,133.0,0.0,185.0,296.6,428.0,0.0,521.22,0.0,0.0,717.98,335.0,88.12,70.0,254.22,176.4,0.0,370.0,0.0,614.0,72.0,399.0,227.0,52.0,251.07999999999998,0.0,146.0,504.0,5250.0,0.0,338.0,0.0,303.0,265.0,0.0,260.0,33.13,42.0,278.0,425.0,0.0,0.0,50.6,0.0,0.0,136.0,0.0,327.0,482.58,0.0,411.6,3.99,0.0,196.0,0.0,746.0,143.0,330.0,324.0,166.0,125.0,620.6,572.0,561.0,262.0,1207.0,409.0,5.46,496.0,203.04,1.0,198.0,1.0,24.0,0.0,242.0,112.0,49.0,9.0,0.0,464.0,130.0,0.0,316.86,323.99,128.0,128.39999999999998,463.99,206.0,167.0,646.0,4.2,0.0,96.83,482.0,0.0,0.0,0.0,535.0,0.0,402.0,69.0,846.24,29.0,119.0,393.0,50.6,0.0,8.0,501.0,250.1,4.0,362.0,564.0,187.0,273.8,0.0,0.0,340.4,0.0,609.0,165.98,145.0,492.96,0.0,111.0,181.0,488.0,453.0,299.88,41.0,485.0,0.0,178.0,266.0,0.0,474.0,834.93,196.0,369.0,2149.0,137.0,6.0,229.03,745.0,0.0,254.0,0.0,195.0,0.0,160.0,59.0,0.0,186.0,872.0,737.0,488.0,8.0,181.0,327.0,259.0,0.0,0.0,0.0,92.65,0.0,383.0,0.0,1079.0,350.0,425.0,3.0,237.0,161.0,186.0,5.0,0.0,0.0,0.0,539.04,269.0,265.53,13867.240000000002,0.0,91.0,0.0,205.97,145.0,94.0,550.66,160.0,0.0,241.4,73.0,0.0,0.0,232.0,273.0,282.2,28.0,622.0,131.5,1.95,0.0,227.0,163.0,327.0,229.74,126.0,191.8,194.0,475.0,0.0,0.0,177.0,253.0,0.0,0.0,105.0,124.0,384.0,257.0,310.0,289.0,61.0,3.97,0.0,375.06,6.0,141.6,279.01,104.0,397.01,255.0,252.0,540.0,385.0,326.0,0.0,5.0,325.0,99.0,0.0,479.0,33.03,134.64000000000001,196.0,0.0,300.12,225.63,0.0,116.0,0.0,279.0,0.0,9.0,172.0,444.0,197.4,1188.0,0.0,89.0,107.77,0.0,112.0,383.0,14.0,166.0,145.93,0.0,289.0,0.0,127.99,435.0,247.0,424.0,173.0,38.0,59.0,0.0,172.0,569.0,0.0,0.0,0.0,0.0,516.64,0.0,0.0,142.0,243.11,317.0,81.0,401.0,64.0,165.0,41.0,0.0,4.0,0.0,515.0,0.0,392.0,0.0,183.0,151.0,273.2,0.0,0.0,282.0,303.0,308.0,0.0,32.0,0.0,244.99,28.0,86.0,196.0,459.8,0.0,0.0,51.0,0.0,454.0,490.0,124.91999999999999,329.39,0.0,0.0,151.0,230.0,183.0,492.0,1109.17,11.06,0.0,0.0,640.0,393.59999999999997,582.0,272.0,0.0,0.0,1.0,690.0,55.95,0.0,210.0,45.0,0.0,0.0,346.0,146.59,715.0,0.0,213.0,58.0,277.0,0.0,316.0,0.0,185.0,153.0,218.4,4.0,0.0,0.0,63.0,42.4,12.0,78.0,185.95999999999998,419.0,187.0,609.9499999999999,0.0,170.0,215.4,87.0,100.0,0.0,9.6,0.0,0.0,258.0,321.0,212.0,0.0,706.4,0.0,92.0,500.9,0.0,423.0,0.0,427.0,128.0,6.0,90.78,40.0,420.0,0.0,8.0,250.0,376.0,133.44,237.0,7.0200000000000005,266.0,352.0,0.0,529.0,0.0,14.0,187.0,141.0,0.0,0.0,140.0,215.0,33.0,209.0,7.0,202.0,241.0,262.0,883.0,10.0,89.44,274.0,373.0,45.0,0.0,0.0,477.4,18.95,23.04,41.519999999999996,0.0,354.0,865.01,0.0,28.01,782.0,388.0,307.0,2.0,263.0,46.0,142.0,157.0,211.0,223.0,420.8,197.0,202.0,348.0,396.85,480.6,124.0,121.0,170.0,81.0,10.0,258.0,336.0,0.0,749.0,102.0,61.0,174.0,582.0,351.0,226.0,88.77,19.2,99.71,0.0,264.0,168.0,257.8,238.0,206.0,0.0,53.0,27.0,525.0,0.0,0.0,0.0,490.57,135.0,0.0,71.77,3085.24,354.0,144.0,220.16,424.0,8.0,576.0,134.0,0.0,373.0,0.0,0.0,110.02,0.0,450.0,16351.12,0.0,17.0,208.0,346.0,0.0,374.0,0.0,278.0,1.0,50.0,267.72,0.0,483.12,1265.0,52.0,67.0,396.01,764.96,343.0,113.0,77.66,349.0,351.0,115.0,71.0,38.0,170.0,0.0,0.0,0.0,0.0,0.0,935.0,0.0,232.32,369.0,5.0,0.0,36.0,0.0,119.0,0.0,69.13,158.31,250.0,510.02,0.0,72.23,129.76,303.0,143.0,219.12,0.0,300.0,49.0,74.0,489.59999999999997,0.0,1024.0,348.0,281.0,22276.18,249.0,393.0,339.0,70.0,205.0,260.0,246.4,0.0,303.0,143.0,692.4,0.0,148.0,129.0,11650.01,1322.6,277.0,6.0,113.0,0.0,226.65,6.0,0.0,827.0,202.0,283.03000000000003,392.0,0.0,778.0,0.0,0.0,413.0,0.0,0.0,110.99000000000001,191.0,226.0,300.0,31.02,44.0,331.4,0.0,295.95,120.0,948.24,546.0,443.32,169.0,214.0,129.0,0.0,107.69,293.0,220.0,263.0,0.0,170.0,379.8,0.0,211.0,401.0,190.0,213.98,0.0,451.0,170.0,39.4,15.0,890.0,-1.01,235.04,106.67,455.0,164.0,0.0,0.0,231.0,299.0,365.8,365.02,65.0,0.0,0.0,0.0,193.98000000000002,0.0,0.0,299.56,0.0,103.0,360.0,195.95000000000002,190.0,0.0,239.98,18.0,189.8,166.0,432.0,98.0,483.0,4.0,16.0,194.0,4.0,166.0,0.0,113.0,211.0,0.0,0.0,0.0,3.0,31.0,138.45,185.0,1.0,215.6,246.75,242.95999999999998,113.0,0.0,70.0,0.0,0.0,84.0,0.0,307.0,252.0,0.0,0.0,0.0,24.0,0.0,309.02,5422.42,5620.03,0.0,582.0,0.0,154.0,108.0,182.4,316.0,214.0,129.13,456.0,108.6,61.57,144.0,215.0,581.0,233.0,154.0,192.0,286.0,575.96,0.0,0.0,65.0,157.38,222.0,286.0,0.0,0.0,202.0,151.0,55.120000000000005,0.0,0.0,175.0,-1.0,79.0,262.36,935.0,126.0,6.0,469.0,351.0,565.6,249.0,12.959999999999999,59.0,36.0,0.0,0.0,0.0,232.0,304.0,415.0,246.0,3.0,520.8000000000001,236.0,13.0,106.0,0.0,108.51,0.0,237.0,37.54,243.60999999999999,171.0,372.0,227.0,123.03,0.0,1748.0,0.0,289.0,43.0,431.6,235.8,45020.0,0.0,0.0,330.0,419.29,505.0,531.0,0.0,352.92,0.0,134.0,0.0,4.0,0.0,0.0,1.0,12.0,187.0,2279.12,237.0,81.0,221.0,378.0,193.0,0.0,460.0,113.0,0.0,3178.0,0.0,0.0,0.0,0.0,0.0,583.0,444.0,68.68,0.0,136.0,311.0,382.56,22.0,71.33,1483.0,607.62,88.0,86.0,197.0,0.0,0.0,576.46,0.0,221.0,615.0,4.0,131.0,512.0,0.0,322.2,285.0,312.23,334.0,294.27,10.6,200.0,0.0,53.0,239.0,346.0,94.0,73.0,212.0,1.0,99.4,668.0,0.0,0.0,0.0,170.03,328.0,46.0,251.8,0.0,6.16,0.0,262.32,428.48,0.0,0.0,277.8,120.1,0.0,301.6,623.0,334.0,29.6,168.04000000000002,642.0,0.0,562.0,99.32,214.0,216.0,133.0,0.0,1378.02,43.0,199.0,451.0,197.0,443.0,40.0,0.0,319.0,0.0,226.4,350.0,196.0,397.0,0.0,104.0,324.48,301.0,197.04000000000002,105.6,16.0,231.0,551.0,17.0,500.0,0.0,233.34,142.04,452.06,387.0,653.44,289.0,411.0,261.59,284.0,0.0,493.0,320.0,17.0,0.0,468.0,281.0,0.0,385.0,410.0,45.0,0.0,255.0,284.0,447.8,0.0,123.67,303.0,289.08,255.0,229.0,163.0,0.0,0.0,0.0,187.0,0.0,62.2,204.0,0.0,244.79999999999998,2126.6,406.0,80.0,176.97,0.0,111.0,143.0,59.0,7.96,225.0,789.0,133.0,180.03,2.0,0.0,103.0,355.0,0.0,0.0,270.0,0.0,0.0,243.0,289.23,17.76,1450.0,18.0,495.0,360.0,221.76,120.0,1298.03,212.0,0.0,106.63000000000001,0.0,228.0,0.0,0.0,1.0,426.03,1.0,352.0,454.0,260.88,0.0,253.0,55.0,650.92,180.9,8.0,107.0,206.8,0.0,239.0,1139.0,70.0,0.0,373.82,0.0,161.0,80.0,364.31,2.0,271.0,279.6,131.0,274.0,0.0,222.0,180.0,160.94,405.14,243.0,281.16,6.48,309.01,53730.0,306.14,396.0,0.0,258.0,0.0,591.0,316.0,0.0,0.0,0.0,433.0,240.0,1.8,0.0,0.0,114.0,912.0,212.0,0.0,210.0,27.0,243.0,0.0,0.0,154.0,64.0,0.0,0.0,0.0,0.0,135.0,0.0,0.0,0.0,153.6,0.0,0.0,174.4,0.0,212.0,0.0,120.25999999999999,246.0,318.09999999999997,39.0,0.0,145.32,0.0,222.0,122.8,150.0,259.0,0.0,0.0,331.0,86.96000000000001,73.0,358.6,0.0,0.0,164.0,741.0,31026.64,7.43,86.0,154.0,486.08,216.0,199.02,178.0,22.0,28.0,0.0,39.0,314.0,60.239999999999995,0.0,105.0,146.0,117.03999999999999,0.0,370.0,119.0,0.0,222.0,14150.0,119.0,107.0,0.0,1.0,269.3,38.94,225.0,274.0,0.0,88.0,1243.0,2.0,199.5,140.01,0.0,0.0,0.0,0.0,198.0,441.0,2250.0,406.44,360.0,32.0,18.0,39.34,0.0,0.0,148.0,79.0,0.0,257.2,26.04,0.0,31256.0,433.0,0.0,4484.0,508.01,563.04,116.0,78.2,406.0,255.0,157.4,0.0,270.0,368.0,291.6,153.16,456.0,0.0,0.0,754.0,507.0,0.0,2.0,452.0,266.0,62.0,94.97,0.0,409.0,0.0,0.0,9.0,81.0,370.0,5.0,165.0,0.0,176.0,59.0,621.0,182.0,110.0,391.03000000000003,0.0,223.0,0.0,9.0,228.0,186.0,0.0,0.0,424.96999999999997,0.0,400.0,124.0,323.0,0.0,89.0,0.0,0.0,144.44,55.160000000000004,0.0,0.0,228.84,224.0,241.0,0.0,245.0,523.0,156.72,12.0,53.55,0.0,191.48,213.60000000000002,223.0,0.0,183.0,90.0,0.0,56.0,159.0,0.0,215.0,311.76,0.0,53.0,0.0,386.0,0.0,0.0,0.0,627.8,143.0,234.0,551.0,21.0,95.0,204.01,0.0,0.0,0.0,304.0,0.0,0.0,129.0,1.99,0.0,0.99,130.0,0.0,304.0,109.0,21.0,285.0,130.0,355.92,296.31,325.0,252.0,401.12,17.2,38.0,318.0,150.0,0.0,296.0,559.0,279.0,57.88,0.0,297.23,135.0,229.0,311.0,161.0,292.0,132.0,512.0,431.0,124.0,195.0,32.0,47.0,349.0,0.0,69.03999999999999,4.6,370.89000000000004,94.0,324.0,324.4,397.2,0.0,47.0,85.86,330.0,225.0,0.0,159.6,5940.0,0.0,127.0,1.0,74.0,94.0,154.0,336.6,187.0,1648.0,0.0,168.0,413.6,61.2,0.0,0.0,403.0,323.0,108.0,0.0,0.0,33.0,217.16,128.0,1.0,215.0,323.0,0.0,0.0,0.0,0.0,7.0,55.4,57.0,0.0,105.82,55.080000000000005,693.0,68.0,692.0,1.99,0.0,0.0,0.0,49.0,380.0,7.6,0.0,0.0,22.8,0.0,55.8,52.0,0.0,294.0,0.0,22.0,222.0,241.42,156.0,0.0,0.0,2.8,122.0,73.0,288.0,597.0,456.0,0.0,193.0,287.0,310.34,285.0,195.0,94.0,236.0,157.0,0.0,344.0,305.0,0.0,176.2,168.0,0.0,0.0,345.0,244.38,187.0,0.0,0.0,0.0,0.0,0.0,376.99,157.0,612.4,0.0,145.0,15.120000000000001,38.0,410.0,0.0,211.0,0.0,1282.98,219.4,103.0,835.35,231.0,628.99,185.0,0.0,84.0,366.92,0.0,65.0,139.0,606.0,1005.86,0.0,0.0,358.59999999999997,11044.0,0.0,0.0,426.4,1.0,37.0,114.8,0.0,279.0,92.0,258.0,154.0,74.0,313.8,264.0,29.0,288.0,322.75,216.0,2680.0,137.0,0.0,47.62,166.0,133.0,163.0,163.0,3.0,432.0,216.0,0.0,0.0,69.0,272.0,178.0,431.02,146.0,394.0,246.0,177.0,28.0,336.0,396.0,359.0,0.0,11.0,144.0,312.0,0.0,37.0,337.0,137.18,442.0,778.0,500.67,0.0,342.55,278.0,0.0,170.0,74.8,364.0,450.0,426.75,5.0,0.0,180.0,0.0,304.0,0.0,0.0,45.42,5.76,348.0,305.76,3540.0,0.0,229.0,397.5,273.0,0.0,327.58,0.0,0.0,87.0,298.0,199.0,246.0,204.0,604.0,78.0,250.0,130.0,657.05,532.0,0.0,0.0,3.0,692.0,1446.0,9.0,304.0,295.0,0.0,91.0,226.0,0.0,0.0,5.0,179.2,0.0,198.0,129.0,80.0,266.0,542.36,0.0,7.0,118.0,0.0,193.0,362.99,2.0,97.0,310.0,402.0,104.0,0.0,150.2,0.0,0.0,250.0,310.0,0.0,0.0,69.8,768.9499999999999,307.4,167.0,0.0,115.0,187.0,10.81,111.0,270.0,43.0,828.6999999999999,1281.0,319.0,108.59,24.04,188.0,0.0,0.0,387.0,0.0,165.0,386.0,260.0,158.06,127.0,51.0,215.0,0.0,273.6,521.7,60.0,4.0,498.0,468.96999999999997,167.61,598.0,0.0,317.0,0.0,380.0,175.20000000000002,325.0,359.2,107.0,0.0,0.0,0.0,128.0,428.4,239.0,0.0,330.0,0.0,452.0,269.0,86.0,0.0,57.0,294.48,92.74,147.0,202.44,0.0,0.0,146.96,452.0,441.0,0.0,0.0,243.15,0.0,0.0,330.0,141.0,192.0,313.0,0.0,106.0,17.6,89.0,197.0,150.0,634.0,168.0,596.0,1149.0,2960.0,276.0,288.0,747.0,0.0,24.0,3823.3199999999997,135.0,555.0,0.0,228.0,494.0,110.0,404.36,110.0,0.0,93.0,0.0,107.0,102.6,404.0,110.0,244.0,315.0,22.0,0.0,0.0,115.0,97.0,226.4,499.76,2.0,222.0,174.82,515.68,239.0,512.0,0.0,274.6,166.0,1.0,149.02,0.0,135.96,0.0,271.0,61.0,567.37,264.18,226.0,278.0,214.0,0.0,34.08,160.0,252.0,47.0,0.0,565.0,268.0,19.0,0.0,594.0,259.68,377.0,626.0,0.0,180.0,207.03,0.0,258.98,77.0,350.2,0.0,170.0,130.96,217.0,191.0,144.96,0.0,7.56,13.0,246.0,0.0,17.8,284.29,108.0,300.0,442.0,211.0,0.0,445.0,482.0,0.0,280.0,0.0,235.0,147.39,106.0,580.0,231.0,95.0,111.0,64.0,219.84,27.0,0.0,99.0,270.0,0.0,151.57,952.0,331.20000000000005,0.0,681.1,716.64,635.0,0.0,287.14,0.0,0.0,0.0,111.79,419.77,0.0,13.02,0.0,1.0,43.64,0.0,202.56,378.14,0.0,0.0,212.64,1.0,452.59999999999997,0.0,0.0,277.0,324.0,441.28,321.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,279.0,155.0,563.0,71.0,881.0,466.0,1.0,0.0,0.0,413.1,49.55,232.0,483.0,46.0,199.0,667.0,20.400000000000002,21.0,542.0,88.6,262.54,2803.92,0.0,3218.04,278.0,872.0,731.0,309.0,1517.8,145.1,12380.0,738.67,30.92,597.0,577.41,333.0,0.0,559.0,20396.66,305.4,9500.1,292.08,348.0,447.0,348.0,1312.0,2130.0,551.0,0.0,629.24,16.72,10490.0,276.0,593.52,0.0,305.0,0.0,99.0,20.0,7456.0,561.0,0.0,276.0,65.0,0.0,21.6,330.6,260.0,0.0,2606.0,0.0,74.7,988.0,0.0,116.16,0.0,3.0,107.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-04,252.0,12.0,0.0,362.0,412.0,0.0,269.98,433.0,7.0,287.0,9.99,0.0,0.0,25.0,155.0,294.0,382.33,607.0,878.0,252.0,6.0,0.0,36.0,106.07,207.0,289.0,0.0,19.0,89.0,210.0,171.0,329.0,0.0,4255.0,0.0,0.0,4282.0,0.0,246.0,0.0,518.0,65.0,17656.41,249.14000000000001,63.0,129.0,81.14,3359.0,129.88,11.0,7.0,215.01,353.0,6.0,0.0,306.0,268.0,616.96,176.94,148.0,314.0,255.76,0.0,0.0,303.35999999999996,49.0,24.04,0.0,513.0,72.0,165.0,1097.0,0.0,61.74,0.0,127.0,0.0,82.0,258.09,0.0,0.0,7.0,183.0,12.0,252.0,0.0,26.64,352.0,145.0,485.0,0.0,212.0,474.0,491.02,266.0,352.0,639.0,285.0,94.0,329.0,459.69,0.0,408.0,124.0,290.0,351.0,186.02,152.0,215.0,220.0,182.0,5710.0,190.2,0.0,302.0,256.0,0.0,0.0,192.0,115.2,993.0,347.0,8151.0,37.0,783.0,711.76,41.0,449.0,245.0,295.0,137.77,62.0,261.0,603.0,790.4,0.0,0.0,165.0,0.0,225.0,104.0,440.0,15.030000000000001,47.0,289.0,0.0,552.0,363.0,9.0,0.0,115.8,0.0,96.89,337.0,56.0,0.0,18.0,114.0,35.6,0.0,150.0,248.0,217.0,90.12,60.0,73.0,221.78,186.0,310.0,322.0,229.0,160.69,1120.4,0.0,254.0,599.19,0.0,119.11999999999999,154.0,126.0,99.0,315.0,156.0,399.0,355.0,0.0,216.0,259.0,0.0,37.01,408.0,426.0,0.0,0.0,587.88,3.0,216.0,46.2,54.0,0.0,284.0,152.26,0.0,0.0,87.0,244.0,0.0,56.0,59.0,0.0,132.0,126.08,97.98,315.8,156.0,36.92,259.71999999999997,274.0,20690.0,0.0,371.0,146.0,214.0,87.99000000000001,256.0,373.0,0.0,47.0,190.01999999999998,152.0,26.0,0.0,438.0,0.0,0.0,195.0,8.0,0.0,16.0,67.0,217.0,200.56,0.0,0.0,49.0,191.0,6.0,147.0,17680.0,126.0,144.64,152.35,129.99,30.74,752.92,164.6,49.0,313.0,455.0,254.0,198.6,214.07999999999998,107.0,480.0,147.6,15.0,0.0,0.0,604.2,291.69,137.0,99.77,0.0,30.0,70.0,148.02,346.64,57.0,571.0,0.0,422.0,86.68,440.02,0.0,0.0,31.61,2.0,0.0,32.0,104.0,15.0,733.0,58570.0,303.5,4.0,587.0,60.0,10.0,260.6,0.0,293.8,95.0,195.0,214.0,171.0,122.0,0.0,144.0,188.0,1.0,227.0,0.0,0.0,28.15,137.0,43.0,0.0,59.0,351.0,413.0,0.0,171.0,0.0,0.0,198.0,324.0,0.0,88.0,4.0,342.0,2.0,320.42,252.4,91.0,1926.0,5138.0,36.0,127.0,56.0,284.0,125.0,20.0,519.0,0.0,70.0,0.0,321.12,0.0,34.0,396.8,0.0,227.6,361.04,733.06,0.0,7076.0,0.0,154.0,6.0,921.0,0.0,4.0,24.0,172.0,100.0,331.0,4.64,112.9,127.0,519.0,0.0,0.0,47.0,0.0,24610.0,4240.0,0.0,23.0,137.0,19.0,13.0,166.0,0.0,125.0,189.2,200.0,172.36,90.53,0.0,357.0,214.0,187.0,337.0,0.0,19.0,536.0,90.0,0.0,3.0,222.0,84.0,280.0,608.0,0.0,0.0,312.0,1263.01,374.0,209.0,0.0,112.41,0.0,66.0,394.0,0.0,395.05,171.0,418.0,0.0,121.0,0.0,517.0,264.0,284.0,210.04,0.0,0.0,0.0,150.01999999999998,0.0,369.0,106.0,150.0,153.0,42.0,104.2,753.0,795.0,0.0,407.0,357.0,0.0,445.0,109.0,458.0,118.0,115.0,113.0,0.0,0.0,0.0,223.0,271.0,176.54,406.44000000000005,0.0,150.0,0.0,406.0,1390.0,211.0,0.0,0.0,0.0,375.0,232.0,4.5600000000000005,864.0,561.0,212.0,10.47,154.0,121.0,0.0,830.0,48.0,0.0,0.0,0.0,7.0200000000000005,18.05,26.0,13.0,282.0,58.0,100.0,14.0,154.2,83.0,0.0,81.0,280.0,296.0,0.0,5.0,173.0,243.0,130.0,784.0,98.0,184.0,326.73,410.0,93.44,159.0,178.0,158.0,209.0,282.52,239.0,247.0,179.0,331.0,336.0,0.0,0.0,454.0,426.0,186.0,0.0,142.0,312.82,0.55,198.0,153.0,622.43,0.0,420.0,144.0,0.0,244.0,426.0,161.16,349.0,107.0,0.0,204.0,1777.0,68.03999999999999,217.0,471.2,311.8,299.0,26.0,118.0,0.0,101.0,0.0,40.0,0.0,206.0,0.0,315.0,0.0,100.0,199.0,359.0,0.0,136.58,521.0,105.78999999999999,152.0,406.0,463.0,0.0,0.0,394.0,45.05,630.0,152.0,267.0,0.0,266.2,264.0,25.0,96.0,113.0,0.0,26.0,64.0,492.0,0.0,1208.46,203.03000000000003,15.18,454.0,69.0,129.0,861.97,170.0,51.4,410.6,0.0,318.0,308.12,0.0,269.0,143.98,30.0,304.2,50.0,176.0,173.0,58.6,216.0,0.0,47.24,292.0,351.0,124.1,27.0,7.0,28.0,172.0,244.0,134.06,214.0,174.0,260.0,318.0,529.8,166.0,0.0,55.0,0.0,0.0,0.0,10.0,1547.0,211.0,0.0,126.0,47.0,105.0,394.0,280.0,0.0,534.76,0.0,209.01,489.0,312.0,111.0,110.0,248.78,248.0,0.0,284.0,27.0,285.0,77.0,376.0,216.0,51.0,382.04,0.0,93.0,292.0,8600.0,0.0,312.0,0.0,80.0,237.98000000000002,0.0,84.0,30.57,138.0,156.0,102.0,33.0,0.0,287.0,0.0,0.0,161.0,0.0,216.0,181.2,9.0,254.0,6.0,0.0,367.0,0.0,361.0,224.0,397.0,173.6,160.0,184.0,330.4,262.0,880.0,178.0,506.0,273.0,5.06,403.0,140.0,329.91,55.0,15.0,0.0,0.0,121.2,68.0,353.0,3.0,0.0,233.0,198.0,0.0,249.17000000000002,14.0,45.0,211.4,337.0,38.0,253.0,260.03999999999996,9.0,0.0,17.16,318.0,0.0,41.0,0.0,267.0,0.0,263.0,220.0,37.82,234.0,35.0,619.0,230.0,0.0,199.0,403.0,232.98000000000002,645.0,5.0,901.0,142.0,248.0,0.0,49.0,462.0,58.0,236.0,266.56,144.0,397.0,0.0,37.0,278.0,411.0,297.0,373.35,272.0,270.0,0.0,139.0,308.0,0.0,151.0,773.0,268.0,512.0,13832.0,213.0,1.0,263.0,779.0,0.0,75.0,0.0,79.0,22.0,131.0,207.0,0.0,250.0,3794.0,408.0,297.0,183.0,72.0,123.99000000000001,235.0,126.0,0.0,0.0,15.46,0.0,364.0,0.0,92.0,707.0,353.0,45.0,698.0,97.0,42.0,532.0,88.0,0.0,0.0,602.0,223.19,226.4,28381.940000000002,0.0,152.0,0.0,213.82999999999998,272.0,201.0,170.29,188.0,11.0,125.6,0.0,0.0,0.0,56.0,252.0,611.81,107.0,905.0,160.45,68.0,0.0,44.0,77.0,204.0,219.12,167.0,375.99,255.0,379.0,128.0,261.0,108.0,277.0,6.0,4.0,517.0,79.04,325.0,305.0,332.0,285.0,3.0,5.03,819.1,208.44,6.0,418.0,319.46,21.0,230.0,127.99,387.97,657.0,234.0,213.0,0.0,249.0,461.0,87.0,0.0,77.0,167.0,179.5,177.0,0.0,216.0,324.4,0.0,97.0,0.0,291.0,0.0,0.0,78.0,587.0,315.0,2010.0,0.0,60.0,13.6,138.0,144.0,470.0,18.0,298.0,194.4,0.0,200.0,0.0,125.0,306.08,256.0,294.0,163.0,8.0,93.0,0.0,0.0,410.0,11.049999999999999,0.0,0.0,167.0,593.18,0.0,0.0,43.0,5.84,210.0,0.0,331.0,103.0,230.0,118.0,166.0,36.0,0.0,783.0,0.0,189.0,0.0,83.0,146.64000000000001,272.0,0.0,0.0,242.0,89.0,242.0,2.0,26.0,45.0,135.98,6.0,143.0,317.0,283.03,0.0,5.0,145.92,0.0,437.0,216.0,442.43,226.06,0.0,4.0,106.0,210.0,205.0,563.5,1038.08,115.0,0.0,0.0,3228.0,421.4,312.0,155.0,81.0,0.0,112.0,171.8,101.08000000000001,0.0,34.97,55.0,6.0,0.0,252.0,250.0,392.06,0.0,117.0,1010.0,105.0,0.0,207.0,0.0,329.0,218.0,257.6,602.0,0.0,0.0,52.0,71.6,62.0,295.0,117.0,202.0,109.0,533.0,0.0,138.0,223.6,3.0,225.0,207.0,3.6,0.0,0.0,305.0,297.0,136.0,0.0,561.01,0.0,229.0,280.0,31.0,162.0,0.0,467.0,367.0,7.0,34.0,37.0,173.0,327.0,8.0,15440.0,308.0,713.54,413.0,2.04,183.0,383.0,14.0,333.0,0.0,96.05,278.0,122.0,0.0,0.0,315.0,118.0,70.02,71.0,62.0,120.0,135.0,262.0,742.0,77.0,118.0,258.0,177.0,22.0,0.0,0.0,414.8,41.0,93.4,50.4,0.0,253.0,802.0,0.0,96.95,496.0,431.0,759.0,46.0,125.0,4011.0,189.0,311.0,208.0,16.0,125.2,109.0,160.77,40.0,512.0,459.78,126.2,95.0,43.0,95.0,40.0,272.6,394.0,0.0,312.0,181.0,150.0,559.0,100.16,297.0,118.0,55.22,6.0,95.2,0.0,195.0,228.0,78.0,245.0,34.0,101.0,244.0,84.0,612.0,0.0,0.0,0.0,213.37,154.0,243.0,74.74,1344.78,447.0,150.0,133.0,32.04,1770.0,369.0,129.6,0.0,468.0,0.0,0.0,150.97,0.0,322.0,15348.82,0.0,58.4,226.0,259.0,0.0,169.0,0.0,314.4,0.0,250.0,195.0,0.0,1873.6599999999999,1348.0,85.0,61.0,142.05,868.0,168.2,117.0,84.72,355.0,243.0,112.0,76.0,104.0,111.0,59.0,0.0,0.0,148.0,45.0,442.0,0.0,218.24,324.0,58.0,0.0,95.0,0.0,90.0,0.0,115.42,50.68,236.0,357.0,0.0,153.65,274.2,255.0,274.0,239.60000000000002,0.0,205.0,27.0,0.0,489.59999999999997,108.0,306.0,183.0,241.0,24012.18,340.0,260.0,485.0,399.0,301.0,127.0,225.0,0.0,245.0,117.0,808.97,0.0,51.0,90.00999999999999,11900.01,22.4,262.0,6.0,280.0,0.0,226.74,125.0,0.0,290.0,210.03,126.96000000000001,312.0,302.03999999999996,567.0,1.0,0.0,191.0,0.0,0.0,1662.0,0.0,254.0,319.0,58.0,81.0,215.71,0.0,190.0,185.0,6975.55,476.0,296.0,209.01999999999998,49.0,98.0,0.0,57.28,242.0,356.0,122.0,257.0,347.0,110.0,0.0,247.0,477.0,495.0,260.0,0.0,287.44,156.0,61.599999999999994,71.0,350.0,-0.6,118.36,85.78,415.0,99.0,0.0,0.0,7.0,336.0,301.64,401.0,172.0,376.0,0.0,0.0,145.0,72.0,13.0,313.48,0.0,112.0,336.0,343.71,376.0,0.0,202.0,151.0,307.76,176.0,278.98,88.0,257.0,1.95,1.0,267.0,106.11,153.0,0.0,140.0,161.0,23.0,47.0,0.0,425.0,68.0,21.900000000000002,142.0,38.0,306.0,247.47,92.92999999999999,105.0,0.0,0.0,0.0,0.0,287.0,59.0,298.0,121.0,0.0,0.0,339.0,77.0,0.0,305.0,11219.91,27760.0,0.0,374.0,0.0,36.0,109.0,389.0,187.0,0.0,83.0,164.0,148.6,277.06,364.0,136.0,679.0,236.96,75.0,390.0,107.0,207.0,0.0,0.0,57.0,130.0,23637.67,132.0,0.0,0.0,304.0,151.0,62.21,43.0,0.0,242.0,0.0,119.0,324.64,95.2,14.0,77.38,175.0,261.0,233.4,70.0,34.0,29.0,9.0,31.0,86.0,0.0,252.04,271.02,143.0,137.0,67.0,436.0,50.0,357.0,63.0,92.0,132.5,0.0,378.01,151.89,188.68,180.0,656.0,224.0,77.18,0.0,1.0,306.0,280.0,49.92,539.06,149.0,45120.0,0.0,932.0,246.0,314.5,316.0,606.0,0.0,215.32,0.0,124.0,0.0,4.0,0.0,0.0,0.0,26.0,213.0,1239.0,180.0,246.0,365.0,185.0,222.0,0.0,874.0,374.98,0.0,8160.0,0.0,0.0,0.0,0.0,0.0,329.0,292.96,75.16,0.0,234.0,178.0,316.96,24.0,244.0,354.0,186.99,82.0,63.0,181.0,259.0,0.0,217.94,0.0,239.0,188.0,311.4,131.0,354.0,6.0,375.8,203.0,322.28000000000003,308.0,237.03,12.0,114.0,10.0,93.0,143.0,701.0,11.0,54.6,218.35,307.0,163.6,441.0,621.0,0.0,0.0,146.0,177.0,59.0,373.2,184.0,52.480000000000004,373.0,566.37,414.53,0.0,11.0,213.4,130.0,0.0,468.9,615.0,360.0,211.4,101.9,340.99,0.0,657.0,32.0,147.0,173.0,19.0,146.0,686.0,76.4,151.01999999999998,667.99,235.0,236.0,257.0,104.0,165.0,0.0,195.0,129.0,225.0,390.0,0.0,167.04,324.48,154.0,157.99,100.2,39.96,126.0,335.0,10101.0,408.0,0.0,204.0,326.59999999999997,287.03000000000003,119.97,439.6,555.0,333.0,222.7,159.0,0.0,518.0,136.0,54.94,0.0,206.0,229.0,0.0,158.0,76.0,320.0,0.0,254.0,168.0,280.2,190.0,283.77,360.0,235.0,154.0,119.0,467.0,6.0,0.0,32.0,224.0,159.0,47.980000000000004,218.0,0.0,229.78,1410.0,331.6,97.0,367.0,28.0,86.0,39.4,124.0,50.0,108.96000000000001,938.0,361.0,279.09999999999997,130.0,28.0,82.0,694.0,37.0,0.0,219.0,0.0,0.0,288.0,146.07,13.75,1552.0,569.0,276.22,121.99000000000001,236.84,26.0,22.0,221.0,0.0,25.44,0.0,204.0,0.0,0.0,0.0,459.99,64.0,240.0,440.0,314.0,0.0,211.0,111.0,506.93,181.95000000000002,50.0,136.0,170.20000000000002,0.0,221.0,1201.0,0.0,66.0,363.27,0.0,116.0,286.0,248.88,1.66,133.0,235.42,128.0,218.0,0.0,163.5,278.0,107.47,424.0,288.0,380.92,74.52000000000001,179.68,20790.0,473.83,177.0,1.0,288.0,0.0,391.0,515.0,0.0,0.0,0.0,506.0,137.0,7.0,0.0,0.0,338.0,883.0,143.0,0.0,99.0,76.85,111.0,7.0,10.0,363.0,41.0,53.96,0.0,0.0,44.0,4.0,0.0,43.0,0.0,110.06,0.0,0.0,180.0,0.0,304.0,0.0,274.77,329.0,406.84999999999997,103.0,0.0,44.96,0.0,183.0,215.23999999999998,106.0,326.0,0.0,0.0,181.0,37.0,49.0,398.0,0.0,0.0,109.0,929.0,43247.33,11.4,183.0,33.32,696.0,232.0,254.03,260.0,35.0,62.0,0.0,360.01,384.0,36.53,142.0,90.0,64.0,0.0,0.0,305.0,25.0,0.0,242.0,13700.0,164.0,38.04,0.0,0.0,479.85,195.0,73.0,527.15,0.0,112.0,765.0,87.0,54.96,76.98,0.0,0.0,0.0,0.0,177.04,278.0,14486.0,361.62,255.0,104.0,273.0,79.95,0.0,0.0,243.0,86.0,0.0,29.0,18.0,0.0,32078.0,356.0,22.0,4867.0,450.01,351.5,1510.0,140.0,281.0,118.0,160.6,0.0,191.0,322.0,263.25,234.0,91.0,0.0,0.0,818.0,442.0,0.0,1208.0,261.0,192.0,109.0,94.0,0.0,642.0,0.0,0.0,963.0,36.0,186.0,0.0,93.0,294.0,123.0,51.0,1368.0,386.0,52.0,214.20000000000002,18.0,374.2,0.0,99.0,205.0,88.0,0.0,0.0,298.98,0.0,627.0,220.4,603.0,0.0,173.0,0.0,0.0,490.52,112.9,0.0,0.0,92.55,262.0,139.0,0.0,409.0,165.0,158.0,36.0,139.6,0.0,359.0,213.60000000000002,123.0,537.0,140.0,107.0,2.97,243.0,234.0,0.0,159.0,197.0,14.0,374.0,0.0,308.0,0.0,802.34,0.0,555.0,1335.98,193.0,361.0,3.0,116.0,110.9,0.0,0.0,85.66,263.0,0.0,0.0,73.0,14.4,0.0,199.07000000000002,189.0,0.0,154.0,273.0,0.0,287.0,48.0,248.05,214.4,428.0,278.0,413.86,58.65,0.0,303.0,43.01,0.0,27.0,441.0,511.0,77.13,0.0,155.97,55.0,243.0,39.0,159.0,41.0,90.6,611.0,80.0,340.0,100.0,70.0,130.0,128.0,678.0,295.0,1664.0,412.04,0.0,107.0,355.6,438.01,0.0,212.0,89.0,196.0,216.0,5.0,217.0,14890.0,0.0,173.0,1.0,57.99,192.0,90.0,392.4,227.0,702.0,0.0,236.0,156.0,100.0,0.0,0.0,178.0,334.0,92.0,102.0,123.0,98.0,270.2,323.0,1.0,41.0,386.0,107.6,0.0,0.0,0.0,0.0,88.0,438.01,0.0,5.79,64.80000000000001,542.0,51.980000000000004,363.0,2.0,0.0,34.0,0.0,102.0,196.0,6.0,0.0,0.0,46.2,0.0,18.5,49.0,0.0,462.0,0.0,235.0,85.0,266.59000000000003,96.0,5.0,49.0,91.8,90.0,375.0,16.0,532.0,180.0,1.0,414.96999999999997,289.67,353.68,285.0,219.0,153.0,123.0,204.2,0.0,542.0,307.0,0.0,123.78,214.0,0.0,7.0,401.0,152.63,224.0,1.0,0.0,2.0,0.0,275.0,391.06,78.0,368.37,0.0,143.0,15.120000000000001,242.0,289.88,134.0,605.0,0.0,921.0,375.6,96.0,547.0,174.0,235.0,88.0,0.0,138.0,264.12,0.0,38.980000000000004,102.0,332.0,1781.01,0.0,0.0,490.0,4842.0,0.0,67.0,322.0,1.6,62.0,295.0,0.0,142.0,25.0,46.0,129.0,0.0,381.0,181.0,63.0,162.0,194.6,306.0,5710.0,209.0,0.0,86.39,2.99,171.0,511.0,112.0,25.0,397.0,154.0,0.0,0.0,232.0,203.0,16.0,359.0,302.0,283.0,297.0,126.0,114.0,699.0,330.0,994.0,0.0,192.0,173.0,163.0,13.0,350.0,344.0,0.0,314.0,910.0,212.16,0.0,687.78,204.0,0.0,311.0,45.199999999999996,216.54,246.0,258.0,5.0,31.0,142.0,0.0,1456.0,0.0,0.0,43.58,2.21,32.0,382.5,23810.0,0.0,256.0,797.4100000000001,366.0,302.79,405.22,68.0,0.0,117.0,422.0,32.0,100.0,200.0,1070.0,291.0,185.0,228.0,253.4,457.0,0.0,169.97,26.0,525.0,3219.0,54.0,421.0,181.0,0.0,61.0,230.0,0.0,0.0,7.0,246.83,20.0,66.03999999999999,149.0,159.0,249.0,301.03,0.0,0.0,92.35000000000001,0.0,292.02,399.97,0.0,321.0,357.0,115.0,1022.0,0.0,105.8,20.0,0.0,248.0,74.0,0.0,0.0,238.0,0.0,271.0,258.2,0.0,0.0,267.96999999999997,13.58,131.0,218.96,4.0,479.10999999999996,1157.0,327.0,332.0,21.919999999999998,42.03,56.0,0.0,548.0,286.0,83.0,566.0,322.0,302.73999999999995,69.0,37.0,129.0,0.0,450.52,128.41,64.0,20.0,520.0,422.0,94.0,779.0,0.0,143.0,0.0,181.0,209.04000000000002,283.0,229.0,158.0,0.0,0.0,0.0,110.0,560.0,159.0,0.0,373.0,0.0,115.0,87.0,260.0,0.0,76.0,437.0,50.97,208.0,104.32000000000001,1.0,11.0,125.85,223.0,660.0,0.0,0.0,237.4,0.0,0.0,221.6,172.0,77.0,488.0,0.0,147.0,1.0,36.04,336.0,217.0,554.0,213.0,601.8,570.0,2926.0,109.0,506.0,320.0,0.0,35.0,19640.0,24.0,244.0,0.0,195.56,479.0,281.0,316.0,208.0,0.0,131.0,0.0,67.0,117.35999999999999,126.0,225.0,171.0,237.0,251.0,0.0,10.66,109.0,123.0,124.39999999999999,562.22,3.0,95.0,126.64,255.0,190.0,382.0,0.0,188.0,140.0,0.0,209.26,0.0,137.36,262.0,223.0,49.0,274.76,278.99,137.0,568.0,249.0,5.98,34.08,88.0,402.56,14.0,224.0,673.0,168.0,283.0,172.0,406.0,356.74,200.31,231.0,0.0,389.98,348.0,55.0,234.97,174.0,121.79,0.0,273.0,57.0,104.0,206.0,186.44,0.0,6.3,21.0,82.0,0.0,23.98,170.0,137.0,232.8,325.0,99.0,0.0,412.0,424.0,1.0,60.0,724.0,468.0,207.71,314.0,551.0,279.0,21.0,141.0,95.0,263.17,12.0,0.0,101.0,440.0,23.0,51.66,643.0,256.21,0.0,1475.04,800.0,234.02,3.63,272.65,0.0,0.0,0.0,129.98,312.94,0.0,173.57000000000002,0.0,177.0,346.46,0.0,313.15999999999997,124.35000000000001,0.0,0.0,212.64,273.0,309.72,32.65,0.0,130.0,521.0,136.54,120.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,149.0,234.97,319.0,51.0,441.99,472.0,7.4,0.0,0.0,388.8,14.16,761.0,477.0,625.92,636.0,355.98,2143.0,10291.0,187.62,186.8,446.54,2805.98,0.0,2040.0,219.4,751.0,259.0,85.0,881.1800000000001,130.73,47327.16,794.0,57.04,416.0,174.9,340.0,0.0,449.0,11985.57,379.68,10669.99,292.08,805.0,502.0,1226.96,199.97,3167.0,365.0,0.0,691.2,18.240000000000002,9854.0,134.0,593.52,0.0,254.0,0.0,149.0,224.0,3113.0,293.0,0.0,144.0,172.0,0.0,17.7,138.0,195.54,710.0,8859.0,0.0,69.73,248.0,0.0,38.84,0.0,0.0,23.700000000000003,0.0,531.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-05,220.0,44.0,0.0,380.0,269.0,0.0,226.96,63.0,0.0,150.0,17.0,0.0,118.0,4.0,158.0,160.0,297.0,503.0,949.0,40.0,6.0,0.0,58.0,223.1,151.0,326.0,0.0,8.0,86.0,296.0,305.0,100.0,0.0,4328.0,0.0,0.0,3596.0,5.0,184.0,0.0,334.0,23.0,12642.949999999999,198.44,147.45,161.0,90.0,4080.0,104.04,0.0,0.0,115.0,208.0,21.01,0.0,325.0,435.0,477.95,201.01999999999998,62.0,330.0,414.0,0.0,0.0,431.02,55.0,101.95,0.0,797.0,122.0,113.0,1807.0,0.0,30.96,0.0,193.0,0.0,101.0,246.96,0.0,0.0,9.030000000000001,193.0,18.0,264.0,0.0,54.349999999999994,176.0,88.0,315.0,0.0,314.0,491.0,568.0,185.0,528.0,672.0,79.0,133.0,166.0,504.35,0.0,278.35,0.0,333.0,222.0,118.8,149.0,230.0,110.02000000000001,244.0,3870.0,438.83000000000004,0.0,452.0,130.0,0.0,0.0,588.0,258.78,755.48,448.0,20425.0,3.0,904.0,652.28,220.0,492.0,207.0,266.0,236.01999999999998,141.6,393.0,574.0,699.61,0.0,0.0,157.0,0.0,460.0,30.0,235.0,7.0,68.0,46.0,0.0,357.0,285.0,680.0,0.0,36.199999999999996,0.0,249.0,119.46,33.0,0.0,17.0,106.0,104.0,0.0,218.0,201.0,218.0,64.89,43.0,26.0,234.0,303.0,252.0,302.2,298.0,80.0,1582.04,0.0,238.0,466.20000000000005,0.0,141.0,181.0,485.6,127.0,162.0,301.98,1106.0,210.0,0.0,204.0,464.0,0.0,34.96,396.0,395.0,0.0,0.0,360.09999999999997,171.0,125.0,40.62,49.68,0.0,146.0,182.08,0.0,0.0,28.96,456.8,0.0,122.0,186.0,0.0,240.5,169.87,111.0,262.2,348.0,45.0,608.48,198.0,16510.0,0.0,14.0,151.0,235.0,106.99000000000001,368.0,212.0,182.57,108.4,126.54,178.0,51.0,0.0,325.0,0.0,0.0,136.0,125.0,0.0,13.0,65.0,220.0,170.44,0.0,0.0,61.0,171.0,38.0,340.0,12200.0,287.0,229.29,268.95,52.0,66.07,705.75,201.4,94.0,264.20000000000005,250.0,206.97,70.36,222.98000000000002,60.0,484.03999999999996,183.0,0.0,0.0,0.0,672.8,237.76,184.0,143.89000000000001,0.0,6.0,158.97,236.94,199.0,49.0,320.0,0.0,391.0,90.64,7.0,0.0,0.0,110.0,1.0,7.0,118.0,262.0,20.0,1448.0,33200.0,322.06,5.0,388.0,62.0,48.0,169.4,0.0,178.62,69.0,190.0,269.0,76.0,177.0,0.0,268.01,79.0,0.0,248.0,0.0,0.0,9.15,105.0,99.0,0.0,68.03,303.0,442.0,11.0,90.0,0.0,0.0,328.0,211.0,0.0,64.0,5.0,176.0,1.0,366.70000000000005,400.4,100.03,2097.0,5748.0,27.0,191.0,40.0,196.0,85.0,0.0,494.14,0.0,58.0,0.0,153.0,0.0,90.0,335.0,13.0,250.0,359.6,5837.070000000001,0.0,5111.0,0.0,233.0,7.0,893.0,0.0,0.0,45.0,206.0,118.0,437.53999999999996,3.44,182.01,127.0,473.0,0.0,0.0,103.48,0.0,19400.0,2480.0,0.0,10.0,163.0,88.0,25.0,160.0,18.0,174.0,187.8,49.0,137.2,85.33,0.0,334.0,398.0,96.0,587.0,37.0,50.0,753.0,64.0,0.0,2.0,345.0,328.0,295.0,1259.0,0.0,0.0,138.0,968.0,280.0,292.0,0.0,107.0,0.0,201.0,513.0,0.0,374.96999999999997,107.0,406.0,0.0,153.0,0.0,670.0,126.0,145.0,122.39999999999999,0.0,0.0,0.0,171.0,0.0,487.0,364.0,95.0,311.0,15.0,108.0,747.0,592.0,0.0,250.0,323.0,0.0,387.0,64.0,886.0,126.0,133.0,162.0,0.0,0.0,0.0,244.0,251.0,201.6,745.0,0.0,107.6,0.0,180.0,685.0,264.0,0.0,0.0,0.0,226.0,262.0,2.09,1079.0,101.0,281.0,3.44,44.019999999999996,215.32,0.0,570.0,0.0,0.0,0.0,0.0,0.0,18.0,27.0,38.0,138.0,48.0,75.0,8.0,238.0,72.0,0.0,85.0,239.0,183.0,0.0,9.2,209.0,399.0,259.0,860.98,307.0,106.98,160.26999999999998,201.0,153.75,91.0,166.0,255.2,328.0,218.44,275.0,237.05,170.0,447.0,352.0,0.0,0.0,0.0,482.0,203.0,0.0,74.0,161.44,87.52,192.75,468.0,481.22,0.0,429.0,231.0,0.0,190.0,399.0,139.2,395.0,25.0,0.0,134.0,0.0,58.0,121.0,521.8,304.0,174.0,28.0,161.0,0.0,124.0,0.0,72.0,0.0,186.0,27.0,446.0,0.0,72.0,137.0,187.0,0.0,90.44,368.0,138.0,68.0,311.0,472.0,604.0,0.0,451.0,70.02,518.0,137.0,133.0,0.0,309.8,295.0,10.0,386.67,273.0,0.0,42.96,110.0,195.0,0.0,784.47,196.0,44.76,352.0,3.0,98.0,632.0,45.0,61.64,778.4,6.0,393.0,245.89,0.0,143.0,175.0,62.0,155.0,80.0,208.0,108.0,130.0,339.0,4.0,242.78,238.0,370.0,150.84,329.0,40.0,52.0,377.0,164.0,210.97,98.0,73.0,206.12,141.0,259.94,124.0,0.0,33.0,0.0,0.0,0.0,15.0,667.0,253.0,0.0,60.0,7.0,159.0,443.0,335.0,0.0,638.0,0.0,234.0,558.84,427.0,0.0,36.0,132.25,116.8,0.0,458.0,18.0,474.0,93.0,354.0,211.0,49.0,217.89,0.0,279.0,362.0,4540.0,0.0,320.0,0.0,44.0,304.0,0.0,113.0,64.97,187.0,369.0,105.0,33.02,0.0,128.0,0.0,0.0,136.0,0.0,177.0,170.8,0.0,90.0,6.0,12.0,452.0,0.0,431.0,39.0,187.0,219.4,134.0,124.0,587.0,446.0,247.0,179.0,1004.0,406.0,0.0,742.0,149.0,258.48,43.0,3.0,0.0,0.0,135.8,53.0,178.0,0.0,0.0,289.0,151.0,0.0,97.0,95.98,54.0,289.58,388.0,74.0,166.0,358.03000000000003,11.2,16.0,11.64,570.0,0.0,135.0,1.0,55.0,0.0,182.62,108.0,49.6,59.0,80.0,152.0,42.0,0.0,136.0,397.0,171.03,85.6,17.0,783.0,175.0,361.0,347.0,34.0,283.0,46.98,316.0,41.0,164.0,353.6,0.0,11.0,386.0,425.0,673.0,343.6,190.0,245.2,0.0,274.0,234.0,0.0,210.0,845.4499999999999,332.0,556.0,17146.0,361.0,2.0,205.0,369.0,0.0,66.0,0.0,175.0,33.0,228.0,130.0,0.0,353.0,3543.0,367.0,174.0,86.0,87.0,211.98,177.0,116.0,0.0,133.32,42.480000000000004,0.0,529.0,0.0,188.0,390.0,316.0,47.0,900.0,86.0,185.0,134.0,49.0,0.0,0.0,547.0,207.8,107.6,183.0,0.0,211.6,0.0,218.16,80.0,147.0,201.8,244.0,16.0,67.0,0.0,0.0,0.0,146.0,292.0,436.2,19.0,819.0,89.2,91.0,0.0,0.0,61.0,240.0,257.69,439.0,149.36,170.0,513.0,316.0,222.62,167.0,257.0,0.0,0.0,159.0,62.0,294.96999999999997,261.0,151.0,229.0,1.0,3.42,1261.85,191.62,6.0,294.0,371.53,16.0,594.0,155.78,194.0,439.0,354.0,233.0,0.0,102.0,639.01,130.0,0.0,349.0,170.0,77.01,107.0,0.0,260.55,278.0,0.0,77.0,3.0,216.0,0.0,0.0,51.0,593.74,163.0,1550.0,0.0,94.4,10.18,168.0,107.0,235.0,10.0,166.0,59.0,0.0,217.0,0.0,148.0,270.43,250.2,340.0,238.0,54.0,91.0,2.0,0.0,300.0,37.98,0.0,0.0,37.0,611.16,0.0,0.0,52.0,6.0,277.0,85.0,311.0,97.0,147.0,76.0,102.0,3.0,0.0,503.0,0.0,231.0,4.0,70.0,171.62,152.6,0.0,0.0,218.0,249.0,310.0,4.0,75.0,215.0,181.0,371.0,93.0,386.0,337.03999999999996,0.0,31.0,149.57999999999998,127.74,403.0,139.0,440.95,221.0,0.0,6.0,200.0,147.0,748.0,45.0,1005.64,239.03000000000003,0.0,0.0,2358.0,405.59999999999997,206.0,65.0,18.0,0.0,124.0,464.2,96.92,0.0,0.0,213.0,0.0,0.0,182.0,338.04,443.0,0.0,24.0,120.0,213.0,0.0,206.0,0.0,394.0,307.0,207.98000000000002,660.0,0.0,0.0,53.0,25.0,434.0,268.0,119.0,310.0,63.0,469.4,0.0,347.0,256.0,4.0,204.0,227.0,14.0,0.0,0.0,396.0,376.0,251.0,0.0,990.0,0.0,98.0,190.4,0.0,234.0,0.0,516.0,189.8,5.0,0.0,53.0,110.22,324.0,6.0,15480.0,185.0,161.8,386.0,84.0,120.0,361.0,31.0,423.0,0.0,0.0,304.0,166.0,12.0,0.0,79.0,118.0,40.660000000000004,67.0,65.0,33.0,142.0,293.0,1037.0,83.0,70.6,314.0,110.0,123.0,0.0,0.0,237.0,49.65,6.4,57.599999999999994,0.0,326.0,929.0,0.0,36.56,639.0,326.0,180.0,29.0,197.0,85.0,135.0,403.2,182.0,25.0,150.11,75.0,73.25999999999999,279.0,482.9,472.6,109.74000000000001,117.0,58.99,57.0,0.0,163.4,406.0,0.0,347.0,95.0,90.0,310.03,87.81,180.0,380.0,111.0,191.0,79.8,0.0,340.0,65.0,135.0,269.0,56.0,35.0,413.0,75.46,687.0,0.0,0.0,0.0,220.97,171.0,190.0,199.35,1845.0,168.0,134.0,84.0,17.97,1780.0,557.0,839.4,1.0,229.0,0.0,0.0,160.03,0.0,394.0,16720.0,0.0,101.6,180.98,437.0,0.0,227.0,0.0,204.6,0.0,168.0,315.99,0.0,1914.61,1251.0,96.0,78.0,120.6,463.2,67.8,269.0,84.72,313.0,360.0,137.0,130.0,27.0,138.0,56.0,0.0,0.0,114.0,48.0,559.0,0.0,207.0,255.0,44.0,0.0,17.0,0.0,338.0,0.0,196.88,53.82,162.0,232.52,0.0,140.1,294.0,280.0,458.0,91.0,0.0,276.0,16.759999999999998,1.0,372.40000000000003,48.0,753.0,482.0,397.0,21239.98,432.0,221.0,450.0,369.0,166.0,168.0,209.8,0.0,312.0,283.0,277.0,0.0,46.0,93.77,11220.0,905.0,407.0,5.0,244.0,0.0,401.7,38.0,0.0,336.0,274.0,161.0,441.0,0.0,369.0,0.0,0.0,208.0,504.0,0.0,1522.0,0.0,139.0,231.0,8.0,37.0,269.52,0.0,158.0,334.0,10.9,351.0,320.99,124.03999999999999,55.0,87.0,0.0,106.21,257.01,430.0,166.0,267.0,325.0,54.4,0.0,188.0,523.0,363.0,204.0,0.0,353.5,172.0,173.0,48.0,249.0,184.61,398.55,181.25,360.0,116.0,16.0,0.0,81.0,424.0,228.32,209.04999999999998,101.0,343.0,0.0,0.0,225.04000000000002,65.0,25.0,431.02,40.0,113.0,490.0,264.28,303.0,0.0,238.0,135.0,185.16,109.0,158.97,135.0,257.0,53.03,0.0,123.6,57.86,115.0,0.0,170.0,45.0,83.0,37.0,0.0,255.0,146.0,153.68,89.0,36.0,118.2,254.44,84.24,143.0,0.0,0.0,0.0,0.0,211.0,119.0,256.0,130.0,0.0,0.0,320.0,99.0,0.0,240.4,1669.99,22590.0,0.0,458.0,0.0,54.0,190.0,394.0,152.0,0.0,107.3,290.0,66.0,361.0,521.0,133.0,744.0,235.92000000000002,76.0,359.0,328.0,189.0,0.0,0.0,21.0,118.0,6344.36,173.0,0.0,0.0,357.0,77.02,83.0,70.0,274.03,314.0,22.0,89.0,1073.7,368.76,49.0,313.61,292.0,318.0,429.0,145.07999999999998,13.0,44.0,32.0,45.0,71.0,0.0,304.0,307.0,290.0,145.01,99.0,269.0,68.0,370.79,84.0,81.0,80.4,0.0,209.0,229.52,285.17,179.0,428.0,256.0,139.76999999999998,0.0,0.0,317.02,365.0,117.12,342.65,214.0,35970.0,0.0,857.0,223.0,169.0,159.0,395.4,0.0,133.81,0.0,1.0,0.0,3.0,0.0,0.0,0.0,67.0,270.0,1117.0,137.0,299.0,70.0,212.0,248.82999999999998,0.0,511.0,385.0,0.0,12070.0,0.0,0.0,0.0,0.0,0.0,278.0,290.62,121.48,0.0,61.0,329.0,222.21,25.0,256.0,271.36,347.0,77.0,86.0,105.0,663.0,0.0,310.06,0.0,243.0,589.0,15.57,152.0,230.0,0.0,240.0,245.0,302.97,379.0,144.0,13.02,164.0,20.0,58.0,120.0,357.0,0.0,65.98,274.69,345.0,211.0,552.0,397.0,0.0,0.0,99.0,202.0,46.0,145.0,169.0,68.56,255.0,646.0,606.0,0.0,30.0,195.0,195.0,0.0,430.05,400.0,390.0,46.0,132.0,523.0,0.0,425.0,23.4,250.0,193.0,111.0,140.0,609.15,114.60000000000001,79.0,219.0,104.0,384.0,1045.0,85.0,118.0,0.0,138.0,298.0,336.0,312.0,0.0,65.0,324.48,142.0,190.96,105.8,111.0,126.0,254.0,0.0,256.0,0.0,141.0,473.72,276.03000000000003,130.0,408.4,688.0,166.0,195.0,242.0,0.0,444.0,270.0,37.53,0.0,175.0,215.0,1.9,252.0,90.04,1074.0,0.0,274.0,144.0,240.0,257.0,223.22,432.0,322.0,358.0,103.0,343.0,12.0,0.0,10.4,178.0,104.0,49.2,130.0,0.0,273.99,1343.8,370.4,73.0,314.0,27.0,154.0,131.6,101.0,69.00999999999999,83.0,419.0,165.0,171.8,86.0,63.0,89.0,292.0,28.0,0.0,120.0,0.0,0.0,301.0,178.0,14.98,492.0,1566.0,438.77,124.0,260.92,47.0,12.0,257.0,0.0,30.39,0.0,253.6,0.0,0.0,1.0,64.0,75.0,317.0,387.0,498.0,0.0,42.0,60.0,626.76,160.26,50.0,210.0,161.21,0.0,157.0,1264.0,0.0,57.0,196.8,0.0,90.0,255.0,166.17999999999998,2.33,255.0,138.33,187.0,261.0,0.0,95.57000000000001,48.0,56.06,399.0,253.0,195.03,44.0,143.85999999999999,22090.0,338.0,198.0,0.0,262.0,0.0,528.0,587.0,0.0,0.0,0.0,279.0,252.0,3.0,0.0,0.0,125.0,638.0,134.0,0.0,190.0,110.2,136.0,11.0,23.880000000000003,187.0,57.0,0.0,0.0,0.0,2.0,14.0,2193.0,69.0,0.0,194.57,0.0,0.0,271.0,0.0,266.0,0.0,317.2,328.0,245.7,22.0,0.0,139.0,0.0,203.4,112.0,165.0,250.0,0.0,0.0,55.0,81.6,33.0,422.62,0.0,0.0,200.0,847.0,31996.0,10.0,87.0,53.66,810.0,205.0,400.0,213.0,77.0,113.0,179.28,358.6,287.0,45.62,118.0,185.0,107.0,202.0,0.0,351.0,26.0,0.0,173.0,13700.0,74.0,49.0,0.0,0.0,439.2,111.0,57.0,674.8,0.0,125.0,297.0,109.0,95.0,120.08000000000001,0.0,0.0,0.0,44.0,244.44,322.0,6544.0,214.24,221.0,39.0,295.0,288.0,0.0,0.0,144.0,117.97,0.0,78.0,31.959999999999997,0.0,31896.0,232.0,6.0,4350.0,398.02,208.46,112.0,151.6,307.0,514.0,165.0,0.0,257.0,357.0,106.46,130.3,116.0,0.0,0.0,423.0,261.0,0.0,264.0,302.0,63.0,88.0,101.89,0.0,18.0,0.0,0.0,460.0,33.0,138.0,335.0,99.0,298.0,143.0,42.0,1212.0,142.0,66.0,285.6,8.99,176.8,0.0,151.0,225.0,106.0,0.0,0.0,190.0,0.0,322.0,116.6,806.0,0.0,412.0,0.0,0.0,506.0,146.0,0.0,0.0,169.57999999999998,247.0,231.0,0.0,841.96,96.0,388.0,33.0,206.0,0.0,330.22,193.9,71.0,752.0,231.0,114.0,1.98,181.0,4.0,0.0,160.0,342.0,0.0,408.0,0.0,148.0,0.0,1091.68,0.0,523.0,1575.04,236.0,427.0,8.0,88.0,153.14999999999998,67.0,0.0,91.33,200.0,0.0,0.0,9.0,21.200000000000003,0.0,133.4,224.0,0.0,276.0,706.0,0.0,304.0,127.0,282.3,282.07,415.0,169.0,244.04,90.92,0.0,360.38,25.02,-1.0,110.0,283.0,477.0,56.57,0.0,462.88,184.0,266.0,118.0,284.0,118.0,126.38000000000001,390.0,177.0,197.0,81.0,42.0,163.0,313.0,1180.0,47.0,2356.0,392.25,0.0,112.0,319.0,431.0,0.0,357.0,73.99,130.0,329.0,0.0,136.0,13810.0,0.0,207.0,1.0,49.0,123.0,36.0,475.6,121.0,1082.0,0.0,185.0,177.0,66.0,0.0,0.0,128.0,429.0,136.0,0.0,120.03,29.0,295.0,546.0,1.0,29.0,625.8,243.4,0.0,0.0,0.0,0.0,63.0,500.0,0.0,15.48,108.61,472.0,55.87,379.0,3.8,0.0,124.98,0.0,19.0,281.0,9.0,0.0,0.0,108.0,0.0,21.6,67.0,117.0,757.0,0.0,268.96000000000004,107.0,261.03000000000003,161.0,0.0,27.02,116.4,79.0,174.0,33.0,559.0,302.97,2.0,393.0,189.94,299.33,275.0,234.0,132.6,146.0,157.8,0.0,467.0,247.01,0.0,146.0,312.0,81.0,0.0,407.0,175.2,259.0,0.0,0.0,18.0,0.0,199.0,175.08,69.0,176.67000000000002,0.0,45.0,34.82,7.0,337.1,59.0,128.0,0.0,1137.0,312.0,31.0,488.52,134.0,291.0,208.0,0.0,114.0,130.0,0.0,115.0,126.0,306.0,917.86,10.0,0.0,329.97,9737.970000000001,0.0,69.0,339.0,1.4,73.0,218.0,0.0,186.0,23.0,50.0,114.0,23.0,240.0,218.99,40.0,179.0,107.4,268.0,6290.0,164.0,0.0,79.0,2.99,82.0,350.70000000000005,82.0,1.0,226.0,88.0,0.0,0.0,244.0,164.0,248.6,264.37,190.0,383.0,256.0,113.03,49.04,981.0,333.0,273.0,0.0,296.0,116.0,154.0,0.0,229.0,369.0,0.0,243.0,288.0,146.85999999999999,0.0,447.03,357.0,0.0,429.0,50.0,235.14000000000001,165.0,258.0,0.0,14.040000000000001,148.04,0.0,1403.0,0.0,0.0,67.0,3.63,152.0,457.63,10900.0,0.0,256.0,641.64,204.0,296.68,483.0,65.97,0.0,133.0,221.0,0.0,143.0,255.0,363.0,66.35,108.0,181.0,267.6,365.38,0.0,42.09,19.0,375.0,2818.0,53.97,642.0,108.0,0.0,104.0,150.0,0.0,0.0,4.0,252.0,0.0,145.0,276.0,88.0,81.0,241.14,1.0,0.0,77.69,0.0,100.0,245.96,1.0,133.0,245.0,167.0,873.0,0.0,85.0,17.0,0.0,229.0,175.0,0.0,0.0,112.0,3.8000000000000003,535.0,218.0,0.0,9.0,383.0,5.95,43.0,435.0,53.0,536.54,992.0,481.0,112.97,25.02,0.0,62.019999999999996,0.0,525.0,96.0,103.0,314.0,128.0,117.32,64.0,24.0,258.0,0.0,276.92,189.0,99.0,20.93,899.0,341.0,225.05,534.0,0.0,280.0,0.0,279.0,159.98,256.0,281.0,195.0,0.0,0.0,0.0,76.0,666.0,158.0,0.0,45.0,0.0,169.0,427.0,159.4,0.0,34.0,238.0,106.4,317.0,281.0,0.0,27.0,127.1,342.0,650.0,0.0,0.0,121.0,0.0,0.0,300.40000000000003,302.0,138.0,334.0,0.0,282.0,0.0,116.0,374.0,493.0,586.0,179.0,649.2,503.0,1445.0,135.0,168.0,385.0,44.0,27.0,2050.0,16.0,215.0,0.0,202.16,451.0,222.0,293.6,146.0,0.0,76.0,0.0,169.0,213.34,423.0,133.0,185.0,226.0,171.0,0.0,3.28,0.0,130.0,97.6,487.0,2.0,178.0,172.32,349.0,237.0,323.0,0.0,327.0,86.0,1.0,181.78,0.0,206.67,249.0,143.0,26.0,132.8,250.0,209.0,693.0,266.0,0.0,44.339999999999996,171.0,342.45,27.0,103.0,1017.0,135.0,3.0,175.0,309.0,343.92,489.72,169.0,0.0,327.96000000000004,119.00999999999999,73.0,318.02,135.52,303.0,0.0,191.0,97.66,212.0,168.0,242.6,0.0,0.0,2.0,283.0,0.0,51.4,61.95,129.0,342.2,436.0,136.0,0.0,342.0,770.0,0.0,0.0,656.18,325.0,133.03,304.0,451.0,185.0,31.0,238.0,190.0,97.86,76.0,0.0,44.0,552.0,15.0,54.16,597.0,303.90000000000003,0.0,1046.02,499.65,379.01,4.17,340.79999999999995,0.0,0.0,0.0,102.97,89.28,0.0,189.36,0.0,268.0,446.05,0.0,511.77,133.78,0.0,0.0,165.94,0.0,334.08,66.46000000000001,1.0,162.0,400.0,196.06,74.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,121.97,85.02,393.0,56.96,325.0,443.0,5.0,0.0,0.0,0.0,8.08,387.0,377.0,330.04,213.0,276.0,18.0,0.0,293.71,93.12,653.04,17301.94,0.0,1715.0,99.6,1728.0,322.0,71.0,1283.76,266.52,11907.34,666.0,34.07,4119.0,476.3,215.0,0.0,276.0,12095.56,358.95,8315.8,292.08,522.0,533.0,430.0,458.68,2260.0,500.0,0.0,688.38,1.52,6856.0,261.0,497.79,0.0,418.0,0.0,324.0,383.0,3180.0,198.0,0.0,187.0,65.0,0.0,10.4,133.0,362.41,580.0,2303.0,0.0,68.53,840.0,455.98,0.0,0.0,0.0,0.0,0.0,561.57,2036.0,17.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-06,276.0,0.0,0.0,453.0,224.51999999999998,0.0,205.6,112.0,0.0,50.0,22.0,0.0,344.0,3.0,158.0,413.0,419.32,513.0,822.0,128.0,6.0,0.0,37.0,226.39999999999998,93.0,262.0,0.0,153.0,80.0,263.0,317.0,58.39,0.0,5408.0,0.0,0.0,7379.0,100.93,222.0,0.0,388.0,117.0,17364.989999999998,277.0,61.34,161.0,80.0,3313.0,613.77,0.0,14.0,105.0,301.0,63.019999999999996,0.0,185.8,179.0,481.0,356.40000000000003,45.63,355.0,98.0,0.0,0.0,132.48,37.0,14.0,0.0,542.0,63.0,155.0,1055.01,0.0,28.63,0.0,59.0,0.0,146.0,314.79,0.0,34.0,9.0,283.0,21.0,251.0,0.0,26.96,127.0,185.0,334.0,0.0,140.0,650.0,790.6,112.0,468.0,623.0,306.0,89.0,337.0,425.9,0.0,517.6800000000001,113.0,307.0,114.0,162.27999999999997,232.0,347.0,87.02,165.0,9700.0,251.48,0.0,250.0,348.0,0.0,0.0,285.0,93.0,1338.92,396.0,9071.0,5.0,691.0,563.82,249.0,596.0,241.0,287.0,189.78,72.48,314.0,735.0,573.03,0.0,0.0,157.0,0.0,356.0,103.0,365.0,14.0,150.0,187.0,0.0,235.0,364.0,880.0,0.0,40.68,0.0,150.0,211.56,106.6,0.0,16.0,103.0,264.0,0.0,276.0,199.0,100.31,217.03,123.0,82.0,151.0,258.03999999999996,309.0,269.8,251.0,160.34,640.0,0.0,200.0,453.78,0.0,144.0,177.0,615.4,174.31,244.0,273.0,480.0,595.0,0.0,503.0,218.0,0.0,77.02,601.0,340.0,0.0,0.0,482.48,276.0,314.0,16.2,66.35,0.0,232.0,153.88,78.0,0.0,33.0,198.2,0.0,147.0,163.0,0.0,358.95,177.98000000000002,98.0,233.0,152.0,57.97,256.64,185.0,17792.0,30.0,542.0,314.0,279.0,100.98,176.0,307.03,85.44,139.6,284.44,68.0,36.0,0.0,404.0,0.0,0.0,190.0,1.0,0.0,11.8,169.0,193.0,208.5,0.0,0.0,41.0,525.0,227.0,210.0,2350.0,138.0,211.2,277.88,159.0,30.96,697.56,179.0,33.0,490.0,428.0,301.98,75.2,292.6,144.0,457.01,203.4,0.0,9.0,0.0,543.94,226.07999999999998,137.6,105.35,0.0,16.0,94.0,213.6,191.0,109.0,513.0,0.0,359.0,222.08,399.0,0.0,0.0,35.29,2.0,44.0,138.0,163.0,22.0,298.0,39280.0,342.96,8.0,180.0,58.0,5.0,223.22,0.0,151.44,118.0,182.0,325.0,103.0,105.0,0.0,162.0,58.0,0.0,254.0,0.0,0.0,0.0,175.0,163.0,0.0,99.03999999999999,458.0,300.0,0.0,203.0,0.0,0.0,204.0,195.0,0.0,48.0,6.0,285.0,2.99,493.1,204.8,97.0,2121.0,5306.0,121.0,44.0,142.8,340.0,102.0,0.0,422.0,0.0,12.0,0.0,280.0,0.0,101.0,440.0,0.0,359.48,244.0,302.21,0.0,6102.0,0.0,291.0,8.0,859.0,0.0,1.2000000000000002,62.0,244.0,144.0,183.44,4.0,108.0,150.0,459.0,0.0,0.0,66.56,0.0,15030.0,4770.0,0.0,13.0,260.0,24.0,26.0,138.0,16.0,171.0,188.0,3.0,145.76,161.66,0.0,185.0,357.0,68.0,556.0,128.0,2.0,216.0,124.0,0.0,2.0,258.0,507.0,312.0,169.0,0.0,0.0,706.0,334.53,488.0,291.0,0.0,135.3,26.0,163.0,391.0,0.0,175.56,82.0,669.0,0.0,332.0,0.0,498.0,117.0,143.0,130.2,0.0,0.0,0.0,116.0,0.0,307.0,121.0,64.0,16.0,19.0,36.0,6741.0,576.0,0.0,269.0,248.0,0.0,530.0,89.0,606.0,162.0,84.0,169.0,5.0,0.0,0.0,113.03999999999999,203.0,156.13,568.98,0.0,134.4,0.0,263.0,302.0,343.0,0.0,0.0,0.0,333.0,299.0,0.0,762.0,360.0,330.0,6.87,36.0,257.76,0.0,510.0,70.0,0.0,0.0,0.0,23.0,18.48,76.0,0.0,159.0,151.0,95.0,15.0,187.0,10.0,0.0,109.0,512.0,230.0,0.0,4.8,105.0,114.0,86.0,982.0,221.97,197.7,236.88,311.0,130.4,167.0,178.0,162.2,215.0,280.0,233.0,232.98000000000002,291.0,339.0,341.0,0.0,0.0,669.0,595.0,84.0,0.0,63.0,188.54999999999998,70.45,231.2,92.99,784.56,0.0,462.0,197.0,0.0,360.0,510.0,116.0,358.0,44.0,0.0,216.0,1778.0,75.0,196.0,510.48,381.0,222.52,25.0,214.0,0.0,95.0,0.0,109.0,0.0,280.0,41.04,281.0,0.0,137.0,201.4,257.0,0.0,54.97,337.0,1667.0,46.0,196.0,506.0,517.0,0.0,416.0,34.88,214.0,319.0,149.0,0.0,307.0,248.0,12.0,285.36,576.0,0.0,11.8,84.0,232.0,0.0,1248.0800000000002,138.0,51.0,508.0,32.0,124.0,847.2,101.0,195.0,559.0,0.0,217.0,208.0,0.0,124.0,200.0,45.0,301.0,50.0,199.0,128.0,46.0,174.0,7.0,298.97,315.0,213.0,218.96,149.0,46.0,32.0,225.0,440.0,108.15,117.0,90.0,274.69,390.0,395.13,181.0,0.0,173.0,0.0,0.0,0.0,0.0,3.0,74.0,0.0,153.0,10.0,207.0,396.0,244.0,0.0,359.71,0.0,287.0,513.49,423.0,0.0,136.0,138.0,81.52,0.0,460.0,20.0,229.01,20.0,354.0,212.0,51.0,154.06,0.0,436.0,282.0,5478.0,0.0,313.0,0.0,111.0,262.03000000000003,0.0,155.0,93.36,55.0,232.0,170.0,19.0,0.0,51.0,0.0,0.0,147.0,0.0,200.0,244.0,8.0,106.0,4.0,15.0,462.0,0.0,409.2,132.0,267.0,230.0,160.0,189.0,401.0,366.0,518.0,261.0,694.0,235.0,5.040000000000001,496.0,82.96000000000001,306.4,15.0,109.0,0.0,0.0,271.0,58.0,228.0,31.0,0.0,194.0,322.0,0.0,152.03,98.2,79.0,242.91,165.0,50.0,141.0,364.0,1.0,0.0,151.43,491.0,0.0,75.0,0.0,13.0,0.0,217.4,134.0,915.4,67.0,182.0,264.0,74.0,0.0,60.4,390.0,257.0,644.4,5.0,397.0,134.0,209.96,353.0,82.0,635.0,37.0,311.96,199.0,148.0,524.4,0.0,50.0,349.96999999999997,201.0,506.0,353.4,169.0,284.8,0.0,230.0,279.0,0.0,110.0,767.5899999999999,412.0,453.0,13917.0,232.0,2.0,236.0,693.0,0.0,76.0,0.0,162.0,6.0,295.0,249.0,0.0,202.0,3826.0,897.0,191.0,101.0,121.0,91.6,251.0,76.0,0.0,296.7,42.480000000000004,0.0,135.0,0.0,310.0,844.0,537.0,15.0,581.0,124.0,54.0,132.0,54.99000000000001,0.0,0.0,505.0,120.98,207.0,23892.56,0.0,277.4,0.0,370.0,227.0,73.0,127.23,201.4,174.0,94.0,130.0,0.0,0.0,153.0,267.0,424.8,11.0,762.0,60.76,33.45,0.0,54.0,100.0,313.0,0.0,160.0,222.76,128.0,480.0,183.0,245.97,166.0,545.0,0.99,4.0,429.0,70.01,144.44,267.0,278.0,279.0,0.0,24.15,969.02,211.0,7.0,624.0,401.77,22.0,190.0,158.2,251.0,303.0,204.0,225.0,0.0,159.0,560.0,207.0,0.0,295.0,153.0,105.7,321.0,0.0,357.0,301.12,0.0,156.0,0.0,325.0,0.0,0.0,87.0,434.2,405.0,1690.0,0.0,209.6,17.45,143.0,135.0,486.0,4.98,259.0,123.0,0.0,309.0,0.0,132.0,293.4,274.79,208.0,252.0,57.0,37.0,0.0,0.0,206.0,10.0,0.0,0.0,152.0,677.06,0.0,0.0,26.6,9.0,153.0,82.0,298.0,164.0,228.0,133.0,121.0,4.0,0.0,293.0,0.0,217.0,0.0,90.0,217.69,157.0,0.0,0.0,95.99000000000001,311.0,151.0,5.0,23.0,186.0,146.0,628.0,92.0,441.0,426.0,0.0,19.0,103.49000000000001,406.21,404.0,207.0,477.0,309.99,0.0,31.02,214.0,260.0,295.0,0.0,953.35,145.0,0.0,0.0,2839.0,405.59999999999997,274.0,136.0,40.0,0.0,142.0,282.0,95.0,0.0,107.0,95.0,6.0,0.0,280.0,129.4,175.6,0.0,126.0,98.0,129.0,1.3800000000000001,366.0,0.0,258.0,334.0,180.99,596.0,0.0,0.0,62.0,72.0,92.0,213.0,162.0,308.0,106.0,708.6,0.0,179.0,246.0,4.0,168.0,227.0,18.0,0.0,0.0,354.0,192.0,93.0,0.0,342.0,0.0,64.0,295.28000000000003,0.0,229.0,0.0,500.0,121.2,0.0,0.0,32.0,138.76,304.0,9.0,15407.22,254.0,242.2,525.0,1.95,213.0,462.0,0.0,477.0,2569.0,0.0,404.0,92.0,0.0,0.0,169.0,112.0,181.4,61.0,102.0,177.0,221.0,355.0,816.0,88.2,70.4,244.0,171.0,82.0,0.0,0.0,251.0,9.32,52.199999999999996,28.61,0.0,354.0,910.0,0.0,35.13,551.0,227.35999999999999,213.0,35.0,137.0,3593.0,168.0,415.8,246.0,45.0,42.88,64.0,44.82,383.0,412.88,445.99,180.0,144.0,112.98,82.0,6.0,449.0,338.0,0.0,294.0,86.0,155.0,246.0,204.06,295.0,130.0,56.0,15.0,48.0,0.0,239.0,121.0,131.0,174.0,212.37,48.0,184.0,120.53,337.0,1.0,0.0,0.0,70.69,126.0,203.0,223.56,2304.0,190.0,191.0,106.0,28.06,1775.0,442.0,79.03999999999999,0.0,566.0,0.0,0.0,138.4,0.0,458.0,24360.0,0.0,111.0,189.6,270.0,0.0,289.0,0.0,467.0,0.0,144.0,467.0,0.0,1761.4,1271.0,165.0,134.0,46.57,512.76,158.0,245.0,78.83,444.0,254.0,164.0,120.0,102.03,120.0,36.0,0.0,0.0,81.0,52.0,565.0,0.0,183.0,203.0,36.0,0.0,67.0,0.0,135.0,0.0,125.6,223.2,89.0,264.44,0.0,71.95,288.0,360.0,262.0,328.67,0.0,326.0,19.98,0.0,694.76,49.96,629.0,231.0,420.0,21377.96,401.98,343.0,438.0,360.0,252.0,68.0,409.0,0.0,202.0,112.0,521.76,0.0,97.0,16.0,10130.0,582.01,279.0,4.0,131.0,0.0,497.01,297.0,0.0,495.0,390.0,165.0,347.0,0.0,442.98,14.0,0.0,220.0,138.0,0.0,1405.99,0.0,209.0,389.0,28.99,59.0,269.52,0.0,197.0,292.0,9.120000000000001,293.0,350.0,113.03999999999999,62.0,163.0,0.0,30.36,222.03,453.0,157.0,409.0,363.0,318.8,0.0,124.0,394.0,286.0,170.0,0.0,183.0,142.0,190.8,75.0,355.0,0.0,206.0,144.0,284.0,70.0,179.0,0.0,60.0,656.0,528.0,178.48,103.0,736.0,189.0,0.0,137.68,63.0,24.0,182.0,0.0,222.0,214.0,237.04,444.0,0.0,349.0,50.0,168.96,95.0,182.0,239.0,301.0,5.15,0.0,108.4,87.96,212.0,0.0,178.0,62.0,87.0,1.0,0.0,280.0,68.0,258.0,179.4,1.0,366.8,281.98,76.46,180.0,0.0,0.0,0.0,0.0,307.98,90.0,325.0,155.0,0.0,0.0,286.0,141.0,0.0,217.60000000000002,6011.6,18410.0,0.0,397.0,0.0,26.0,106.0,377.0,365.0,0.0,81.75,206.0,128.0,411.24,316.2,231.0,663.0,237.32,75.0,224.0,267.0,310.0,0.0,0.0,11.0,139.0,255.0,157.0,0.0,0.0,178.0,80.0,61.56,54.0,250.9,239.0,25.0,111.0,1371.4399999999998,901.0,27.0,304.0,279.0,192.0,401.0,220.88,36.0,99.0,8.0,29.0,43.0,0.0,98.0,328.0,249.0,252.0,151.0,634.0,42.0,412.52,54.0,84.0,159.6,0.0,300.0,141.39,335.19,147.0,542.0,151.0,157.96,0.0,0.0,277.0,626.0,39.0,397.57,202.0,45050.0,0.0,832.0,226.0,233.04,291.0,328.57,0.0,166.06,0.0,1.0,0.0,2.0,0.0,0.0,0.0,17.0,205.0,1390.0,177.0,113.0,275.0,274.0,431.18,0.0,354.0,486.65,0.0,12780.0,0.0,0.0,0.0,7.0,0.0,355.0,386.59,49.0,0.0,265.0,192.0,265.94,241.0,262.02,472.88,389.0,77.0,60.0,167.0,593.0,0.0,291.97,0.0,244.0,442.0,256.03999999999996,171.0,340.0,0.0,323.0,203.0,212.04,504.0,209.2,10.0,99.0,29.0,108.0,146.0,538.0,8.0,158.98000000000002,187.38,380.0,143.0,397.0,388.0,0.0,0.0,123.0,116.0,68.0,242.0,98.0,40.0,165.0,643.2,514.0,0.0,54.0,106.0,74.98,0.0,981.56,312.0,377.0,47.0,205.95999999999998,410.0,0.0,751.0,63.6,136.0,113.0,30.0,94.0,914.73,82.0,94.0,372.0,179.0,334.0,610.97,128.0,129.0,0.0,108.02000000000001,455.0,145.0,148.0,0.0,110.0,293.06,95.0,247.26,392.0,74.0,326.0,444.0,10124.0,255.0,0.0,85.98,197.5,213.0,292.0,704.0,517.0,236.0,208.0,135.0,0.0,479.0,224.0,130.4,18.0,145.0,250.03,18.05,204.0,145.0,1375.0,0.0,281.0,267.02,381.0,198.0,275.0,294.0,329.43,155.0,103.0,315.0,5.0,0.0,0.0,140.0,196.0,53.7,0.0,0.0,285.07,2272.2,388.0,76.0,443.64000000000004,13.0,112.0,128.0,122.0,62.06,110.0,665.0,237.01999999999998,349.0,103.0,26.0,418.0,275.0,45.0,0.0,160.0,0.0,0.0,335.0,89.0,16.04,818.0,401.0,363.03,264.98,113.0,16.0,1344.0,291.0,0.0,29.22,0.0,300.4,70.0,0.0,0.0,239.76,75.0,243.0,298.0,412.0,0.0,150.0,36.0,416.06,218.85,38.0,204.0,237.56,0.0,137.0,1087.0,0.0,58.0,491.92,0.0,117.0,285.0,219.99,2.0,113.0,444.64,150.0,256.8,0.0,105.60000000000001,58.0,67.35,385.0,214.0,305.93,72.0,132.4,54510.0,456.0,241.0,0.0,347.0,0.0,861.0,322.0,0.0,309.0,0.0,402.0,171.0,11.4,0.0,0.0,187.0,695.0,149.0,110.4,127.0,37.02,217.0,0.0,12.11,235.0,45.0,6.0,0.0,0.0,58.0,308.0,0.0,46.03,0.0,181.79999999999998,0.0,0.0,264.0,0.0,339.0,16.0,334.82,386.8,279.28000000000003,77.0,0.0,157.0,0.0,132.6,236.0,100.0,231.0,0.0,0.0,173.0,81.6,49.0,254.39999999999998,0.0,0.0,185.0,593.0,42210.0,11.22,36.0,111.96000000000001,669.02,244.0,412.0,180.0,29.0,8.8,267.84000000000003,296.4,267.0,236.36,172.0,272.0,108.0,223.0,0.0,312.0,42.0,0.0,139.0,53460.0,67.78,45.019999999999996,0.0,0.0,518.95,131.0,86.0,474.1,0.0,184.0,785.0,30.0,197.98,94.02,0.0,0.0,0.0,0.0,279.36,299.0,6347.0,239.0,350.0,19.0,289.0,186.96,0.0,0.0,231.0,119.96,0.0,52.0,43.0,0.0,32343.0,206.0,5.0,4438.0,400.0,325.0,187.98000000000002,271.4,342.0,222.4,159.0,0.0,314.0,233.0,205.32999999999998,51.64,219.0,0.0,0.0,435.0,485.0,0.0,298.95,398.0,225.0,125.0,217.08,0.0,402.0,0.0,0.0,464.0,118.0,135.0,0.0,105.0,260.0,144.8,16.0,304.0,189.0,62.03,186.3,39.0,237.03,0.0,50.0,237.0,98.0,0.0,0.0,220.0,0.0,256.0,265.0,368.8,0.0,228.0,0.0,0.0,410.0,83.99,0.0,0.0,140.43,234.0,163.0,0.0,348.94,71.0,473.0,25.0,38.480000000000004,0.0,553.76,61.96000000000001,81.0,818.0,385.0,212.0,5.0,175.0,222.0,0.0,79.0,171.0,8.0,292.0,0.0,259.06,0.0,739.6999999999999,1.0,602.0,1000.02,161.0,261.0,7.0,121.0,109.95,198.0,0.0,149.0,195.0,6.0,0.0,14.0,10.0,0.0,219.68,142.0,0.0,178.0,215.0,0.0,288.0,53.0,249.13,353.04,277.0,108.0,300.95,134.24,0.0,259.59000000000003,33.96,0.0,86.0,442.0,364.0,120.93,0.0,558.98,104.0,52.0,134.0,137.0,227.0,116.0,499.0,56.0,174.0,68.0,10.0,87.0,175.0,753.0,49.0,1725.9900000000002,512.38,0.0,225.0,259.0,399.0,0.0,227.0,139.06,244.0,350.0,7.0,222.0,15670.0,0.0,115.0,0.0,88.0,177.0,72.0,301.4,305.0,546.0,0.0,172.0,181.0,219.0,0.0,0.0,206.0,236.0,166.0,33.0,0.0,35.55,280.0,382.0,1.0,98.0,99.19999999999999,296.6,0.0,0.0,0.0,15.0,64.0,359.03000000000003,0.0,21.66,152.32999999999998,523.0,90.16,399.0,2.19,0.0,163.0,0.0,417.0,195.0,5.4,0.0,0.0,64.0,0.0,35.24,43.0,5.0,390.0,0.0,385.0,151.0,252.95999999999998,27.0,22.0,35.96,169.0,63.0,251.0,51.4,522.0,152.0,0.0,477.0,312.84000000000003,408.65999999999997,217.0,490.0,189.4,2683.0,265.01,0.0,814.0,244.0,0.0,182.0,233.0,447.04,46.0,395.0,115.19999999999999,144.0,129.0,0.0,21.0,0.0,199.0,231.88,146.0,351.61,0.0,149.0,89.87,119.0,253.0,47.0,317.0,0.0,835.0,423.0,40.36,705.4399999999999,113.0,258.0,58.0,0.0,120.0,432.03999999999996,0.0,14.0,167.0,556.0,1490.2,0.0,0.0,418.03000000000003,3426.0,0.0,5.0,302.6,35.0,121.0,275.0,0.0,209.0,38.0,9.0,89.0,1.98,248.0,176.0,37.0,193.0,202.16,228.03000000000003,7010.0,94.0,0.0,35.0,3.0,27.0,615.1999999999999,50.0,5.0,141.0,253.0,0.0,0.0,73.0,269.0,352.63,459.64,173.0,356.0,411.0,292.0,52.0,809.0,257.0,244.0,0.0,155.0,165.0,150.0,0.0,259.0,280.0,0.0,237.72,550.0,142.0,0.0,771.84,317.0,0.0,341.6,92.0,379.44,138.0,464.99,35.0,20.0,21.0,0.0,1828.0,0.0,0.0,51.0,0.35,107.0,367.44,24390.0,19.4,173.0,602.34,275.0,304.34000000000003,333.0,78.0,0.0,148.0,253.0,0.0,237.0,126.0,1256.0,82.68,305.0,140.0,197.42000000000002,502.17,0.0,73.98,57.0,453.0,2756.0,32.0,289.0,143.0,0.0,122.0,232.0,0.0,0.0,4.0,246.0,14.0,68.0,180.95,145.6,165.0,208.47,15.01,0.0,122.24,0.0,159.01999999999998,332.0,0.0,152.0,227.0,106.0,1054.0,0.0,65.0,7.0,146.0,189.0,117.0,0.0,0.0,300.0,7.2,262.0,167.0,0.0,0.0,279.0,11.18,133.0,84.67,321.0,596.5,918.0,255.0,118.32,22.0,45.01,44.1,0.0,577.0,119.0,76.0,382.0,289.0,144.0,49.0,34.4,196.0,0.0,403.20000000000005,141.01999999999998,72.0,10.0,562.0,359.0,112.0,476.0,0.0,240.0,0.0,200.0,208.77,341.0,321.6,218.0,0.0,0.0,0.0,74.0,558.0,279.0,0.0,68.0,0.0,142.0,272.0,235.6,0.0,75.0,294.0,112.80000000000001,292.0,251.32,2.0,22.0,203.0,445.0,718.0,0.0,0.0,253.0,0.0,0.0,236.0,105.0,158.0,370.0,0.0,178.0,0.0,246.0,220.0,339.0,633.0,264.0,455.0,720.0,840.0,84.0,278.0,524.0,0.0,29.0,3220.0,75.0,259.0,0.0,295.59,479.0,290.0,304.4,118.0,0.0,96.0,0.0,84.0,186.01,132.0,139.0,251.0,340.0,126.0,0.0,10.0,76.0,189.0,87.7,468.0,1.0,173.0,235.24,450.0,140.0,422.0,0.0,209.0,100.0,0.0,146.64000000000001,0.0,195.8,386.0,329.0,31.0,228.24,169.0,242.0,713.0,365.0,18.0,37.800000000000004,41.0,194.96,21.0,140.0,731.0,203.0,2.98,156.94,302.0,298.65999999999997,182.0,246.0,0.0,378.49,204.0,72.0,294.03,58.24,163.0,0.0,132.0,59.33,139.0,303.0,145.0,0.0,0.0,22.0,221.0,0.0,10.6,165.0,234.8,332.0,453.0,144.0,0.0,363.0,197.0,0.0,0.0,952.71,247.0,121.4,445.0,401.0,284.0,27.0,331.0,62.0,198.02,43.0,0.0,52.0,468.0,31.0,71.03999999999999,487.0,333.6,0.0,1353.8799999999999,683.21,505.0,7.03,410.56,45.98,0.0,0.0,135.26,0.0,0.0,134.96,0.0,265.0,334.5,0.0,606.76,102.48,0.0,0.0,112.21000000000001,0.0,334.08,27.26,165.16,159.2,454.0,193.88,310.0,0.0,0.0,0.0,0.0,318.92,0.0,0.0,0.0,64.0,0.0,447.24,119.89999999999999,610.0,170.0,717.98,732.0,7.0,0.0,28.8,0.0,1.0,300.0,574.0,489.4,40.0,391.0,2156.0,8427.0,271.2,82.16,755.85,4020.28,0.0,1541.0,87.34,814.0,384.0,215.0,712.2,295.5,24792.18,371.04,73.06,3945.0,668.68,470.0,3918.0,315.0,18816.93,291.15,5993.21,292.08,677.0,417.0,150.0,931.74,2409.0,481.0,0.0,74.24,0.0,382.0,181.0,494.15999999999997,0.0,300.0,0.0,753.0,2.0,3757.0,320.0,0.0,448.0,65.0,0.0,19.200000000000003,94.0,283.96,0.0,0.0,0.0,0.0,295.0,404.03000000000003,0.0,0.0,0.0,0.0,0.0,833.15,6371.15,0.0,109.10000000000001,20.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-07,277.0,6.99,0.0,390.0,218.25,0.0,325.4,52.0,0.0,265.0,10.01,0.0,59.0,3.0,141.0,133.0,382.69,458.0,622.0,238.0,5.0,0.0,48.0,405.72,224.0,322.0,0.0,8.0,66.0,263.0,356.4,95.67,0.0,3875.0,0.0,0.0,4592.0,62.07,425.0,0.0,530.0,49.0,11132.66,375.04,68.57000000000001,85.0,141.0,3392.0,198.2,0.0,0.0,53.0,248.0,21.0,0.0,258.2,335.0,398.05,275.8,116.35000000000001,348.0,429.0,0.0,0.0,232.29,77.0,29.0,0.0,583.0,81.0,113.0,1800.0,0.0,33.12,0.0,49.0,0.0,109.0,359.76,0.0,29.0,21.009999999999998,137.0,24.0,304.0,0.0,85.07,218.0,109.0,264.0,0.0,102.0,209.0,786.4,216.07000000000002,328.0,1035.0,116.0,117.0,218.0,551.7,0.0,426.02,92.0,221.0,189.0,182.36,191.0,468.0,251.01,252.0,9780.0,337.32,0.0,411.0,211.0,0.0,0.0,477.2,254.0,1335.6,341.0,10750.0,20.0,728.0,807.32,378.0,549.0,253.0,319.0,145.35,58.66,352.0,1284.0,485.0,0.0,0.0,92.0,0.0,187.68,69.0,505.0,81.0,77.0,40.0,0.0,284.8,219.0,645.0,0.0,166.55,0.0,361.0,300.0,43.4,0.0,9.99,229.0,42.0,0.0,197.0,318.0,108.64,99.0,145.0,29.0,187.0,316.0,235.0,579.0,180.0,170.68,645.0,0.0,317.0,456.98,0.0,205.0,146.0,319.0,170.66,244.0,138.0,275.0,195.0,0.0,258.0,330.0,0.0,50.4,602.0,286.0,0.0,0.0,355.55,424.0,154.0,54.0,47.0,0.0,223.0,211.0,1.0,0.0,50.0,413.0,0.0,68.0,181.0,0.0,326.36,165.0,87.0,240.0,296.0,43.02,507.0,239.0,15647.94,0.0,70.98,210.0,267.0,104.0,327.0,349.0,50.4,219.0,129.0,179.0,44.0,0.0,464.0,0.0,0.0,235.0,1.0,0.0,9.2,69.0,172.01,232.09,0.0,0.0,63.0,295.0,185.0,135.0,11660.0,350.0,150.76,294.8,297.0,43.06,727.0,301.0,17.0,287.0,313.0,197.0,112.56,207.05,103.0,578.44,251.0,0.0,0.0,0.0,757.0,84.11,183.4,140.38,0.0,5.0,73.4,225.51999999999998,149.0,124.0,511.95,0.0,277.0,57.0,363.0,0.0,0.0,32.8,1.0,0.0,156.0,101.0,17.0,299.0,30570.0,351.19,43.0,362.0,41.0,54.0,288.76,0.0,138.0,121.0,180.0,376.0,101.0,186.0,0.0,82.0,122.0,0.0,184.0,0.0,0.0,0.0,37.0,160.0,0.0,61.0,297.0,426.0,0.0,120.0,0.0,0.0,151.0,173.96,0.0,105.0,5.0,317.0,2.0,371.35,192.8,65.2,1964.0,5913.0,69.0,0.0,225.2,211.0,196.0,0.0,448.03,0.0,202.0,0.0,200.0,0.0,24.99,656.0,0.0,134.24,353.0,373.54,0.0,4409.0,0.0,74.0,7.0,838.0,0.0,3.8000000000000003,43.03,147.0,72.0,160.0,56.0,153.14,128.0,328.0,0.0,0.0,109.0,0.0,14220.0,1840.0,0.0,0.0,54.0,116.0,49.0,173.0,9.0,156.0,21.0,4.0,169.68,82.45,0.0,498.0,305.0,100.0,580.0,211.01999999999998,17.0,645.0,164.0,0.0,2.0,206.2,457.0,142.0,386.0,0.0,0.0,853.0,622.43,386.0,321.0,95.0,180.61,0.0,293.0,556.0,0.0,317.88,136.0,527.2,0.0,137.0,0.0,552.0,216.0,116.0,119.00999999999999,0.0,0.0,0.0,183.0,0.0,491.0,321.0,115.41000000000001,191.0,43.0,56.0,838.0,682.0,0.0,523.0,432.0,0.0,446.0,103.0,430.0,190.0,113.0,268.0,0.0,0.0,0.0,112.00999999999999,261.0,163.1,660.52,0.0,177.0,0.0,291.0,212.0,191.0,0.0,0.0,0.0,220.0,184.0,0.0,879.0,369.0,464.0,11.35,41.0,232.14000000000001,0.0,1470.0,111.0,0.0,0.0,0.0,133.0,18.53,10.0,1.0,167.0,35.0,57.0,16.0,110.2,57.0,0.0,38.0,241.0,202.0,0.0,11.0,86.0,261.01,130.0,835.0,181.41,149.54999999999998,215.17999999999998,286.0,85.95,96.0,286.0,142.57,241.0,332.99,279.0,245.36,165.0,300.4,172.0,0.0,0.0,1719.0,773.0,163.0,0.0,75.0,130.0,60.0,154.0,3.98,784.56,0.0,287.0,303.0,0.0,199.0,479.0,69.0,312.0,10.0,0.0,81.0,0.0,68.0,145.0,490.58000000000004,358.0,132.44,2.97,213.0,0.0,125.0,0.0,115.0,0.0,317.0,48.0,327.0,0.0,219.0,133.0,164.0,0.0,145.24,350.0,102.24,111.0,156.0,356.0,619.0,0.0,694.0,63.919999999999995,373.0,158.0,106.0,0.0,277.0,101.0,90.95,502.0,179.0,0.0,9.2,74.0,275.04,0.0,816.15,193.01999999999998,79.0,339.0,17.0,97.0,782.8,135.0,68.0,577.0,0.0,291.0,122.0,0.0,120.0,560.0,47.0,203.0,40.0,90.0,63.0,210.0,164.0,0.0,170.64,252.0,332.0,229.43,92.0,28.0,38.0,296.0,183.0,270.78,62.0,88.0,285.25,250.0,251.56,122.0,0.0,78.0,0.0,0.0,0.0,0.0,681.0,202.0,0.0,190.0,2449.0,172.0,412.03000000000003,308.0,0.0,784.36,0.0,184.0,412.86,389.0,0.0,93.0,138.0,168.64,0.0,458.0,30.0,340.0,253.0,393.0,242.0,54.0,388.43,0.0,174.0,597.0,5412.0,0.0,333.0,0.0,95.0,261.0,0.0,103.0,72.69,71.6,464.0,254.0,22.3,0.0,102.0,0.0,0.0,155.0,0.0,163.0,343.4,0.0,99.0,3.0,2.0,325.0,0.0,508.8,128.0,242.0,241.0,97.0,181.0,420.2,370.0,328.0,295.0,718.0,207.0,0.0,692.0,102.0,219.96999999999997,2.98,148.0,0.0,0.0,148.0,69.0,185.0,47.0,0.0,103.0,82.0,0.0,183.44,111.8,120.0,207.21,422.0,64.0,138.0,355.0,0.0,0.0,49.24,391.0,0.0,94.0,0.0,211.0,0.0,206.64,294.0,245.0,130.0,84.0,113.4,183.0,0.0,177.6,489.0,288.0,84.0,61.0,237.0,204.0,182.0,373.0,54.0,604.0,23.03,520.0,37.0,146.0,405.0,0.0,80.0,481.01,479.0,482.0,170.0,175.0,374.0,0.0,170.0,256.0,0.0,39.0,361.0,261.0,442.0,11046.0,351.0,2.0,170.97,491.0,0.0,66.97,0.0,93.0,0.0,160.0,114.0,0.0,177.0,3283.0,435.0,215.0,108.0,68.0,280.4,280.0,130.0,0.0,424.0,28.01,0.0,688.0,0.0,274.96,1068.0,168.0,112.0,596.6,75.0,314.0,554.0,82.0,0.0,0.0,494.0,355.0,276.8,1512.41,0.0,337.0,0.0,157.99,200.0,153.0,160.99,242.6,8.0,120.0,92.0,0.0,0.0,138.0,252.0,337.0,29.0,774.0,241.0,112.46000000000001,0.0,234.0,243.0,135.0,0.0,269.0,310.0,161.0,340.0,634.0,179.32,182.0,254.0,0.0,0.0,111.0,71.0,145.74,352.0,316.0,371.0,0.0,90.0,1551.01,190.4,5.0,522.0,369.5,41.0,452.88,114.4,201.0,214.0,248.6,215.8,0.0,196.0,445.0,145.0,0.0,95.0,212.0,196.68,191.8,7494.0,412.33,256.52,0.0,51.0,11.0,221.0,0.0,0.0,274.0,452.52,269.0,1672.0,0.0,62.0,86.19,123.0,143.0,369.8,5.54,115.0,202.0,0.0,308.0,0.0,113.0,401.02,270.0,403.0,164.97,38.0,30.0,1.0,0.0,294.0,18.96,0.0,0.0,70.0,424.08000000000004,0.0,0.0,18.400000000000002,8.99,189.0,72.0,383.0,189.0,155.0,92.0,85.0,2.0,0.0,482.0,0.0,161.0,0.0,88.0,310.0,232.79999999999998,0.0,0.0,178.0,182.0,247.0,1.0,37.0,175.0,275.03999999999996,425.0,124.8,345.0,224.6,0.0,8.0,86.82000000000001,299.13,379.0,287.0,393.0,220.0,0.0,10.05,140.0,167.0,273.0,0.0,832.0,165.68,0.0,0.0,2445.0,405.59999999999997,287.0,430.0,37.0,0.0,83.0,98.0,51.0,0.0,70.0,113.0,0.0,0.0,339.0,278.56,436.43,0.0,83.0,144.0,190.0,1.61,315.0,0.0,268.0,318.2,187.0,608.0,0.0,0.0,132.0,9.0,329.0,288.0,175.0,397.0,178.0,568.0,0.0,238.0,186.0,4.0,186.0,165.98000000000002,11.0,0.0,0.0,401.0,418.0,215.0,0.0,872.97,0.0,87.0,187.14,0.0,310.0,0.0,691.0,347.0,24.0,9.0,46.010000000000005,58.0,295.0,8.0,15542.869999999999,139.0,64.0,300.0,10.0,95.0,489.0,5.0,224.0,4288.0,0.0,258.0,171.0,0.0,0.0,178.0,210.0,163.0,63.0,31.0,232.0,188.0,243.0,802.0,23.8,111.0,132.0,162.0,42.0,0.0,0.0,309.0,78.11,37.33,31.92,0.0,358.0,874.0,0.0,59.66,683.0,261.66,260.0,116.0,187.02,34.0,271.0,432.0,263.05,52.0,34.0,68.0,86.24000000000001,1280.0,577.24,577.0,130.32,191.0,116.0,29.0,3.0,139.0,394.8,0.0,314.0,91.0,67.0,454.0,408.02,146.0,171.0,76.0,137.0,109.1,0.0,112.0,220.0,47.0,244.0,166.6,60.0,266.0,42.04,476.03,0.0,0.0,0.0,153.4,137.0,305.0,71.2,2547.0,156.0,150.0,99.4,23.93,3.0,392.0,147.06,0.0,472.0,0.0,0.0,178.46,0.0,372.0,21220.0,0.0,80.0,232.8,300.0,0.0,178.0,0.0,440.0,0.0,160.0,127.04,0.0,1372.95,1270.0,46.0,40.0,133.22,437.96,148.27,206.0,0.0,172.0,308.0,129.0,43.0,69.0,93.0,46.0,0.0,0.0,72.0,70.0,823.0,450.99,185.0,195.0,40.0,0.0,19.0,30.0,111.0,0.0,82.33,267.8,164.0,314.0,0.0,30.0,131.0,79.0,174.0,214.94,0.0,259.0,156.2,0.0,196.24,62.980000000000004,970.0,370.0,319.0,21756.0,185.0,296.0,587.0,284.0,76.0,128.0,146.6,0.0,264.0,232.0,322.22,0.0,103.0,114.2,9870.0,980.99,217.0,4.0,153.54,0.0,439.8,85.0,0.0,204.0,182.01999999999998,165.52,380.0,0.0,425.0,1.0,0.0,179.0,219.0,0.0,1388.94,5.0,185.0,240.0,8.2,30.0,207.97,0.0,321.0,98.0,1367.71,282.0,330.0,97.32,46.0,112.0,0.0,158.4,253.01,339.2,186.0,235.0,373.0,89.99000000000001,0.0,237.0,635.0,287.02,163.0,0.0,323.0,131.0,150.20000000000002,62.0,245.0,1.6600000000000001,148.0,121.0,388.0,232.0,143.0,0.0,411.0,179.0,175.4,326.55,98.0,334.0,110.0,0.0,106.83,74.0,16.0,227.96,0.0,111.0,326.0,177.9,396.0,0.0,110.0,103.0,171.0,208.0,316.0,135.0,359.0,20.0,0.0,118.01,88.03,162.0,0.0,151.0,264.0,40.0,0.0,0.0,366.0,80.0,39.0,295.6,0.0,122.0,289.0,76.64,166.0,0.0,0.0,0.0,0.0,308.0,120.0,331.0,67.88,0.0,0.0,316.0,142.0,0.0,337.0,5664.08,25220.0,0.0,315.0,0.0,21.0,146.0,321.0,287.0,0.0,101.0,201.0,125.0,443.68,219.8,331.0,887.0,241.79,101.0,425.0,201.0,314.98,0.0,0.0,68.0,77.0,7076.0,252.0,0.0,0.0,347.0,109.0,44.03,59.0,314.4,242.0,31.96,110.0,538.03,415.0,39.0,135.68,217.0,295.0,497.0,117.50999999999999,7.0,26.0,4.0,19.0,67.0,11.0,468.94,277.0,272.0,181.0,104.0,566.0,70.0,274.56,121.0,31.0,123.97,0.0,179.0,133.42000000000002,180.0,110.0,475.0,159.0,130.69,0.0,0.0,212.0,135.0,305.0,306.42,150.03,45200.0,0.0,957.0,270.0,366.38,371.0,214.4,0.0,210.02,0.0,1.0,0.0,1.0,0.0,0.0,0.0,66.0,154.0,1489.0,270.0,207.0,201.0,408.0,209.0,0.0,313.0,591.3199999999999,0.0,11210.0,0.0,0.0,0.0,38.0,0.0,350.0,309.8,233.2,0.0,77.0,224.0,286.94,24.0,217.0,294.8,173.26,77.0,123.0,146.0,578.0,0.0,449.0,0.0,376.0,185.0,410.0,161.0,456.0,13.0,264.0,187.0,246.68,384.0,136.81,10.96,191.0,22.0,73.0,177.0,476.0,6.0,119.0,-582.34,319.0,152.0,526.0,41.0,0.0,0.0,170.56,367.0,94.0,170.0,193.0,98.0,279.0,643.2,341.0,0.0,50.0,319.0,161.0,0.0,1023.1,438.0,378.0,67.0,231.0,399.0,0.0,734.0,14.0,324.0,140.0,26.0,137.0,1231.34,95.0,42.0,350.0,184.0,240.0,218.98,100.0,273.0,0.0,135.0,186.0,324.0,240.0,0.0,73.0,314.88,222.0,171.65,365.0,126.0,355.0,297.0,0.0,155.0,0.0,189.92,91.28999999999999,166.0,180.0,620.0,137.0,177.0,148.8,86.0,0.0,347.0,111.0,35.0,58.0,404.0,142.0,0.0,212.0,61.97,1440.0,0.0,282.0,161.0,235.0,213.0,260.3,353.0,315.58,212.0,213.0,273.0,0.0,0.0,0.0,224.0,114.0,43.76,46.0,0.0,206.61,1784.0,336.0,44.0,671.32,9.0,101.0,51.0,97.0,56.0,208.0,473.0,329.0,420.0,75.0,100.0,94.0,233.0,63.0,0.0,233.0,0.0,0.0,282.0,192.0,16.93,472.0,457.0,423.52,158.01,158.0,16.01,12.6,189.0,0.0,24.11,0.0,200.0,0.0,0.0,1.0,519.36,58.03,337.0,421.0,458.0,0.0,242.0,93.0,270.01,196.32,57.0,139.0,301.19,0.0,234.0,1198.0,0.0,34.0,227.22,0.0,85.0,284.0,223.0,1.0,144.0,156.76,180.8,203.2,0.0,105.60000000000001,48.0,105.0,445.6,267.0,234.92,31.0,126.74,42020.0,350.96,333.0,0.0,159.0,0.0,512.0,279.0,0.0,326.0,0.0,286.0,178.0,2.05,0.0,0.0,392.0,564.0,167.0,45.6,114.0,39.3,242.0,0.0,31.0,153.0,45.0,39.0,0.0,0.0,68.0,475.0,0.0,62.00000000000001,0.0,136.28,0.0,0.0,216.98,0.0,425.0,0.0,384.20000000000005,416.2,364.08,185.0,0.0,112.0,0.0,74.0,169.0,138.0,344.0,0.0,0.0,204.0,23.669999999999998,46.0,283.0,0.0,0.0,126.0,687.0,32050.0,11.76,68.12,130.2,707.0,344.0,298.97,99.0,235.0,108.2,220.12,447.0,219.0,57.42,151.0,38.0,82.0,168.0,0.0,388.0,19.0,0.0,254.0,13910.0,46.91,31.0,0.0,0.0,262.02,176.0,113.0,258.8,0.0,91.0,691.0,67.0,72.0,102.0,0.0,0.0,53.0,0.0,391.66999999999996,298.0,6235.0,544.99,328.0,96.0,160.0,56.0,0.0,0.0,191.0,98.08,0.0,94.0,24.49,0.0,32836.0,539.0,13.0,4807.0,498.03000000000003,201.98,154.0,94.0,320.0,381.6,244.56,0.0,217.0,493.0,243.99,163.0,159.0,0.0,0.0,673.0,498.0,305.6,320.0,337.8,136.0,82.0,114.01,0.0,1004.0,0.0,0.0,467.0,51.0,285.0,5.0,131.0,259.0,54.2,64.0,111.0,204.0,58.0,160.02,65.2,145.4,0.0,68.0,217.0,112.0,0.0,0.0,320.0,0.0,290.0,187.0,714.2,0.0,273.0,0.0,0.0,354.54,85.97,0.0,0.0,276.58,244.0,161.0,0.0,340.0,328.0,405.0,49.0,84.56,0.0,481.5,0.0,37.0,647.0,263.0,108.0,11.0,245.0,339.0,0.0,94.00999999999999,337.0,0.0,503.56,0.0,600.0,0.0,1108.04,0.0,498.0,1688.96,189.0,280.0,4.0,72.0,-239.24,149.0,0.0,78.0,187.0,0.0,6.0,147.0,0.0,0.0,330.09,238.0,0.0,268.0,172.0,22.0,304.0,35.0,228.66,425.07000000000005,259.0,185.0,378.0,71.10000000000001,0.0,376.0,63.0,-1.0,47.0,155.0,494.0,139.4,0.0,269.14,156.0,115.0,117.0,287.0,101.0,70.97999999999999,383.0,161.99,216.0,96.0,74.0,118.0,252.0,1007.0,27.01,0.0,350.88,0.0,66.0,291.0,431.0,0.0,127.0,108.88,353.78,251.0,0.0,170.02,13320.0,0.0,99.0,1.0,48.3,228.0,70.96000000000001,232.4,175.0,462.0,0.0,124.0,240.0,89.0,0.0,0.0,193.0,193.0,262.0,95.0,0.0,39.44,325.0,491.0,1.0,91.0,78.0,241.4,0.0,0.0,0.0,17.0,70.0,224.60000000000002,0.0,35.2,37.0,629.0,53.0,542.0,3.0,0.0,163.0,0.0,48.0,288.0,5.6,0.0,0.0,70.0,0.0,27.24,46.0,0.0,1282.0,0.0,253.0,70.95,176.0,158.0,0.0,69.05,156.0,142.0,109.0,31.6,568.0,334.0,1.0,386.0,308.52,380.01,380.0,163.0,170.0,2.0,140.0,0.0,658.0,148.0,0.0,311.0,98.0,107.0,0.0,329.0,62.8,212.0,246.0,0.0,33.0,0.0,267.0,319.0,44.0,344.64,0.0,236.0,134.14000000000001,23.0,350.0,95.0,409.0,316.0,911.0,191.0,60.72,457.8,72.0,242.0,218.0,0.0,122.0,226.97,0.0,47.0,89.0,174.0,1066.0,0.0,5.0,338.0,8762.0,0.0,65.0,567.4,1.0,26.0,260.0,0.0,132.0,25.0,32.0,151.0,17.0,291.98,182.8,62.0,210.0,132.8,229.04999999999998,6080.0,156.0,0.0,70.0,12.0,95.0,478.04,114.0,1.0,243.0,264.0,199.0,0.0,383.0,235.0,102.28,358.8,289.0,130.0,330.0,106.02,152.32,596.0,384.0,281.0,0.0,213.0,129.0,128.0,0.0,395.0,214.0,0.99,219.36,715.0,108.2,0.0,118.41,284.0,0.0,208.4,45.4,206.98,228.03,0.0,6.0,17.0,87.0,0.0,1924.04,0.0,0.0,44.0,0.0,21.0,329.87,5330.0,16.33,233.0,762.99,207.0,251.42,302.0,120.23,0.0,287.0,271.0,0.0,141.0,277.0,371.0,23.0,110.0,116.0,306.41,571.48,0.0,196.0,41.0,287.0,2989.0,52.0,997.0,308.0,6.0,120.0,182.0,0.0,0.0,4.0,180.52,0.0,107.01,320.0,104.4,151.0,364.68,0.0,0.0,148.44,0.0,439.0,117.98,1.0,366.0,332.0,262.0,1268.0,0.0,95.0,4.0,26.0,224.0,72.0,0.0,0.0,162.0,7.0200000000000005,515.98,168.0,0.0,0.0,241.0,15.83,71.0,234.34,216.0,561.44,1586.0,243.0,217.59,20.0,110.0,31.4,0.0,404.0,108.0,206.0,717.0,199.0,144.0,75.0,122.8,166.0,0.0,411.32,155.72,95.0,17.0,549.0,312.4,92.6,646.0,0.0,211.0,0.0,305.0,305.04,172.0,290.34999999999997,208.0,0.0,0.0,0.0,72.0,586.0,142.0,0.0,97.0,0.0,158.0,162.0,60.0,0.0,35.0,271.0,108.10000000000001,572.0,118.68,0.0,41.0,165.0,268.0,591.0,0.0,0.0,291.02,0.0,0.0,293.2,101.0,141.0,365.0,0.0,116.2,0.0,94.0,210.0,141.0,604.0,331.0,528.0,678.0,714.56,107.0,322.0,376.0,0.0,24.0,1940.0,204.0,419.0,0.0,259.28,497.0,217.0,217.0,209.0,0.0,115.0,0.0,45.0,116.85,120.0,175.0,115.0,373.0,0.0,0.0,0.0,0.0,49.0,18.29,418.32,1.0,153.0,187.0,198.58,198.0,204.96,0.0,172.0,130.0,0.0,226.57,0.0,224.14999999999998,263.0,271.0,35.0,158.06,225.99,111.0,836.0,129.8,0.0,36.480000000000004,154.0,273.0,14.0,117.00999999999999,749.96,171.0,4.0,218.95,758.0,261.0,273.8,246.0,0.0,288.26,196.98000000000002,88.0,395.66,82.2,377.0,0.0,190.0,83.03,37.0,94.0,219.97,0.0,0.0,10.0,258.0,0.0,61.0,296.02,277.2,297.0,283.0,63.0,0.0,667.0,364.0,0.0,0.0,569.83,269.0,249.66,503.0,321.0,226.0,68.0,525.0,110.0,156.96,9.0,0.0,86.0,702.0,15.0,71.03999999999999,379.0,333.6,0.0,0.0,724.08,327.0,4.08,268.21999999999997,58.08,0.0,0.0,134.88,0.0,0.0,0.0,0.0,223.0,355.68,0.0,136.32,180.25,0.0,0.0,80.64,0.0,334.08,31.200000000000003,22.84,83.8,464.0,78.72,231.0,0.0,0.0,0.0,0.0,303.48,0.0,0.0,0.0,0.0,0.0,323.8,122.01,265.68,205.76,618.0,472.0,7.99,0.0,163.2,0.0,2.86,345.0,655.0,467.8,179.0,151.8,25.4,0.0,445.29999999999995,47.97,153.2,15537.56,0.0,1770.0,188.32999999999998,644.0,297.0,118.0,860.8,211.18,6393.32,702.88,61.019999999999996,4100.0,598.3,420.0,0.0,518.0,8253.08,330.84999999999997,7354.8,292.08,580.0,269.96999999999997,757.0,183.64,4552.0,486.4,39.0,0.0,0.0,566.0,211.0,494.15999999999997,0.0,577.0,0.0,363.0,0.0,4811.0,340.0,0.0,247.8,244.0,0.0,12.799999999999999,121.0,205.44,0.0,2319.0,0.0,0.0,277.99,509.06,0.0,0.0,0.0,0.0,0.0,49.0,4785.66,0.0,196.0,0.0,66.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-08,193.0,0.0,0.0,419.0,429.25,0.0,178.0,82.0,1.0,85.0,13.030000000000001,0.0,654.0,6.0,32.0,0.0,384.69,473.0,917.0,195.0,6.0,0.0,47.0,300.98,145.0,306.0,0.0,0.0,87.0,0.0,184.6,89.96,0.0,9269.0,0.0,0.0,4109.8,45.0,159.0,0.0,326.0,77.0,16523.28,143.6,72.08,43.0,216.0,5831.0,180.26,6.0,86.0,117.0,261.0,36.0,0.0,311.2,395.0,379.0,202.0,24.0,300.0,247.0,0.0,0.0,299.76,46.0,16.07,0.0,587.0,85.0,72.0,1102.0,0.0,27.7,0.0,438.0,0.0,86.0,359.76,0.0,0.0,12.88,218.01,61.0,221.0,0.0,58.56,117.0,125.0,385.0,0.0,359.0,68.0,425.0,149.0,328.0,767.0,104.0,393.0,189.0,741.29,0.0,425.0,164.0,201.0,248.0,171.66,163.0,365.0,185.0,179.0,9490.0,310.24,0.0,294.0,252.0,0.0,0.0,404.8,59.03,726.0,298.0,5355.0,6.0,564.0,653.88,212.0,358.0,248.0,265.0,170.01,62.980000000000004,331.0,464.0,592.0,0.0,0.0,94.0,0.0,14.39,93.0,324.0,24.0,144.0,70.0,0.0,358.85,331.0,1009.0,0.0,47.8,0.0,169.01,299.0,94.0,0.0,49.0,164.0,49.0,0.0,143.0,172.0,173.98000000000002,189.0,87.0,46.0,359.0,239.0,84.0,291.0,239.0,188.81,686.0500000000001,0.0,214.0,391.99,0.0,150.0,203.0,396.0,208.8,230.0,234.0,619.0,194.98000000000002,0.0,183.0,445.0,6.0,95.6,330.0,248.0,0.0,0.0,290.52,191.0,215.0,47.0,75.0,0.0,135.0,225.04,0.0,0.0,24.05,337.0,0.0,172.0,292.0,0.0,345.44,131.4,228.0,259.0,67.0,97.0,254.0,170.0,21760.0,0.0,198.99,59.0,202.0,183.96,271.68,265.0,57.599999999999994,44.0,316.0,156.0,134.0,0.0,324.0,0.0,0.0,191.0,97.0,0.0,5.0,146.0,277.0,175.92,0.0,0.0,58.0,270.0,47.0,86.0,13220.0,136.98000000000002,177.0,281.0,217.2,66.0,703.04,192.0,37.0,104.0,433.0,150.0,107.63000000000001,110.94,91.0,254.32,147.62,0.0,0.0,0.0,632.0,78.0,91.0,126.46,0.0,15.0,52.6,245.0,186.10000000000002,17.0,594.0,0.0,407.0,71.21000000000001,327.0,0.0,3.64,29.0,2.0,0.0,155.0,128.0,5.0,568.0,75830.0,457.0,5.0,559.0,75.0,0.0,131.0,0.0,184.0,130.0,222.0,279.0,180.0,153.0,61.0,208.4,44.0,0.0,232.0,0.0,0.0,0.0,80.0,122.0,0.0,180.0,419.0,536.0,0.0,235.0,0.0,0.0,113.0,181.04,0.0,206.0,6.0,195.0,2.0,304.7,231.16,86.8,2072.0,6017.0,38.0,0.0,46.0,186.0,63.0,10.98,475.02,0.0,145.0,0.0,181.0,0.0,134.0,475.0,13.0,152.87,354.0,540.2,0.0,5538.0,0.0,245.0,8.0,821.0,0.0,0.0,39.0,219.0,74.0,414.0,3.42,169.82999999999998,237.0,339.0,0.0,0.0,143.0,0.0,21680.0,7840.0,0.0,49.0,249.0,37.0,0.0,169.0,0.0,174.0,199.0,173.0,293.31,72.19,0.0,352.0,268.0,94.0,336.0,120.0,14.0,354.0,218.0,0.0,6.0,434.8,654.0,220.0,313.0,0.0,0.0,301.0,497.35,314.8,181.0,0.0,183.0,0.0,199.0,412.0,0.0,247.95999999999998,165.0,437.8,0.0,166.0,0.0,369.0,198.0,242.0,108.0,0.0,0.0,0.0,437.0,0.0,333.0,98.0,195.62,155.0,40.0,1.0,997.0,670.0,0.0,326.0,284.0,0.0,395.0,111.0,314.78000000000003,200.0,71.0,189.0,1.0,0.0,0.0,158.0,222.0,165.81,627.44,0.0,124.0,0.0,320.0,193.34,123.94,0.0,0.0,0.0,398.0,189.0,9.6,746.0,141.0,436.0,12.24,51.0,264.0,0.0,650.0,228.0,0.0,0.0,0.0,124.0,21.0,38.0,6.0,162.0,87.0,56.980000000000004,15.0,124.8,84.0,0.0,98.0,283.0,230.0,0.0,6.0,134.0,185.0,169.0,417.0,244.6,199.61,240.04,96.0,128.6,45.0,102.0,289.0,241.01,212.98,426.0,146.67,194.96,242.0,141.0,0.0,141.0,648.0,321.0,139.0,0.0,99.0,378.0,36.0,118.0,236.8,691.96,0.0,195.0,558.0,0.0,228.0,266.0,46.97,304.0,25.0,0.0,276.0,3828.0,76.0,215.0,562.03,214.0,213.0,32.65,132.0,0.0,28.0,0.0,26.0,0.0,98.0,50.0,276.0,0.0,189.0,135.6,245.0,0.0,62.78,607.0,150.79999999999998,97.0,340.0,547.0,1005.0,201.0,260.0,56.0,262.0,89.0,209.0,0.0,236.82999999999998,123.0,6.0,351.97,310.0,0.0,11.0,79.0,261.2,0.0,1794.84,219.92,42.11,348.0,1.0,138.0,841.94,100.0,71.92,617.0,13.0,162.0,145.0,0.0,230.0,406.0,31.0,205.0,24.0,180.0,98.0,40.0,248.0,0.0,129.29,336.0,260.0,272.64,232.0,58.0,58.0,184.0,315.0,147.0,108.0,244.0,181.03,54.0,360.8,200.0,0.0,131.0,0.0,0.0,0.0,0.0,1559.0,181.0,0.0,190.0,20.0,141.0,307.0,132.0,0.0,312.7,0.0,208.0,351.95,278.0,6.0,95.0,171.5,53.0,0.0,232.0,66.0,358.0,167.0,266.0,233.0,57.0,327.78999999999996,0.0,211.0,464.0,7370.0,0.0,354.0,0.0,104.0,304.0,0.0,159.0,64.16,158.4,325.0,327.0,33.71,0.0,211.0,6.0,0.0,69.0,35.0,198.0,172.6,16.0,205.0,3.0,0.0,260.0,0.0,360.01,114.0,343.0,196.0,284.0,132.0,560.8,237.0,262.0,401.0,1265.99,182.0,0.0,389.0,162.0,314.61,60.0,42.0,0.0,0.0,148.0,55.0,235.0,0.0,0.0,239.0,146.0,0.0,163.54000000000002,172.0,70.0,202.4,134.01,102.0,180.0,473.0,1.0,6.0,27.76,543.0,0.0,186.0,0.0,293.0,0.0,150.29999999999998,100.0,227.0,514.0,88.0,584.6,41.0,0.0,279.0,421.0,132.0,647.0,28.0,313.0,172.0,136.0,404.0,84.0,520.01,47.97,408.0,208.0,189.64,409.0,0.0,228.0,249.0,262.0,452.0,317.0,418.0,396.0,0.0,178.0,233.0,0.0,207.0,241.69,269.0,450.0,10213.0,236.6,17.0,266.0,651.0,0.0,195.4,0.0,433.0,51.0,222.0,417.0,0.0,148.0,2941.0,418.0,302.0,238.0,168.0,61.2,167.0,139.0,8.0,475.0,51.0,0.0,1677.0,0.0,242.0,492.0,499.0,57.0,679.4,52.0,82.0,436.0,47.96,0.0,6.0,497.04,259.1,236.24,24918.01,0.0,370.0,0.0,193.0,92.0,44.0,257.0,257.2,19.0,95.0,141.0,0.0,26.0,60.0,256.0,483.02,37.0,845.0,90.99,36.989999999999995,0.0,204.0,101.0,223.0,0.0,210.0,408.0,205.0,318.0,430.0,148.97,197.0,431.0,0.0,16.0,300.0,62.0,268.8,207.0,106.0,259.0,0.0,86.25,1361.0,301.45,5.0,241.0,309.6,70.0,304.11,160.6,256.0,505.0,327.4,251.2,0.0,133.0,270.0,305.0,0.0,200.0,248.0,87.67999999999999,306.2,0.0,385.77,387.4,0.0,181.0,0.0,463.01,0.0,0.0,134.0,613.46,166.0,1766.2,0.0,106.0,202.8,116.0,153.0,232.2,2.44,133.0,172.0,0.0,148.0,1.0,28.0,383.92,208.0,172.0,446.0,27.0,20.0,0.0,0.0,502.0,19.04,9.0,0.0,305.0,541.64,0.0,0.0,37.96,7.0,149.2,66.4,405.0,755.0,86.0,88.0,154.0,2.0,0.0,396.0,0.0,220.0,0.0,177.0,285.0,193.08,0.0,0.0,138.0,172.2,90.0,5.0,43.0,63.0,132.4,696.03,139.2,313.0,373.4,0.0,104.0,17.2,392.93,400.0,215.0,398.0,247.88,0.0,0.0,136.0,387.0,169.0,0.0,1220.63,281.34000000000003,0.0,0.0,4792.0,369.02,198.0,37.55,93.0,0.0,122.0,117.0,231.0,0.0,13.0,97.0,15.0,0.0,139.0,259.0,376.64,73.0,119.0,910.0,57.0,0.0,453.0,0.0,181.0,185.8,90.0,588.0,0.0,0.0,147.0,20.4,114.0,258.0,134.0,450.0,75.0,694.0,0.0,316.0,351.0,4.0,214.0,123.96,4.0,0.0,0.0,181.0,262.0,111.0,0.0,335.15999999999997,0.0,66.0,433.6,7.0,151.0,0.0,626.0,63.0,27.0,40.01,54.019999999999996,131.0,450.8,9.0,15540.0,352.0,580.97,495.0,3.0,129.0,391.0,0.0,372.0,415.0,0.0,125.0,231.0,0.0,0.0,132.0,126.0,50.0,78.0,129.0,335.0,177.0,284.41,746.0,428.0,34.0,182.0,123.0,78.0,0.0,0.0,369.0,240.88,35.37,31.92,0.0,306.0,729.0,0.0,191.71,301.0,348.36,261.0,42.0,268.0,4018.0,139.0,455.0,246.0,40.0,49.0,125.0,185.0,8122.0,356.0,479.0,131.66,132.0,167.0,27.0,2.0,220.55,416.2,0.0,225.0,80.0,196.0,215.0,405.08,195.0,252.0,112.0,132.6,114.96,35.0,239.0,128.0,185.0,181.0,149.0,49.0,184.0,63.0,563.4,0.0,0.0,0.0,226.32,106.0,309.0,43.76,2152.0,286.0,91.0,35.6,7.64,1897.0,477.0,198.0,0.0,237.0,0.0,0.0,140.56,0.0,422.0,22450.0,0.0,106.0,225.66000000000003,374.0,0.0,60.0,0.0,391.0,0.0,336.0,380.64,0.0,1055.0,1279.0,68.0,98.0,95.08,369.0,170.7,217.0,0.0,290.02,320.0,120.0,75.0,70.0,105.0,56.0,0.0,0.0,126.01,48.0,812.0,731.01,57.0,224.0,30.0,0.0,16.0,0.0,150.0,0.0,31.92,206.98000000000002,212.0,463.8,0.0,43.08,89.0,82.0,275.0,377.04,0.0,212.20000000000002,25.0,24.0,582.96,52.76,537.0,354.4,291.0,23188.0,321.0,349.0,473.0,458.0,320.0,122.0,155.23000000000002,0.0,256.0,78.0,382.0,0.0,152.0,54.0,46913.34,723.6,223.0,5.0,268.44,0.0,492.2,49.2,0.0,192.0,221.99,111.42,239.0,288.0,339.0,0.0,0.0,262.0,208.0,0.0,1602.91,51.0,196.0,118.03999999999999,33.8,69.0,301.2,0.0,297.0,159.0,3608.3999999999996,170.0,385.53999999999996,101.8,20.0,119.0,0.0,76.0,145.02,508.8,101.0,343.98,412.0,111.0,0.0,153.0,380.0,425.0,120.0,0.0,373.96000000000004,257.0,137.99,69.0,251.0,6.2700000000000005,184.6,157.99,411.0,29.0,166.0,0.0,157.0,218.0,293.59999999999997,167.32,118.0,0.0,209.0,0.0,222.58,65.0,0.0,405.76,24.0,187.0,462.0,173.28,304.0,0.0,0.0,164.0,180.4,194.0,149.0,169.0,189.0,4.78,308.0,134.0,45.0,116.0,0.0,81.0,152.0,53.0,0.0,0.0,524.0,70.6,98.67999999999999,221.0,22.0,229.0,189.98999999999998,149.34,97.0,0.0,0.0,0.0,0.0,271.37,156.0,236.98,109.11,0.0,0.0,274.0,47.0,0.0,235.6,7134.48,19910.0,1.0,299.0,0.0,96.0,112.0,390.0,247.0,0.99,158.88,459.0,39.0,550.0,578.0,212.0,715.96,253.98000000000002,74.0,370.0,279.0,285.0,0.0,0.0,14.0,359.0,258.0,205.0,0.0,0.0,361.0,140.0,112.77000000000001,178.0,314.4,280.0,19.0,123.0,1041.04,343.0,44.0,205.34,232.0,283.0,374.01,224.45999999999998,5.0,73.0,6.0,45.0,48.0,64.0,185.0,342.99,247.0,201.0,94.0,385.0,114.0,263.0,45.4,134.0,144.0,0.0,235.0,260.03000000000003,388.0,139.0,360.0,155.0,141.35999999999999,0.0,0.0,162.0,317.0,21.0,498.05,143.0,44940.0,0.0,1231.0,224.0,484.07000000000005,90.0,535.6,0.0,231.9,0.0,1.0,0.0,3.0,0.0,113.0,0.0,8.0,245.0,1709.0,102.0,275.0,142.0,191.0,200.0,0.0,577.4,414.96000000000004,0.0,10440.0,0.0,0.0,0.0,1.0,0.0,310.0,309.96,81.02000000000001,0.0,134.0,190.0,509.18,24.0,213.0,242.0,398.81,69.0,84.0,118.0,1910.0,0.0,412.0,0.0,109.0,183.0,442.0,107.0,169.0,0.0,296.0,157.2,302.72,410.0,244.4,10.0,184.0,16.0,35.0,137.0,418.0,5.0,148.68,227.03000000000003,241.0,92.0,563.0,235.0,0.0,0.0,160.45,306.0,135.0,338.2,129.0,71.6,246.0,333.8,293.0,0.0,71.0,260.0,323.0,0.0,888.02,425.0,401.0,85.0,298.0,371.0,0.0,513.0,60.0,168.2,195.0,90.0,96.0,1123.78,76.0,62.0,324.0,191.0,321.0,122.2,91.0,193.0,0.0,119.11,221.0,235.0,119.0,0.0,129.0,40.0,89.0,65.8,148.0,138.0,212.0,192.0,10119.0,511.0,0.0,272.46,87.8,323.02,92.0,194.2,82.0,243.0,274.2,366.0,0.0,316.0,166.0,119.43,32.0,237.0,193.5,6.0,176.0,87.2,554.4,0.0,318.0,175.0,260.0,163.0,123.15,380.0,266.0,279.0,118.0,309.0,7.0,0.0,0.0,230.0,91.0,59.04,21.0,0.0,322.28,2236.0,316.0,59.0,159.56,22.0,92.0,163.0,71.0,29.02,118.0,685.0,153.0,72.0,40.0,52.0,84.0,179.0,38.0,0.0,241.0,0.0,0.0,258.0,262.52,19.0,1201.0,506.0,633.44,175.0,113.89999999999999,30.48,13.2,296.0,0.0,41.519999999999996,0.0,205.0,0.0,0.0,0.0,541.88,55.97,156.0,282.0,671.0,0.0,272.0,55.0,768.37,196.32,38.0,145.0,207.0,0.0,124.0,1197.0,0.0,72.0,267.21,0.0,99.0,202.4,248.35,2.0,140.48,301.2,185.2,218.0,0.0,291.99,39.0,88.0,411.37,262.0,326.43,54.0,199.18,67170.0,331.0,395.0,0.0,183.0,0.0,430.0,555.99,0.0,572.0,0.0,316.0,258.0,9.9,0.0,0.0,103.0,706.0,118.0,109.88,105.0,98.75999999999999,113.0,4.0,21.0,194.0,45.0,23.0,0.0,0.0,161.0,244.0,3.0,26.0,0.0,172.83,0.0,0.0,172.0,0.0,343.2,0.0,227.0,354.0,324.2,165.0,0.0,132.0,0.0,94.0,450.6,133.0,254.0,0.0,0.0,57.0,37.62,60.0,230.32,0.0,0.0,187.0,699.0,43780.0,8.97,113.11,32.84,561.0,147.0,175.54,156.0,38.0,96.0,87.12,414.0,252.0,13.57,67.0,130.0,142.0,229.0,0.0,379.0,89.0,0.0,51.0,13930.0,43.36,52.89,0.0,0.0,388.0,89.0,60.0,530.06,0.0,111.0,944.0,105.0,20.98,97.89,0.0,0.0,681.0,0.0,86.16,582.0,6640.0,395.86,237.0,238.0,129.0,160.67,0.0,0.0,258.0,130.84,0.0,39.0,34.6,0.0,32303.0,263.0,13.0,4682.0,239.0,216.76,221.96999999999997,98.0,389.0,227.0,224.48,0.0,201.0,424.0,92.0,105.9,224.0,0.0,0.0,640.0,744.0,369.6,318.0,346.2,207.0,328.0,33.04,0.0,898.0,0.0,0.0,958.0,62.0,114.0,173.0,137.0,392.0,128.0,64.0,780.0,113.0,64.0,95.96,44.959999999999994,589.6,0.0,35.0,226.0,392.0,0.0,0.0,210.0,0.0,150.0,279.0,402.0,0.0,236.0,0.0,0.0,222.18,75.0,0.0,0.0,190.01,206.0,599.0,0.0,373.77,168.0,342.0,114.0,105.0,0.0,420.0,0.0,147.0,596.0,171.0,135.0,18.0,324.0,475.0,0.0,85.0,213.12,8.0,823.46,0.0,173.0,0.0,884.4,7.0,464.0,726.0,172.0,291.0,10.0,154.0,128.20000000000002,181.0,0.0,140.0,188.0,0.0,0.0,23.0,0.0,0.0,123.06,145.0,0.0,254.0,319.4,4.0,278.0,235.0,416.0,296.16,237.0,152.0,258.0,271.02,0.0,352.0,187.0,0.0,63.0,380.0,475.0,86.24,0.0,203.04000000000002,168.0,183.0,39.0,166.0,43.0,76.0,479.0,129.0,236.0,89.0,32.0,88.0,102.0,1689.0,95.0,0.0,350.88,35.0,162.0,309.0,360.0,0.0,199.0,106.0,289.23,358.0,4.0,117.00999999999999,18070.0,0.0,195.0,0.0,134.69,126.0,270.0,182.6,234.0,603.0,0.0,195.0,269.0,81.0,0.0,0.0,322.0,303.0,90.0,53.0,0.0,35.980000000000004,216.0,49.0,2.0,132.0,73.0,332.6,0.0,0.0,0.0,7.0,38.0,302.39,183.0,15.0,54.04,294.0,27.2,125.0,4.0,0.0,224.0,0.0,147.0,126.0,92.0,0.0,0.0,70.0,0.0,110.52,79.0,0.0,475.0,0.0,277.0,156.0,208.0,105.0,2.0,72.95,52.96,141.0,178.0,24.0,729.0,141.0,0.0,275.96000000000004,270.0,375.0,344.0,268.0,184.0,2.0,104.6,0.0,467.0,172.0,0.0,225.99,191.0,83.96,0.0,299.0,77.0,206.2,287.0,0.0,13.0,0.0,169.0,113.0,38.0,290.96,0.0,153.0,26.0,19.0,413.0,26.0,85.0,229.0,968.0,272.0,122.6,565.25,189.0,284.0,77.0,0.0,137.0,101.0,0.0,133.0,259.19,408.0,1308.0,0.0,0.0,256.6,3758.0,0.0,2.0,423.0,1.0,133.0,360.0,0.0,101.0,67.0,124.0,246.0,0.0,230.0,257.2,47.0,177.0,166.10999999999999,254.0,6130.0,116.0,0.0,68.0,4.0,52.0,570.0,81.0,0.0,93.0,166.0,446.0,0.0,321.01,179.0,114.55000000000001,395.2,117.0,136.0,316.0,138.98000000000002,306.66,859.0,334.0,249.0,0.0,374.0,130.0,167.0,0.0,146.0,126.0,0.0,291.0,442.0,91.8,0.0,773.6,86.0,0.0,934.0,34.04,226.83,21.0,0.0,0.0,32.0,48.0,0.0,1901.01,0.0,0.0,28.0,0.0,42.0,285.34999999999997,22470.0,45.3,320.0,631.03,121.0,297.8,322.0,57.72,0.0,245.0,469.0,7.0,126.0,115.0,559.0,90.0,156.0,92.0,372.2,411.84000000000003,0.0,214.31,39.0,358.0,3453.0,60.98,206.0,225.0,0.0,74.97,227.46,0.0,0.0,4.0,200.42000000000002,0.0,124.0,93.0,82.0,75.0,427.68,189.0,0.0,55.33,0.0,285.0,323.0,0.0,271.0,260.0,252.0,1195.0,0.0,169.4,0.0,0.0,292.0,88.0,0.0,0.0,52.0,66.0,350.0,146.0,0.0,1.0,233.0,16.0,148.0,64.0,166.0,330.0,989.0,422.0,182.0,16.0,124.0,61.660000000000004,0.0,613.0,98.0,50.0,335.0,142.0,144.0,161.0,31.8,251.0,0.0,477.12,239.31,96.0,50.0,732.0,189.6,75.13,453.0,0.0,242.0,0.0,157.0,187.68,227.0,227.0,172.0,0.0,0.0,0.0,70.0,464.0,224.0,0.0,25.0,0.0,116.0,155.0,182.8,0.0,53.0,278.33,20.0,430.01,229.99,7.0,16.0,192.01,339.0,510.0,0.0,0.0,147.98,0.0,0.0,193.8,195.0,57.0,389.0,0.0,53.8,0.0,180.99,110.0,331.0,484.0,113.0,398.0,535.0,589.46,232.0,409.36,289.0,0.0,70.0,2800.0,115.0,279.0,0.0,315.36,340.2,224.0,196.0,244.0,0.0,231.0,0.0,253.0,356.09999999999997,299.0,101.0,191.0,556.0,0.0,0.0,0.0,186.0,155.0,309.0,639.45,21.0,176.0,152.99,246.93,194.0,294.65999999999997,0.0,207.0,71.04,0.0,196.23999999999998,0.0,304.4,431.0,212.0,113.0,279.93,209.0,308.0,717.0,128.2,6.0,26.92,254.0,261.59000000000003,30.0,176.0,694.0,276.0,4.0,97.10000000000001,391.0,449.0,489.2,162.0,0.0,591.78,175.0,40.0,248.51,56.0,215.0,0.0,257.0,59.63,249.0,182.0,191.0,0.0,0.0,13.0,198.0,0.0,100.0,152.7,164.0,395.0,175.0,168.04,0.0,546.0,96.0,0.0,0.0,663.19,181.0,198.97,387.0,346.0,226.0,17.0,200.0,50.0,225.59,132.0,0.0,38.0,336.0,25.0,71.03999999999999,481.0,333.6,0.0,2266.0,413.03,182.0,4.67,156.66,58.08,0.0,0.0,123.64,0.0,0.0,0.0,0.0,47.0,355.68,0.0,136.32,106.47,0.0,0.0,43.2,0.0,376.31,28.6,0.0,140.0,500.0,174.16,349.0,11.940000000000001,0.0,0.0,0.0,332.68,0.0,0.0,0.0,0.0,0.0,137.0,180.0,540.35,255.23000000000002,347.02,432.0,5.7,0.0,224.0,0.0,68.64,181.0,316.0,644.4,69.0,409.2,2154.6,8407.0,232.12,136.4,1020.7,2753.21,0.0,2413.0,108.61000000000001,238.0,258.0,297.0,1324.0,31.660000000000004,39919.68,713.0,77.76,3948.0,620.05,278.0,535.0,489.0,7041.11,475.65,7225.3,292.08,487.0,437.0,668.0,116.0,533.12,417.6,29.95,0.0,0.0,719.0,312.0,501.79,218.0,256.0,0.0,114.0,0.0,4289.0,278.0,0.0,367.2,188.0,0.0,42.559999999999995,91.2,153.67000000000002,0.0,189.0,0.0,0.0,223.98000000000002,680.1,0.0,0.0,206.8,0.0,0.0,0.0,7213.76,0.0,0.0,0.0,84.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
            "2019-02-09,262.0,92.06,0.0,459.0,306.0,0.0,96.0,24.0,0.0,358.0,0.0,0.0,0.0,4.0,233.0,0.0,426.03,793.04,805.0,93.0,5.0,0.0,121.0,279.28,213.0,382.0,0.0,227.0,135.0,0.0,112.0,133.13,0.0,7593.0,0.0,0.0,792.2,0.0,256.0,0.0,357.0,40.0,4539.0,234.8,176.51999999999998,77.0,247.0,5178.0,339.2,0.0,113.0,8.0,212.0,0.0,0.0,331.8,-1.0,590.0,197.0,0.0,200.0,479.89,0.0,0.0,229.77,21.0,18.0,0.0,709.0,122.0,99.0,1738.0,0.0,8.0,0.0,135.0,0.0,269.0,0.0,0.0,0.0,19.08,225.0,0.0,195.0,0.0,19.4,182.0,144.0,417.0,0.0,248.0,32.0,348.44,121.0,392.0,12.0,33.0,331.0,142.0,744.8,20.0,496.0,118.0,217.0,129.0,83.0,141.0,365.0,222.0,0.0,12110.0,360.9,0.0,406.0,186.0,0.0,0.0,239.0,184.38,520.0,417.0,0.0,0.0,51.0,408.92,286.0,673.0,307.0,170.0,424.8,118.36,272.03999999999996,383.0,406.4,0.0,0.0,77.0,0.0,11.0,459.0,414.0,15.030000000000001,71.96000000000001,123.0,0.0,743.3199999999999,62.0,1326.0,0.0,153.46,0.0,205.0,257.96000000000004,50.0,0.0,381.0,180.0,134.6,0.0,391.0,262.0,166.0,265.0,75.0,12.0,223.67,280.0,449.0,441.0,167.0,145.76,590.16,0.0,66.0,808.0,0.0,409.0,101.0,393.0,181.2,185.0,102.0,759.0,1.0,0.0,215.0,459.0,0.0,349.03999999999996,541.01,443.0,0.0,0.0,347.29,195.0,159.0,18.0,115.0,0.0,119.0,82.0,1.0,0.0,0.0,272.98,0.0,145.45,162.0,0.0,430.77,128.6,65.0,319.0,86.0,19.99,100.0,137.0,21290.0,0.0,0.0,78.0,315.0,167.62,242.34,310.0,57.599999999999994,68.0,238.0,140.0,73.0,0.0,466.0,0.0,0.0,105.0,28.0,0.0,9.0,115.0,247.03,226.56,0.0,0.0,107.0,170.0,23.0,336.0,730.0,1.0,142.0,347.92,365.8,49.33,733.0,212.15,247.0,130.0,174.0,277.0,177.4,564.0,92.0,229.22,139.6,0.0,0.0,0.0,468.0,200.96,240.0,301.15999999999997,0.0,257.0,32.0,92.0,194.8,56.0,612.0,0.0,218.0,174.8,319.6,0.0,11.440000000000001,40.0,2.0,21.0,155.0,265.0,62.0,518.0,36100.0,285.74,0.0,185.0,19.0,0.0,255.0,0.0,258.0,171.0,316.0,351.0,229.0,287.0,0.0,109.6,39.0,0.0,218.0,0.0,0.0,13.68,217.0,38.0,0.0,38.0,642.0,342.0,0.0,318.0,0.0,0.0,209.8,94.9,0.0,147.0,4.0,380.0,1.0,0.0,215.2,0.0,819.0,210.0,39.0,0.0,183.0,449.0,7.0,0.0,636.73,0.0,138.0,0.0,284.0,0.0,0.0,192.0,0.0,224.22,279.0,3059.0,0.0,1367.0,0.0,133.0,0.0,802.2,0.0,0.0,36.0,63.0,67.0,392.0,4.29,129.0,228.0,389.0,0.0,0.0,26.6,0.0,19710.0,1680.0,0.0,38.0,129.0,44.0,0.0,263.0,0.0,105.0,163.0,163.0,289.0,136.3,0.0,266.0,199.0,238.0,625.0,59.96000000000001,2.0,582.0,81.0,0.0,35.0,884.0,431.0,294.0,0.0,0.0,0.0,67.0,333.78000000000003,333.2,246.0,37.0,0.0,0.0,90.0,356.0,0.0,128.25,1.0,414.0,0.0,378.0,0.0,671.0,194.0,416.0,0.0,0.0,0.0,0.0,232.0,0.0,120.0,136.0,56.230000000000004,89.0,0.0,2.0,494.0,753.0,0.0,674.0,302.0,26.0,719.0,82.0,324.12,173.0,0.0,92.0,0.0,0.0,0.0,160.0,146.0,131.98,348.96000000000004,0.0,135.0,0.0,230.0,313.69,140.0,0.0,0.0,0.0,246.0,238.0,35.8,3.0,460.0,183.0,12.24,124.0,264.0,0.0,639.97,140.0,0.0,0.0,0.0,122.0,24.0,74.0,3.0,295.0,46.0,0.0,53.0,568.2,214.0,0.0,82.0,372.0,96.0,416.0,8.0,258.0,271.0,294.0,992.0,382.01,419.17,166.0,209.0,134.83,0.0,350.0,228.0,183.0,218.0,426.0,0.0,333.0,496.01,323.0,0.0,273.98,0.0,800.0,146.0,0.0,191.5,245.54,73.0,148.0,512.2,890.03,0.0,186.0,164.0,0.0,375.0,476.0,494.76,259.0,1.0,0.0,111.0,0.0,200.0,206.0,474.8,293.0,200.4,2.3,123.0,0.0,350.0,0.0,105.0,0.0,214.0,30.0,312.0,0.0,118.0,123.0,265.35,0.0,133.98,421.0,283.4,267.0,450.0,534.0,861.0,199.0,2.0,0.0,414.0,293.0,88.0,0.0,234.70999999999998,367.8,0.0,54.0,395.0,0.0,14.0,197.0,406.8,0.0,1953.0,41.06,39.9,430.0,1.99,115.0,1094.0,527.0,57.120000000000005,303.0,2.7800000000000002,508.0,165.0,0.0,234.0,214.0,23.0,239.0,122.0,201.0,120.0,59.0,119.0,0.0,386.12,266.0,259.0,391.32,225.0,102.0,21.0,229.0,177.57999999999998,215.60000000000002,70.0,69.0,246.0,74.0,547.68,183.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,260.0,0.0,199.0,4.0,0.0,284.0,529.0,0.0,685.5,0.0,0.0,523.9,354.0,63.4,11.0,140.29999999999998,94.8,0.0,393.0,5.0,223.0,54.0,421.0,162.0,58.0,562.14,0.0,201.0,435.0,4140.04,0.0,295.0,0.0,134.0,267.0,0.0,123.0,19.96,13.0,268.0,201.0,0.0,0.0,134.0,0.0,0.0,78.0,0.0,292.0,147.0,0.0,205.0,24.0,0.0,370.0,0.0,486.0,249.0,119.0,260.0,101.0,180.0,394.0,518.0,298.0,367.0,1568.0,261.0,0.0,403.0,83.32,18.400000000000002,37.0,0.0,47.0,0.0,247.0,103.0,88.0,50.0,0.0,652.0,41.0,0.0,207.4,395.04,47.0,0.0,185.0,64.0,162.0,87.0,1.0,0.0,37.0,460.0,0.0,90.0,0.0,326.0,0.0,56.0,217.0,838.0,194.6,293.0,236.0,352.0,0.0,7.0,445.0,304.0,1.98,358.0,605.0,143.0,221.0,443.0,52.0,320.0,46.0,370.0,103.0,142.32999999999998,369.0,0.0,149.0,168.0,526.0,1.0,227.0,70.0,428.0,0.0,3.0,219.0,0.0,298.0,557.28,261.0,269.0,2109.0,336.4,4.0,241.4,895.0,0.0,124.0,0.0,108.0,71.00999999999999,115.0,0.0,0.0,342.0,1263.0,848.0,330.0,186.0,56.0,272.81,255.0,146.0,0.0,295.0,134.96,0.0,281.0,0.0,129.03,717.0,551.0,28.0,433.0,152.0,74.0,4.0,0.0,0.0,0.0,645.0,227.88,112.0,28.8,0.0,231.2,0.0,221.0,235.0,11.0,195.98000000000002,92.6,0.0,117.0,104.0,0.0,0.0,172.0,317.0,278.79999999999995,219.0,612.0,53.0,21.0,0.0,166.0,346.0,307.0,0.0,347.0,247.62,125.0,475.0,6.0,2.64,114.0,417.0,0.0,0.0,190.0,209.0,190.0,340.0,590.0,182.0,88.0,50.98,1021.4,274.56,31.0,457.0,257.14,61.0,407.0,49.0,174.0,325.0,278.0,341.0,0.0,58.0,338.96000000000004,230.0,0.0,597.0,154.0,112.27,143.0,0.0,416.15999999999997,313.0,0.0,190.0,0.0,254.2,0.0,31.4,163.0,458.52,147.0,1304.8,0.0,147.0,228.0,0.0,175.0,408.0,1.0,154.0,82.0,0.0,201.0,0.0,198.0,80.0,248.6,423.0,113.0,7.0,17.0,4.0,981.0,402.0,5.949999999999999,0.0,0.0,134.0,635.04,0.0,0.0,150.0,7.2,105.8,90.6,165.0,76.0,128.0,26.0,4.0,2.0,0.0,589.0,0.0,357.0,0.0,135.0,263.62,108.98,0.0,0.0,165.0,146.8,376.0,5.0,39.0,0.0,102.6,304.0,153.0,353.0,463.0,0.0,22.0,0.0,363.0,405.0,339.0,63.16,243.31,0.0,0.0,204.0,331.0,220.0,0.0,720.61,379.0,0.0,0.0,659.0,411.84000000000003,168.0,73.44,134.0,0.0,1.0,109.0,89.0,0.0,0.0,122.0,0.0,0.0,297.0,171.8,381.32,286.0,171.0,222.0,267.0,0.0,223.0,0.0,381.0,6.0,169.03,0.0,0.0,0.0,47.0,9.6,134.0,231.0,155.0,339.0,165.0,535.02,0.0,263.0,156.0,3.0,274.0,7.0,20.0,0.0,0.0,270.0,144.0,95.0,0.0,539.86,0.0,127.0,341.4,0.0,103.0,0.0,519.0,345.2,31.0,105.23,19.0,171.0,3.2,10.0,280.0,158.0,289.97,140.0,52.96,186.0,270.0,82.0,466.0,0.0,0.0,257.0,170.0,0.0,0.0,166.0,142.0,28.0,209.0,19.0,208.0,148.4,313.56,1291.0,182.0,44.0,112.0,204.0,29.0,0.0,0.0,637.0,42.0,74.78,31.92,0.0,414.0,841.0,0.0,13.0,341.0,451.8,199.0,0.0,172.0,94.0,435.0,578.0,221.0,244.0,53.0,179.0,41.0,2277.0,513.98,271.0,133.0,57.0,41.0,82.0,12.0,227.44,460.2,0.0,306.0,8.0,71.65,397.0,2889.82,405.0,80.0,87.0,43.4,58.0,332.02,102.0,257.2,310.0,52.0,233.0,0.0,518.0,22.0,416.6,0.0,0.0,0.0,350.68,152.0,0.0,71.2,2787.0,168.0,463.0,167.0,96.32,2.0,788.0,288.0,0.0,443.0,0.0,0.0,0.0,0.0,345.0,14930.0,0.0,3.0,153.0,270.0,0.0,254.0,0.0,302.0,0.0,210.0,265.36,0.0,0.0,1180.0,251.0,98.0,153.96,397.0,364.0,250.0,0.0,244.0,482.0,214.0,64.0,27.0,332.0,13.0,0.0,0.0,72.0,6.0,512.0,320.0,216.97,208.0,9.0,0.0,15.0,0.0,238.0,0.0,122.89,150.5,154.0,551.2,0.0,229.75,292.48,425.0,243.0,234.37,0.0,301.8,171.95999999999998,433.0,796.28,3.23,383.0,216.6,178.0,18963.08,67.0,232.0,521.0,299.0,234.0,183.0,778.56,0.0,316.98,318.0,367.0,0.0,183.0,191.0,19886.67,832.8,440.0,6.0,0.0,0.0,116.0,197.4,0.0,312.0,285.0,227.0,608.0,0.0,392.0,0.0,0.0,160.0,-1.0,0.0,2189.94,175.0,66.0,622.0,1.0,65.0,276.91,0.0,342.66,174.0,3608.3999999999996,376.0,353.44,26.8,301.0,135.6,0.0,15.379999999999999,32.0,560.0,190.0,440.4,290.0,204.0,0.0,179.0,372.0,462.96,202.0,0.0,365.0,317.0,71.0,18.0,76.0,20.0,233.4,168.0,284.0,166.0,0.0,0.0,114.0,145.0,343.68,469.65000000000003,213.0,0.0,394.0,0.0,289.88,0.0,0.0,216.4,0.0,199.0,445.0,480.64,363.0,0.0,0.0,105.0,235.6,228.0,325.01,203.0,210.0,22.02,33.0,47.03,19.5,103.0,0.0,240.0,266.0,58.0,0.0,0.0,8.0,119.4,84.37,79.0,215.0,511.0,330.03999999999996,86.88,207.0,0.0,25.0,0.0,0.0,398.6,36.0,215.0,151.12,0.0,0.0,170.0,65.0,0.0,255.02,3871.36,5430.0,0.0,321.0,0.0,166.0,98.0,383.0,154.0,215.8,79.10000000000001,218.0,121.0,736.0,147.0,182.0,872.4,236.0,245.0,84.0,139.0,423.0,0.0,0.0,32.0,24.0,11663.0,264.0,0.0,0.0,221.0,175.0,71.12,18.0,314.4,247.0,-2.0,157.0,0.0,578.4399999999999,75.0,0.0,459.0,357.0,230.01,306.02,7.0,111.0,62.0,0.0,0.0,43.0,258.0,412.0,199.0,159.0,83.0,491.0,71.0,407.04,46.6,0.0,113.26,0.0,90.0,103.14,428.0,52.0,664.0,272.0,72.63,0.0,0.0,0.0,487.0,67.0,479.69,36.0,27100.0,0.0,828.0,229.0,399.66,678.97,339.0,0.0,118.44,0.0,163.0,0.0,3.0,0.0,0.0,0.0,68.0,208.0,1820.0,174.0,87.0,200.0,108.0,361.0,0.0,338.63,493.0,0.0,8680.0,0.0,0.0,0.0,6.0,0.0,463.0,392.0,77.2,0.0,240.0,249.0,587.76,23.0,292.0,338.0,438.0,57.0,11.0,244.0,0.0,0.0,291.98,243.0,260.0,305.0,171.0,349.0,164.0,57.0,339.0,322.8,223.8,227.0,259.6,10.0,185.0,16.0,203.0,249.0,483.0,0.0,231.2,167.0,2.0,178.0,534.0,0.0,7.0,0.0,123.77,298.0,63.0,244.79999999999998,0.0,79.19999999999999,0.0,309.84000000000003,475.0,0.0,91.0,240.0,-283.94999999999993,0.0,1375.16,651.0,284.0,42.0,173.88,578.0,0.0,660.0,195.0,121.0,77.0,139.0,0.0,1310.0,39.0,0.0,510.0,208.0,391.0,346.4,0.0,191.0,0.0,123.22,228.0,299.0,314.0,0.0,60.0,0.0,117.0,154.96,58.0,239.0,92.0,376.0,0.0,259.0,0.0,574.74,302.76,210.5,389.0,263.8,417.0,465.0,273.67,222.0,0.0,536.0,203.0,183.51999999999998,75.0,189.0,180.0,0.0,136.0,97.8,361.59999999999997,0.0,312.0,191.0,355.0,0.0,212.87,355.0,205.0,177.0,350.0,82.0,0.0,0.0,10.4,233.0,0.0,64.36,0.0,0.0,261.06,1964.0,268.0,154.0,387.39,0.0,94.0,134.4,0.0,7.0,132.01,252.0,305.0,613.0,225.0,0.0,135.0,71.0,17.0,0.0,265.0,0.0,0.0,323.0,250.54,14.040000000000001,2459.0,61.0,0.0,179.0,190.95,16.560000000000002,1307.2,369.0,0.0,29.46,0.0,205.0,0.0,0.0,14.0,501.8,1.0,357.0,212.0,524.31,0.0,134.0,77.0,600.54,234.44,1.0,178.0,256.0,0.0,168.0,1347.0,53.0,75.0,216.35,0.0,125.0,121.4,314.19,3.0,106.56,259.0,2.0,277.0,0.0,141.44,51.0,171.02,434.0,174.0,270.6,0.0,17.6,87640.0,277.0,610.0,0.0,184.0,0.0,477.6,732.0,0.0,201.0,0.0,441.0,469.0,1.0,0.0,0.0,114.0,684.0,244.0,56.11,169.0,87.93,215.0,0.0,11.0,365.0,96.0,0.0,0.0,0.0,42.0,282.0,0.0,0.0,0.0,140.5,0.0,0.0,271.0,0.0,273.8,0.0,78.1,301.0,289.90000000000003,42.0,0.0,176.0,0.0,219.0,171.08,180.0,491.0,0.0,0.0,262.0,42.72,72.0,442.64,0.0,0.0,74.0,1005.0,31919.94,9.040000000000001,126.8,65.2,318.0,217.0,170.45,194.0,7.0,69.0,32.67,241.0,386.0,10.4,0.0,0.0,48.0,204.0,0.0,514.0,40.0,0.0,250.0,14180.0,154.01,43.14,0.0,0.0,305.83000000000004,1.0,59.0,369.97,0.0,158.0,846.0,294.0,48.92,212.17,0.0,0.0,4.0,15.0,119.30000000000001,223.0,2014.0,457.64,130.0,57.58,160.0,106.34,0.0,0.0,205.0,199.0,0.0,164.0,19.71,0.0,33116.0,938.0,43.0,4034.0,466.0,166.87,213.45,75.0,391.0,371.0,259.0,0.0,199.0,434.0,511.94,118.98,346.0,0.0,0.0,362.0,420.0,369.6,49.97,56.0,71.0,197.0,0.0,0.0,279.0,0.0,0.0,0.0,119.0,194.0,7.0,107.0,267.0,172.0,154.0,1120.0,428.0,134.04,108.4,5.7299999999999995,417.0,0.0,192.0,210.0,318.0,0.0,0.0,72.0,0.0,442.0,124.0,450.0,0.0,53.0,0.0,0.0,324.32,57.0,0.0,0.0,86.78,209.0,159.0,0.0,389.02,276.0,473.0,35.0,89.0,0.0,286.93,0.0,103.0,0.0,48.0,301.0,0.0,108.0,200.0,0.0,168.2,326.92,0.0,1659.1,0.0,4.0,0.0,338.52,0.0,336.0,0.0,11.0,262.0,6.0,99.0,214.56,194.0,0.0,0.0,222.0,0.0,0.0,248.0,0.0,0.0,10.03,312.0,0.0,102.0,100.6,0.0,351.0,57.0,1484.0,405.79,302.0,249.0,210.0,158.76,0.0,558.99,171.0,0.0,177.0,228.0,390.0,194.68,0.0,319.88,252.0,192.0,136.0,222.0,184.0,149.28,496.0,191.0,268.0,251.0,45.0,115.0,168.0,694.0,125.02000000000001,0.0,538.96,138.0,172.0,457.0,375.0,0.0,0.0,67.11,357.0,420.0,0.0,238.03,21560.0,0.0,36.0,1.0,68.0,180.0,9.28,127.0,132.0,1252.0,0.0,196.0,116.0,158.0,0.0,0.0,206.0,392.0,98.0,0.0,0.0,284.0,235.0,63.0,0.0,90.0,71.0,224.4,0.0,0.0,0.0,0.0,55.0,260.02,343.0,35.0,34.94,624.0,10.8,263.0,28.0,0.0,4.0,0.0,218.0,200.0,142.0,0.0,0.0,227.0,0.0,28.46,448.0,0.0,1301.0,0.0,17.0,177.0,282.0,138.0,0.0,32.0,97.0,153.0,162.0,126.0,713.0,195.0,0.0,546.0,311.0,374.0,434.0,105.0,132.0,18.0,271.8,0.0,172.22,340.0,0.0,60.46,342.0,9.01,0.0,416.0,101.06,252.8,5.0,0.0,35.0,0.0,0.0,134.0,83.0,740.5500000000001,0.0,91.0,82.0,91.0,346.0,1.0,316.0,0.0,794.0,681.0,95.19,685.0,108.0,601.0,0.0,0.0,58.0,137.0,0.0,133.0,248.8,154.0,660.02,0.0,1.0,326.4,4407.95,0.0,0.0,163.0,2.0,117.0,88.0,0.0,141.0,23.0,92.0,225.0,182.0,385.02,123.6,32.0,262.0,53.65,300.0,2510.0,299.0,0.0,579.0,165.0,60.0,489.0,95.0,0.0,166.0,235.0,626.0,0.0,354.0,385.0,225.0,423.95000000000005,319.0,107.0,334.0,171.96,20.03,344.0,388.0,315.0,0.0,97.0,112.0,69.0,0.0,100.0,197.0,71.0,279.0,692.0,251.17,0.0,540.02,535.0,0.0,235.0,151.21,333.32,208.0,0.0,4.0,6.0,44.0,0.0,472.0,0.0,0.0,100.0,0.0,30.0,388.79999999999995,2740.0,67.0,175.0,654.02,341.0,0.0,470.0,25.2,0.0,72.0,213.0,0.0,232.0,149.0,691.0,90.0,148.0,244.0,480.79,582.9300000000001,0.0,0.66,50.0,718.0,2414.0,102.0,79.0,305.0,0.0,119.0,188.35999999999999,0.0,0.0,7.0,213.0,0.0,114.12,45.0,199.68,153.0,567.0,30.0,0.0,102.0,0.0,173.91,343.59,0.0,139.0,294.0,136.0,127.0,0.0,230.6,0.0,20.0,413.0,134.0,0.0,0.0,183.0,9.0,255.0,235.0,0.0,72.0,270.0,14.0,147.0,522.03,150.0,1426.31,998.0,356.0,166.43,17.0,154.0,26.4,0.0,741.0,0.0,135.0,855.0,338.0,115.0,73.0,13.0,246.0,0.0,609.36,333.0,65.0,6.03,587.0,137.0,161.2,679.0,0.0,434.0,0.0,223.0,187.68,279.0,371.0,280.0,0.0,0.0,0.0,205.0,372.0,265.0,0.0,212.0,0.0,312.0,112.0,239.2,0.0,50.0,468.44,31.99,263.0,107.0,0.0,9.0,175.4,271.0,502.0,0.0,0.0,250.96,0.0,0.0,211.0,128.0,136.0,309.0,0.0,144.0,18.0,86.0,331.0,213.0,604.0,334.0,413.0,746.0,640.0,193.0,483.68,503.0,0.0,12.0,1470.0,123.0,499.0,0.0,148.0,470.8,82.0,293.54,164.0,0.0,89.0,0.0,89.0,352.99,331.0,92.0,187.0,185.0,0.0,0.0,0.0,62.0,212.0,50.0,227.23,154.0,326.0,200.1,335.9,227.0,529.4,0.0,118.0,428.0,0.0,213.89,0.0,10.0,308.0,262.0,70.0,301.0,466.0,170.0,719.0,465.0,0.0,30.48,106.0,295.42,24.0,0.0,439.0,257.0,3.0,0.0,378.0,438.96999999999997,461.0,318.0,0.0,887.0,121.94000000000001,0.0,306.8,119.0,195.44,0.0,249.0,25.33,116.0,155.0,60.03,0.0,0.0,70.0,143.0,0.0,149.0,2.2,174.0,229.0,230.0,19.0,0.0,309.0,373.0,0.0,0.0,0.0,218.0,160.2,244.0,315.8,305.0,144.0,156.0,174.0,285.21,89.0,0.0,30.0,83.6,0.0,43.19,550.0,308.3,0.0,0.0,335.5,620.0,3.38,389.04,33.879999999999995,85.8,0.0,116.63,0.0,0.0,0.0,0.0,54.0,291.67,0.0,136.32,318.14,0.0,0.0,43.2,0.0,410.64,85.01,0.0,306.0,349.0,297.22999999999996,370.0,118.56,0.0,0.0,0.0,304.32,0.0,0.0,0.0,0.0,0.0,373.52,153.0,153.0,57.0,318.0,444.0,4.3100000000000005,0.0,42.37,0.0,68.64,213.0,442.0,210.0,124.0,348.96999999999997,24.0,0.0,407.42,208.79999999999998,246.64999999999998,6295.0,0.0,3671.92,162.69,83.0,591.0,249.0,1616.0,345.36,8077.96,365.98,182.17000000000002,197.0,615.68,232.0,0.0,342.0,6298.88,2458.26,9961.189999999999,292.08,570.0,376.03,875.0,460.0,664.9,401.0,0.0,0.0,0.0,4.0,270.0,48.38,0.0,673.0,0.0,58.0,62.0,4787.9,301.0,0.0,210.0,167.8,0.0,5.22,444.8,264.12,0.0,2324.0,0.0,0.0,573.0,8.4,0.0,0.0,194.35,0.0,0.0,0.0,140.4,0.0,0.0,0.0,122.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,48.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a comenzar importando los datos preprocesados."
      ],
      "metadata": {
        "id": "kNorqzONJy_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import dataset\n",
        "data = pd.read_csv(f\"{ROOT_PATH}/Modelar_UH2022_preprocess_pivot.txt\",\n",
        "                   sep=\",\",\n",
        "                   dtype={\n",
        "                       \"SAMPLETIME\": 'str',\n",
        "                   })\n",
        "data[\"SAMPLETIME\"] = pd.to_datetime(data[\"SAMPLETIME\"], format=\"%Y-%m-%d %H:%M\")"
      ],
      "metadata": {
        "id": "yUvRHXfm0v04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "FCmrialJDfMK",
        "outputId": "009b5f8c-11b3-4308-9642-627d63b6158a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  SAMPLETIME      0     1    2      3       4       5       6      7    8  \\\n",
              "0 2019-02-01  243.0   8.0  0.0  492.0  247.56  442.98   80.00   45.0  4.0   \n",
              "1 2019-02-02  236.0  47.0  0.0  381.0  235.68    0.00   11.00   36.0  0.0   \n",
              "2 2019-02-03  335.0   6.0  0.0  313.0  254.35    0.00   69.00  426.0  0.0   \n",
              "3 2019-02-04  252.0  12.0  0.0  362.0  412.00    0.00  269.98  433.0  7.0   \n",
              "4 2019-02-05  220.0  44.0  0.0  380.0  269.00    0.00  226.96   63.0  0.0   \n",
              "\n",
              "   ...  2739  2742  2743  2744  2745  2746  2747  2748  2749  2756  \n",
              "0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "1  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "2  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "3  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "4  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 2748 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-600d55a0-381f-4755-8ed1-7545c84e3397\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2739</th>\n",
              "      <th>2742</th>\n",
              "      <th>2743</th>\n",
              "      <th>2744</th>\n",
              "      <th>2745</th>\n",
              "      <th>2746</th>\n",
              "      <th>2747</th>\n",
              "      <th>2748</th>\n",
              "      <th>2749</th>\n",
              "      <th>2756</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-02-01</td>\n",
              "      <td>243.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>492.0</td>\n",
              "      <td>247.56</td>\n",
              "      <td>442.98</td>\n",
              "      <td>80.00</td>\n",
              "      <td>45.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-02-02</td>\n",
              "      <td>236.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>381.0</td>\n",
              "      <td>235.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-02-03</td>\n",
              "      <td>335.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>313.0</td>\n",
              "      <td>254.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>69.00</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-02-04</td>\n",
              "      <td>252.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>362.0</td>\n",
              "      <td>412.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>269.98</td>\n",
              "      <td>433.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>220.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>269.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>226.96</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2748 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-600d55a0-381f-4755-8ed1-7545c84e3397')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-600d55a0-381f-4755-8ed1-7545c84e3397 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-600d55a0-381f-4755-8ed1-7545c84e3397');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "pRVrXHKbaic5",
        "outputId": "a28419d8-7cc4-431b-ec65-eec51a07d668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    SAMPLETIME      0      1     2      3       4    5      6      7       8  \\\n",
              "360 2020-01-27  213.0   4.00  39.0  289.0  263.60  0.0  204.0  118.0  887.00   \n",
              "361 2020-01-28  232.0   1.00  40.0  380.0  303.40  0.0  169.0  135.0   12.00   \n",
              "362 2020-01-29  403.0   1.98  45.0  404.0  421.00  0.0  195.0   77.0  472.00   \n",
              "363 2020-01-30  425.0  10.00  27.0  339.0  420.00  0.0  152.0   39.0    0.00   \n",
              "364 2020-01-31  255.0  32.95  59.0  391.0  253.98  0.0  210.0   38.0  332.14   \n",
              "\n",
              "     ...  2739  2742  2743  2744  2745  2746  2747  2748  2749  2756  \n",
              "360  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "361  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "362  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "363  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "364  ...  15.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 2748 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc9ad658-d1ce-4b43-92eb-74db15555e37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLETIME</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2739</th>\n",
              "      <th>2742</th>\n",
              "      <th>2743</th>\n",
              "      <th>2744</th>\n",
              "      <th>2745</th>\n",
              "      <th>2746</th>\n",
              "      <th>2747</th>\n",
              "      <th>2748</th>\n",
              "      <th>2749</th>\n",
              "      <th>2756</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>2020-01-27</td>\n",
              "      <td>213.0</td>\n",
              "      <td>4.00</td>\n",
              "      <td>39.0</td>\n",
              "      <td>289.0</td>\n",
              "      <td>263.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>204.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>887.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>2020-01-28</td>\n",
              "      <td>232.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>40.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>303.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>12.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>2020-01-29</td>\n",
              "      <td>403.0</td>\n",
              "      <td>1.98</td>\n",
              "      <td>45.0</td>\n",
              "      <td>404.0</td>\n",
              "      <td>421.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>472.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>2020-01-30</td>\n",
              "      <td>425.0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>27.0</td>\n",
              "      <td>339.0</td>\n",
              "      <td>420.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>255.0</td>\n",
              "      <td>32.95</td>\n",
              "      <td>59.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>253.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>332.14</td>\n",
              "      <td>...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2748 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc9ad658-d1ce-4b43-92eb-74db15555e37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bc9ad658-d1ce-4b43-92eb-74db15555e37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bc9ad658-d1ce-4b43-92eb-74db15555e37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, tranformamos nuestro DataFrame a una matrix de Numpy"
      ],
      "metadata": {
        "id": "bJpzPxMXFyVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = data.drop(\"SAMPLETIME\", axis=1).to_numpy()\n",
        "data_np"
      ],
      "metadata": {
        "id": "221K82ObDhqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aecedf2-b230-43c5-fb32-ace087ac1a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[243.  ,   8.  ,   0.  , ...,   0.  ,   0.  ,   0.  ],\n",
              "       [236.  ,  47.  ,   0.  , ...,   0.  ,   0.  ,   0.  ],\n",
              "       [335.  ,   6.  ,   0.  , ...,   0.  ,   0.  ,   0.  ],\n",
              "       ...,\n",
              "       [403.  ,   1.98,  45.  , ...,   0.  ,   0.  ,   0.  ],\n",
              "       [425.  ,  10.  ,  27.  , ...,   0.  ,   0.  ,   0.  ],\n",
              "       [255.  ,  32.95,  59.  , ...,   0.  ,   0.  ,   0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente que haremos es escalar nuestros datos"
      ],
      "metadata": {
        "id": "a5XIM1U-F5C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_np_scaled = scaler.fit_transform(data_np)"
      ],
      "metadata": {
        "id": "_dN1OSB7dC0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos dar formato a nuestros datos para entrenar una red recurrente. Para ello:\n",
        "\n",
        "*   X contendrá los valores de delta para cada contador, `loopback`días antes.\n",
        "*   y contendrá los valores de delta para cada contador, los siete días posteriores, n el valor de delta para dicha semana completa, y la siguiente semana.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kkhcr1ShF-zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_sequences(sequences, lookback):\n",
        "    X, y = list(), list()\n",
        "\n",
        "    for i in range(len(sequences)):\n",
        "        end_idx = i + lookback\n",
        "\n",
        "        out_first_week_idx = end_idx + 7\n",
        "        out_second_week_idx = end_idx + 14\n",
        "\n",
        "        if out_second_week_idx > len(sequences):\n",
        "            break\n",
        "\n",
        "        seq_x = sequences[i:end_idx, :]\n",
        "        X.append(seq_x)\n",
        "\n",
        "        first_week = sequences[end_idx:out_first_week_idx, :]\n",
        "        second_week = sequences[end_idx:out_second_week_idx, :]\n",
        "\n",
        "        first_week_sum = np.sum(first_week, axis=0)\n",
        "        second_week_sum = np.sum(second_week, axis=0)\n",
        "        \n",
        "        seq_y = np.vstack([first_week, first_week_sum, second_week_sum])\n",
        "\n",
        "        y.append(seq_y)\n",
        "          \n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "zui3T9TutxRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 3\n",
        "X, y = split_sequences(data_np_scaled, lookback)"
      ],
      "metadata": {
        "id": "MH1ztIv3vWPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "\tprint(X[i], y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K957h9ZGviSA",
        "outputId": "b3686438-c623-49c0-8ca4-5c5bfd47ad68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.39836066 0.02787456 0.         ... 0.         0.         0.        ]\n",
            " [0.38688525 0.16376307 0.         ... 0.         0.         0.        ]\n",
            " [0.54918033 0.02090592 0.         ... 0.         0.         0.        ]] [[0.41311475 0.04181185 0.         ... 0.         0.         0.        ]\n",
            " [0.36065574 0.1533101  0.         ... 0.         0.         0.        ]\n",
            " [0.45245902 0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.51639344 0.21602787 0.         ... 0.         0.         0.        ]\n",
            " [2.94262295 0.75627178 0.         ... 0.         0.         0.        ]\n",
            " [6.30327869 1.111777   0.12068966 ... 0.         0.         0.        ]]\n",
            "[[0.38688525 0.16376307 0.         ... 0.         0.         0.        ]\n",
            " [0.54918033 0.02090592 0.         ... 0.         0.         0.        ]\n",
            " [0.41311475 0.04181185 0.         ... 0.         0.         0.        ]] [[0.36065574 0.1533101  0.         ... 0.         0.         0.        ]\n",
            " [0.45245902 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.45409836 0.0243554  0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.36721311 0.04209059 0.         ... 0.         0.         0.        ]\n",
            " [2.89672131 0.75655052 0.         ... 0.         0.         0.        ]\n",
            " [6.4147541  1.06996516 0.12068966 ... 0.         0.         0.        ]]\n",
            "[[0.54918033 0.02090592 0.         ... 0.         0.         0.        ]\n",
            " [0.41311475 0.04181185 0.         ... 0.         0.         0.        ]\n",
            " [0.36065574 0.1533101  0.         ... 0.         0.         0.        ]] [[0.45245902 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.45409836 0.0243554  0.         ... 0.         0.         0.        ]\n",
            " [0.31639344 0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.55245902 0.07651568 0.         ... 0.         0.         0.        ]\n",
            " [3.08852459 0.6797561  0.         ... 0.         0.         0.        ]\n",
            " [6.72622951 0.91665505 0.12068966 ... 0.         0.         0.        ]]\n",
            "[[0.41311475 0.04181185 0.         ... 0.         0.         0.        ]\n",
            " [0.36065574 0.1533101  0.         ... 0.         0.         0.        ]\n",
            " [0.45245902 0.         0.         ... 0.         0.         0.        ]] [[0.45409836 0.0243554  0.         ... 0.         0.         0.        ]\n",
            " [0.31639344 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.4295082  0.32076655 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.35901639 0.0174216  0.         ... 0.         0.         0.        ]\n",
            " [2.99508197 0.6971777  0.         ... 0.         0.         0.        ]\n",
            " [6.69836066 0.94452962 0.18965517 ... 0.         0.         0.        ]]\n",
            "[[0.36065574 0.1533101  0.         ... 0.         0.         0.        ]\n",
            " [0.45245902 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.45409836 0.0243554  0.         ... 0.         0.         0.        ]] [[0.31639344 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.4295082  0.32076655 0.         ... 0.         0.         0.        ]\n",
            " [0.51639344 0.21602787 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.43278689 0.         0.         ... 0.         0.         0.        ]\n",
            " [2.97377049 0.6728223  0.         ... 0.         0.         0.        ]\n",
            " [6.75737705 0.92017422 0.18965517 ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6mr3rhSwy9G",
        "outputId": "b69d7b99-d7d1-4a0a-bbed-e16442dc78e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(349, 3, 2747)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifzChPmZ3Me8",
        "outputId": "1de7f40f-b456-4b92-bc3d-655b47757e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(349, 9, 2747)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a comenzar entrenando un modelo simple de red LSTM."
      ],
      "metadata": {
        "id": "6RwKIKWTHjEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "n_features = X.shape[2]\n",
        "n_steps = y.shape[1]\n",
        "n_series = y.shape[2]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', input_shape=(lookback, n_features)))\n",
        "model.add(RepeatVector(n_steps))\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(n_series, activation='relu')))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ION6ih2XwAv4",
        "outputId": "5c2767a0-13fe-4eb4-d8d7-0d9de7a744fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/400\n",
            "11/11 [==============================] - 5s 56ms/step - loss: 1.7022\n",
            "Epoch 2/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.0595\n",
            "Epoch 3/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7725\n",
            "Epoch 4/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6081\n",
            "Epoch 5/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.5062\n",
            "Epoch 6/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4676\n",
            "Epoch 7/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4398\n",
            "Epoch 8/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4117\n",
            "Epoch 9/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3676\n",
            "Epoch 10/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3196\n",
            "Epoch 11/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.2911\n",
            "Epoch 12/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.2764\n",
            "Epoch 13/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.2639\n",
            "Epoch 14/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.2475\n",
            "Epoch 15/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.2343\n",
            "Epoch 16/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.2222\n",
            "Epoch 17/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.2125\n",
            "Epoch 18/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.2054\n",
            "Epoch 19/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.2015\n",
            "Epoch 20/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1967\n",
            "Epoch 21/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1922\n",
            "Epoch 22/400\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.1891\n",
            "Epoch 23/400\n",
            "11/11 [==============================] - 1s 71ms/step - loss: 0.1838\n",
            "Epoch 24/400\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.1773\n",
            "Epoch 25/400\n",
            "11/11 [==============================] - 1s 92ms/step - loss: 0.1659\n",
            "Epoch 26/400\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 0.1596\n",
            "Epoch 27/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.1530\n",
            "Epoch 28/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1487\n",
            "Epoch 29/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1421\n",
            "Epoch 30/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1373\n",
            "Epoch 31/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1320\n",
            "Epoch 32/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1321\n",
            "Epoch 33/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1301\n",
            "Epoch 34/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.1250\n",
            "Epoch 35/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1221\n",
            "Epoch 36/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1206\n",
            "Epoch 37/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.1175\n",
            "Epoch 38/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1138\n",
            "Epoch 39/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.1107\n",
            "Epoch 40/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.1079\n",
            "Epoch 41/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1056\n",
            "Epoch 42/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.1040\n",
            "Epoch 43/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.1027\n",
            "Epoch 44/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.1017\n",
            "Epoch 45/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1008\n",
            "Epoch 46/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.1010\n",
            "Epoch 47/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.1000\n",
            "Epoch 48/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.1011\n",
            "Epoch 49/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.1008\n",
            "Epoch 50/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0963\n",
            "Epoch 51/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0938\n",
            "Epoch 52/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0921\n",
            "Epoch 53/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0903\n",
            "Epoch 54/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0889\n",
            "Epoch 55/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0883\n",
            "Epoch 56/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0874\n",
            "Epoch 57/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0871\n",
            "Epoch 58/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0861\n",
            "Epoch 59/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0851\n",
            "Epoch 60/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0846\n",
            "Epoch 61/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0839\n",
            "Epoch 62/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0830\n",
            "Epoch 63/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0826\n",
            "Epoch 64/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0830\n",
            "Epoch 65/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0851\n",
            "Epoch 66/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0850\n",
            "Epoch 67/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0882\n",
            "Epoch 68/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0834\n",
            "Epoch 69/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0812\n",
            "Epoch 70/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0794\n",
            "Epoch 71/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0779\n",
            "Epoch 72/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0765\n",
            "Epoch 73/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0756\n",
            "Epoch 74/400\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0766\n",
            "Epoch 75/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0753\n",
            "Epoch 76/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0746\n",
            "Epoch 77/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0748\n",
            "Epoch 78/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0777\n",
            "Epoch 79/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0852\n",
            "Epoch 80/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0843\n",
            "Epoch 81/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0787\n",
            "Epoch 82/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0749\n",
            "Epoch 83/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0746\n",
            "Epoch 84/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0759\n",
            "Epoch 85/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0736\n",
            "Epoch 86/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0729\n",
            "Epoch 87/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0707\n",
            "Epoch 88/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0681\n",
            "Epoch 89/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0675\n",
            "Epoch 90/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0675\n",
            "Epoch 91/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0660\n",
            "Epoch 92/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0655\n",
            "Epoch 93/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0645\n",
            "Epoch 94/400\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0639\n",
            "Epoch 95/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0635\n",
            "Epoch 96/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0628\n",
            "Epoch 97/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0622\n",
            "Epoch 98/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0620\n",
            "Epoch 99/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0616\n",
            "Epoch 100/400\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0612\n",
            "Epoch 101/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0614\n",
            "Epoch 102/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0614\n",
            "Epoch 103/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0610\n",
            "Epoch 104/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0613\n",
            "Epoch 105/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0615\n",
            "Epoch 106/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0611\n",
            "Epoch 107/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0622\n",
            "Epoch 108/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0632\n",
            "Epoch 109/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0638\n",
            "Epoch 110/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0636\n",
            "Epoch 111/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0619\n",
            "Epoch 112/400\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0611\n",
            "Epoch 113/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0599\n",
            "Epoch 114/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0593\n",
            "Epoch 115/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0585\n",
            "Epoch 116/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0574\n",
            "Epoch 117/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0567\n",
            "Epoch 118/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0567\n",
            "Epoch 119/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0565\n",
            "Epoch 120/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0561\n",
            "Epoch 121/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0558\n",
            "Epoch 122/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0556\n",
            "Epoch 123/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0556\n",
            "Epoch 124/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0555\n",
            "Epoch 125/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0555\n",
            "Epoch 126/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0556\n",
            "Epoch 127/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0556\n",
            "Epoch 128/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0556\n",
            "Epoch 129/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0552\n",
            "Epoch 130/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0551\n",
            "Epoch 131/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0554\n",
            "Epoch 132/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0553\n",
            "Epoch 133/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0549\n",
            "Epoch 134/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0553\n",
            "Epoch 135/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0551\n",
            "Epoch 136/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0553\n",
            "Epoch 137/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0553\n",
            "Epoch 138/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0564\n",
            "Epoch 139/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0589\n",
            "Epoch 140/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0600\n",
            "Epoch 141/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0580\n",
            "Epoch 142/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0592\n",
            "Epoch 143/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0583\n",
            "Epoch 144/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0578\n",
            "Epoch 145/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0565\n",
            "Epoch 146/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0549\n",
            "Epoch 147/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0538\n",
            "Epoch 148/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0534\n",
            "Epoch 149/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0537\n",
            "Epoch 150/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0534\n",
            "Epoch 151/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0527\n",
            "Epoch 152/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0525\n",
            "Epoch 153/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0523\n",
            "Epoch 154/400\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0520\n",
            "Epoch 155/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0519\n",
            "Epoch 156/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0518\n",
            "Epoch 157/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0516\n",
            "Epoch 158/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0515\n",
            "Epoch 159/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0515\n",
            "Epoch 160/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0515\n",
            "Epoch 161/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0515\n",
            "Epoch 162/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0519\n",
            "Epoch 163/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0516\n",
            "Epoch 164/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0516\n",
            "Epoch 165/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0514\n",
            "Epoch 166/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0513\n",
            "Epoch 167/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0512\n",
            "Epoch 168/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0513\n",
            "Epoch 169/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0513\n",
            "Epoch 170/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0511\n",
            "Epoch 171/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0509\n",
            "Epoch 172/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0508\n",
            "Epoch 173/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0508\n",
            "Epoch 174/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0508\n",
            "Epoch 175/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0510\n",
            "Epoch 176/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0542\n",
            "Epoch 177/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0658\n",
            "Epoch 178/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0625\n",
            "Epoch 179/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0564\n",
            "Epoch 180/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0517\n",
            "Epoch 181/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0497\n",
            "Epoch 182/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0490\n",
            "Epoch 183/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0482\n",
            "Epoch 184/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0473\n",
            "Epoch 185/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0469\n",
            "Epoch 186/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0471\n",
            "Epoch 187/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0476\n",
            "Epoch 188/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0476\n",
            "Epoch 189/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0472\n",
            "Epoch 190/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0479\n",
            "Epoch 191/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0469\n",
            "Epoch 192/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0464\n",
            "Epoch 193/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0462\n",
            "Epoch 194/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0461\n",
            "Epoch 195/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0459\n",
            "Epoch 196/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0463\n",
            "Epoch 197/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0463\n",
            "Epoch 198/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0464\n",
            "Epoch 199/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0461\n",
            "Epoch 200/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0459\n",
            "Epoch 201/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0456\n",
            "Epoch 202/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0455\n",
            "Epoch 203/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0454\n",
            "Epoch 204/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0453\n",
            "Epoch 205/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0452\n",
            "Epoch 206/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0451\n",
            "Epoch 207/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0451\n",
            "Epoch 208/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0450\n",
            "Epoch 209/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0449\n",
            "Epoch 210/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0449\n",
            "Epoch 211/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0448\n",
            "Epoch 212/400\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0448\n",
            "Epoch 213/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0448\n",
            "Epoch 214/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0448\n",
            "Epoch 215/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0447\n",
            "Epoch 216/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0447\n",
            "Epoch 217/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0445\n",
            "Epoch 218/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0445\n",
            "Epoch 219/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0444\n",
            "Epoch 220/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0444\n",
            "Epoch 221/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0444\n",
            "Epoch 222/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0444\n",
            "Epoch 223/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0444\n",
            "Epoch 224/400\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0444\n",
            "Epoch 225/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0446\n",
            "Epoch 226/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0448\n",
            "Epoch 227/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0451\n",
            "Epoch 228/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0448\n",
            "Epoch 229/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0448\n",
            "Epoch 230/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0447\n",
            "Epoch 231/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0449\n",
            "Epoch 232/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0447\n",
            "Epoch 233/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0463\n",
            "Epoch 234/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0521\n",
            "Epoch 235/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0558\n",
            "Epoch 236/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0547\n",
            "Epoch 237/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0527\n",
            "Epoch 238/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0487\n",
            "Epoch 239/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0470\n",
            "Epoch 240/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0457\n",
            "Epoch 241/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0450\n",
            "Epoch 242/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0446\n",
            "Epoch 243/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0444\n",
            "Epoch 244/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0443\n",
            "Epoch 245/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0440\n",
            "Epoch 246/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0439\n",
            "Epoch 247/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0436\n",
            "Epoch 248/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0435\n",
            "Epoch 249/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0432\n",
            "Epoch 250/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0430\n",
            "Epoch 251/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0429\n",
            "Epoch 252/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0429\n",
            "Epoch 253/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0429\n",
            "Epoch 254/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0429\n",
            "Epoch 255/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0428\n",
            "Epoch 256/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0427\n",
            "Epoch 257/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0427\n",
            "Epoch 258/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0426\n",
            "Epoch 259/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0428\n",
            "Epoch 260/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0432\n",
            "Epoch 261/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0439\n",
            "Epoch 262/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0446\n",
            "Epoch 263/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0437\n",
            "Epoch 264/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0433\n",
            "Epoch 265/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0431\n",
            "Epoch 266/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0429\n",
            "Epoch 267/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0431\n",
            "Epoch 268/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0429\n",
            "Epoch 269/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0430\n",
            "Epoch 270/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0429\n",
            "Epoch 271/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0431\n",
            "Epoch 272/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0427\n",
            "Epoch 273/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0425\n",
            "Epoch 274/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0424\n",
            "Epoch 275/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0427\n",
            "Epoch 276/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0427\n",
            "Epoch 277/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0430\n",
            "Epoch 278/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0430\n",
            "Epoch 279/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0423\n",
            "Epoch 280/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0425\n",
            "Epoch 281/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0426\n",
            "Epoch 282/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0422\n",
            "Epoch 283/400\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0422\n",
            "Epoch 284/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0426\n",
            "Epoch 285/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0424\n",
            "Epoch 286/400\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0427\n",
            "Epoch 287/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0427\n",
            "Epoch 288/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0423\n",
            "Epoch 289/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0424\n",
            "Epoch 290/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0428\n",
            "Epoch 291/400\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 0.0427\n",
            "Epoch 292/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0427\n",
            "Epoch 293/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0426\n",
            "Epoch 294/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0424\n",
            "Epoch 295/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0425\n",
            "Epoch 296/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0423\n",
            "Epoch 297/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0423\n",
            "Epoch 298/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0422\n",
            "Epoch 299/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0427\n",
            "Epoch 300/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0425\n",
            "Epoch 301/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0423\n",
            "Epoch 302/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0423\n",
            "Epoch 303/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0426\n",
            "Epoch 304/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0426\n",
            "Epoch 305/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0431\n",
            "Epoch 306/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0430\n",
            "Epoch 307/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0427\n",
            "Epoch 308/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0422\n",
            "Epoch 309/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0417\n",
            "Epoch 310/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0413\n",
            "Epoch 311/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0412\n",
            "Epoch 312/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0412\n",
            "Epoch 313/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0414\n",
            "Epoch 314/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0413\n",
            "Epoch 315/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0413\n",
            "Epoch 316/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0412\n",
            "Epoch 317/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0411\n",
            "Epoch 318/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0409\n",
            "Epoch 319/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0411\n",
            "Epoch 320/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0419\n",
            "Epoch 321/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0426\n",
            "Epoch 322/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0424\n",
            "Epoch 323/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0437\n",
            "Epoch 324/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0447\n",
            "Epoch 325/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0562\n",
            "Epoch 326/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0637\n",
            "Epoch 327/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0524\n",
            "Epoch 328/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0470\n",
            "Epoch 329/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0446\n",
            "Epoch 330/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0428\n",
            "Epoch 331/400\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0415\n",
            "Epoch 332/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0405\n",
            "Epoch 333/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0399\n",
            "Epoch 334/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0394\n",
            "Epoch 335/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0392\n",
            "Epoch 336/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0391\n",
            "Epoch 337/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0390\n",
            "Epoch 338/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0389\n",
            "Epoch 339/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0388\n",
            "Epoch 340/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0387\n",
            "Epoch 341/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0387\n",
            "Epoch 342/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0387\n",
            "Epoch 343/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0386\n",
            "Epoch 344/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0386\n",
            "Epoch 345/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0386\n",
            "Epoch 346/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0386\n",
            "Epoch 347/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0386\n",
            "Epoch 348/400\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0385\n",
            "Epoch 349/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0384\n",
            "Epoch 350/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0384\n",
            "Epoch 351/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0384\n",
            "Epoch 352/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0384\n",
            "Epoch 353/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0384\n",
            "Epoch 354/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0383\n",
            "Epoch 355/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0382\n",
            "Epoch 356/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0382\n",
            "Epoch 357/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0382\n",
            "Epoch 358/400\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0382\n",
            "Epoch 359/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0382\n",
            "Epoch 360/400\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0381\n",
            "Epoch 361/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0381\n",
            "Epoch 362/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0381\n",
            "Epoch 363/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0382\n",
            "Epoch 364/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0382\n",
            "Epoch 365/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0382\n",
            "Epoch 366/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0382\n",
            "Epoch 367/400\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0382\n",
            "Epoch 368/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0381\n",
            "Epoch 369/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0381\n",
            "Epoch 370/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0381\n",
            "Epoch 371/400\n",
            "11/11 [==============================] - 1s 70ms/step - loss: 0.0380\n",
            "Epoch 372/400\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.0380\n",
            "Epoch 373/400\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 0.0380\n",
            "Epoch 374/400\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.0381\n",
            "Epoch 375/400\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0384\n",
            "Epoch 376/400\n",
            "11/11 [==============================] - 1s 111ms/step - loss: 0.0394\n",
            "Epoch 377/400\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 0.0394\n",
            "Epoch 378/400\n",
            "11/11 [==============================] - 1s 107ms/step - loss: 0.0389\n",
            "Epoch 379/400\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.0396\n",
            "Epoch 380/400\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0390\n",
            "Epoch 381/400\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0384\n",
            "Epoch 382/400\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 0.0383\n",
            "Epoch 383/400\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.0383\n",
            "Epoch 384/400\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.0383\n",
            "Epoch 385/400\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 0.0387\n",
            "Epoch 386/400\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.0384\n",
            "Epoch 387/400\n",
            "11/11 [==============================] - 1s 93ms/step - loss: 0.0380\n",
            "Epoch 388/400\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0375\n",
            "Epoch 389/400\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 0.0374\n",
            "Epoch 390/400\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.0373\n",
            "Epoch 391/400\n",
            "11/11 [==============================] - 1s 109ms/step - loss: 0.0373\n",
            "Epoch 392/400\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 0.0372\n",
            "Epoch 393/400\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.0373\n",
            "Epoch 394/400\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.0373\n",
            "Epoch 395/400\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 0.0372\n",
            "Epoch 396/400\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.0371\n",
            "Epoch 397/400\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.0371\n",
            "Epoch 398/400\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0371\n",
            "Epoch 399/400\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0373\n",
            "Epoch 400/400\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0cce2a750>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yPred = model.predict(X)"
      ],
      "metadata": {
        "id": "6jbAfdVyzPaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a visualizar los valores reales de *DELTA* frente a los valores estimados por nuestra red. Como se puede ver, la red consigue dar con la forma del consumo, aunque la variabilidad de los valores estimados es menor que la de los reales."
      ],
      "metadata": {
        "id": "IKusb73IHucx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counter = 0\n",
        "day = 4\n",
        "\n",
        "plt.plot(y[:, counter, day])\n",
        "plt.plot(yPred[:, counter, day])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "138t-QAPw_EE",
        "outputId": "2b148c86-1574-4165-e6f1-595f47f73936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yPred.shape"
      ],
      "metadata": {
        "id": "Ow7fk8mcye-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b4539d-b5d6-49a3-8b1a-cb7feff8368f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(349, 9, 2747)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yPred[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJaLIZMbYBbt",
        "outputId": "1dc188f5-e4d7-4083-e771-a42fe78d9209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4270809 , 0.05270596, 0.0183109 , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.42759502, 0.06829808, 0.01579954, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.41112015, 0.07621522, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.4390845 , 0.07006988, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [2.8882945 , 0.59995675, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [6.347964  , 1.0437386 , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a rotar los resultados, de forma que tengan el formato requerido por el problema:\n",
        "\n",
        "Constará de 2.747 filas con las siguientes columnas:\n",
        "* Dia_1: Predicción para el primer día\n",
        "* Dia_2: Predicción para el segundo día\n",
        "* Dia_3: Predicción para el tercer día\n",
        "* Dia_4: Predicción para el cuarto día\n",
        "* Dia_5: Predicción para el quinto día\n",
        "* Dia_6: Predicción para el sexto día\n",
        "* Dia_7: Predicción para el séptimo día\n",
        "* Semana_1: Predicción para la primera semana\n",
        "* Semana_2: Predicción para la segunda semana"
      ],
      "metadata": {
        "id": "NNE5zB5kIIO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yPredRot = np.rot90(yPred, 1, (1,2))\n",
        "yPredRot = np.flip(yPredRot, 1)"
      ],
      "metadata": {
        "id": "MNAKV0xMZY-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yPredRot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTT5oQGZ68j",
        "outputId": "b29a6100-6b3e-4447-b399-d025bce8633e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(349, 2747, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema nos pide estimar los valores de *DELTA* para las dos primeras semanas de febrero de 2020. Para ellos vamos a predecir estos valores a partir del último día de los datos proporcionados, dicho día es el 31 de enero."
      ],
      "metadata": {
        "id": "m5-3P_XdJGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lastDay = X[-1]\n",
        "lastDay.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3VgA-b0az8K",
        "outputId": "b190fc06-c41d-4ee4-a91a-a2d6b94a8e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2747)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yEntrega = model.predict(np.array([lastDay,]))"
      ],
      "metadata": {
        "id": "UVRoRlTkZ8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GetSubmissionResults(model, X):\n",
        "    lastDay = X[-1]\n",
        "    yPred = model.predict(np.array([lastDay,]))\n",
        "\n",
        "    yOrig = scaler.inverse_transform(yPred[0])\n",
        "\n",
        "    yOrigRot = np.rot90(yOrig)\n",
        "    yOrigRot = np.flip(yOrigRot, 0)\n",
        "\n",
        "    return yOrigRot"
      ],
      "metadata": {
        "id": "IZ_WCpWPfMJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetSubmissionResults(model, X)"
      ],
      "metadata": {
        "id": "VKdGApH4ycfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para buscar los mejores hiperparámetros aplicables a nuestra red, hemos realizado un grid-search, donde probamos diferentes combinaciones de hiperparámetros:\n",
        "\n",
        "* Loopback\n",
        "* Número de capas LSTM\n",
        "* Número de capas densas\n",
        "* Utilización de Dropout\n",
        "* Función de activación\n",
        "* Learning rate"
      ],
      "metadata": {
        "id": "rHWlkTrQKDW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import TruePositives\n",
        "from pickle import TRUE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "\n",
        "def create_model(lookback=0,\n",
        "                 n_features=0,\n",
        "                 n_steps=0,\n",
        "                 n_series=0,\n",
        "                 lstm_layers=(),\n",
        "                 dense_layers=(),\n",
        "                 use_dropout=False,\n",
        "                 activation_func=\"sigmoid\",\n",
        "                 optimizer=Adam,\n",
        "                 lr=0.01):\n",
        "    assert lookback!=0 or n_features!=0 or n_steps!=0 or n_series!=0\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    model = Sequential()\n",
        "        \n",
        "    # LSTM Layers\n",
        "    for i in range(len(lstm_layers)):  \n",
        "        return_sequences = (i < len(lstm_layers)-1) \n",
        "             \n",
        "        if i == 0:\n",
        "            model.add(LSTM(lstm_layers[i],\n",
        "                           activation=activation_func,\n",
        "                           input_shape=(lookback, n_features),\n",
        "                           return_sequences=return_sequences))\n",
        "        else:\n",
        "            model.add(LSTM(units=lstm_layers[i],\n",
        "                           activation=activation_func,\n",
        "                           return_sequences=return_sequences))\n",
        "\n",
        "        if use_dropout:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(RepeatVector(n_steps))\n",
        "                    \n",
        "    # Dense layers\n",
        "    for i in range(len(dense_layers)):        \n",
        "        model.add(TimeDistributed(Dense(dense_layers[i],\n",
        "                                        activation=activation_func)))\n",
        "        \n",
        "        if use_dropout:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(TimeDistributed(Dense(n_series, activation=\"relu\")))\n",
        "\n",
        "    \n",
        "    model.compile(optimizer=optimizer(learning_rate=lr), loss=\"mse\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9S_LHkjHgbR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "param_grid = dict(\n",
        "    lookback=[3, 7, 14],\n",
        "    lstm_layers=[\n",
        "        (200,),\n",
        "        (64, 64), (200, 200),\n",
        "        (400, 200, 100),\n",
        "        (200,100,50,10)\n",
        "    ],\n",
        "    dense_layers=[\n",
        "        (),\n",
        "        (64,),\n",
        "    ],\n",
        "    activation_func=[\"relu\", \"tanh\"],\n",
        "    use_dropout = [True, False],\n",
        "    optimizer = [Adam],\n",
        "    lr = [0.01]\n",
        ")\n",
        "\n",
        "ts = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "output_dir = f\"{ROOT_PATH}/results/{ts}\"\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "colnames=[\"lookback\", \"lstm_layers\", \"dense_layers\", \"use_dropout\", \"activation_func\", \"optimizer\", \"mse\"]\n",
        "results_grid = []\n",
        "\n",
        "combinations = list(ParameterGrid(param_grid))\n",
        "print(f\"Testing {len(combinations)} combinations\")\n",
        "\n",
        "for i, params in enumerate(combinations):\n",
        "    X, y = split_sequences(data_np_scaled, params[\"lookback\"])\n",
        "\n",
        "    lookback = X.shape[1]\n",
        "    n_features = X.shape[2]\n",
        "    n_steps = y.shape[1]\n",
        "    n_series = y.shape[2]\n",
        "    lstm_layers = params[\"lstm_layers\"]\n",
        "    dense_layers = params[\"dense_layers\"]\n",
        "    use_dropout = params[\"use_dropout\"]\n",
        "    activation_func = params[\"activation_func\"]\n",
        "    optimizer = params[\"optimizer\"]\n",
        "\n",
        "    print(f\"Exec {i}/{len(combinations)}:\\n{params}\")\n",
        "\n",
        "    model = create_model(lookback=lookback,\n",
        "                         n_features=n_features,\n",
        "                         n_steps=n_steps,\n",
        "                         n_series=n_series,\n",
        "                         lstm_layers=lstm_layers,\n",
        "                         dense_layers=dense_layers,\n",
        "                         use_dropout=use_dropout,\n",
        "                         activation_func=activation_func,\n",
        "                         optimizer=optimizer)\n",
        "    \n",
        "    model.fit(X, y, epochs=200, verbose=0)\n",
        "    mse = model.evaluate(X, y, batch_size=1)\n",
        "    print(f\"Exec {i}/{len(combinations)} MSE: {mse}\") \n",
        "\n",
        "    results_grid.append([lookback, lstm_layers, dense_layers, use_dropout, activation_func, optimizer, mse])\n",
        "    results_interm_df = pd.DataFrame(results_grid, columns=colnames)\n",
        "    results_interm_df.to_csv(f\"{output_dir}/results_{i}_of_{len(combinations)}.txt\")\n",
        "     \n",
        "results_df = pd.DataFrame(results_grid, columns=colnames)\n",
        "results_df.to_csv(f\"{output_dir}/results_all.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7XRqyLEkMgJ",
        "outputId": "6c23c77a-c8d6-4aad-f5c5-abdc47db4aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 120 combinations\n",
            "Exec 0/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 1s 3ms/step - loss: 1.5366\n",
            "Exec 0/120 MSE: 1.536622166633606\n",
            "Exec 1/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 1s 3ms/step - loss: 1.8804\n",
            "Exec 1/120 MSE: 1.8804408311843872\n",
            "Exec 2/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.3979\n",
            "Exec 2/120 MSE: 1.3978854417800903\n",
            "Exec 3/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.5493\n",
            "Exec 3/120 MSE: 1.549278974533081\n",
            "Exec 4/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4395\n",
            "Exec 4/120 MSE: 1.439455270767212\n",
            "Exec 5/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.5222\n",
            "Exec 5/120 MSE: 1.5222033262252808\n",
            "Exec 6/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.5648\n",
            "Exec 6/120 MSE: 1.5647892951965332\n",
            "Exec 7/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.8420\n",
            "Exec 7/120 MSE: 1.8419722318649292\n",
            "Exec 8/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.4234\n",
            "Exec 8/120 MSE: 1.4234116077423096\n",
            "Exec 9/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.8501\n",
            "Exec 9/120 MSE: 1.8500778675079346\n",
            "Exec 10/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 1.6750\n",
            "Exec 10/120 MSE: 1.6750415563583374\n",
            "Exec 11/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 2.0939\n",
            "Exec 11/120 MSE: 2.093871831893921\n",
            "Exec 12/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 6ms/step - loss: 1.7438\n",
            "Exec 12/120 MSE: 1.7438156604766846\n",
            "Exec 13/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 6ms/step - loss: 1.9267\n",
            "Exec 13/120 MSE: 1.9267425537109375\n",
            "Exec 14/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 5ms/step - loss: 2.0907\n",
            "Exec 14/120 MSE: 2.0906898975372314\n",
            "Exec 15/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 6ms/step - loss: 2.0939\n",
            "Exec 15/120 MSE: 2.093871831893921\n",
            "Exec 16/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 8ms/step - loss: 2.0939\n",
            "Exec 16/120 MSE: 2.093871831893921\n",
            "Exec 17/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 4s 9ms/step - loss: 2.0939\n",
            "Exec 17/120 MSE: 2.093871831893921\n",
            "Exec 18/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 4s 11ms/step - loss: 1.8837\n",
            "Exec 18/120 MSE: 1.8837149143218994\n",
            "Exec 19/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 5s 11ms/step - loss: 1.9546\n",
            "Exec 19/120 MSE: 1.9545632600784302\n",
            "Exec 20/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 2s 6ms/step - loss: 2.0961\n",
            "Exec 20/120 MSE: 2.096082925796509\n",
            "Exec 21/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 3s 7ms/step - loss: 2.0968\n",
            "Exec 21/120 MSE: 2.0968329906463623\n",
            "Exec 22/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 11ms/step - loss: 2.0968\n",
            "Exec 22/120 MSE: 2.0968329906463623\n",
            "Exec 23/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 11ms/step - loss: 2.0968\n",
            "Exec 23/120 MSE: 2.0968329906463623\n",
            "Exec 24/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 10ms/step - loss: 2.0968\n",
            "Exec 24/120 MSE: 2.0968329906463623\n",
            "Exec 25/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 10ms/step - loss: 2.0968\n",
            "Exec 25/120 MSE: 2.0968329906463623\n",
            "Exec 26/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 6s 15ms/step - loss: 2.0968\n",
            "Exec 26/120 MSE: 2.0968329906463623\n",
            "Exec 27/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 5s 14ms/step - loss: 2.0968\n",
            "Exec 27/120 MSE: 2.0968329906463623\n",
            "Exec 28/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 7s 18ms/step - loss: 2.0968\n",
            "Exec 28/120 MSE: 2.0968329906463623\n",
            "Exec 29/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 7s 18ms/step - loss: 2.0882\n",
            "Exec 29/120 MSE: 2.0882201194763184\n",
            "Exec 30/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 1s 3ms/step - loss: 1.4165\n",
            "Exec 30/120 MSE: 1.416481375694275\n",
            "Exec 31/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 1s 3ms/step - loss: 1.4642\n",
            "Exec 31/120 MSE: 1.4642298221588135\n",
            "Exec 32/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.4068\n",
            "Exec 32/120 MSE: 1.4068342447280884\n",
            "Exec 33/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.4940\n",
            "Exec 33/120 MSE: 1.4940121173858643\n",
            "Exec 34/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4344\n",
            "Exec 34/120 MSE: 1.4343748092651367\n",
            "Exec 35/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4886\n",
            "Exec 35/120 MSE: 1.4886095523834229\n",
            "Exec 36/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 6ms/step - loss: 1.4883\n",
            "Exec 36/120 MSE: 1.4882800579071045\n",
            "Exec 37/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 2s 6ms/step - loss: 1.8143\n",
            "Exec 37/120 MSE: 1.8143070936203003\n",
            "Exec 38/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 3s 7ms/step - loss: 1.4271\n",
            "Exec 38/120 MSE: 1.427073359489441\n",
            "Exec 39/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "349/349 [==============================] - 3s 7ms/step - loss: 1.7743\n",
            "Exec 39/120 MSE: 1.7742855548858643\n",
            "Exec 40/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 5ms/step - loss: 2.0939\n",
            "Exec 40/120 MSE: 2.093871831893921\n",
            "Exec 41/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 2s 5ms/step - loss: 1.9790\n",
            "Exec 41/120 MSE: 1.9790198802947998\n",
            "Exec 42/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 7ms/step - loss: 2.0939\n",
            "Exec 42/120 MSE: 2.093871831893921\n",
            "Exec 43/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 7ms/step - loss: 1.4989\n",
            "Exec 43/120 MSE: 1.4988789558410645\n",
            "Exec 44/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 6ms/step - loss: 1.8477\n",
            "Exec 44/120 MSE: 1.8476790189743042\n",
            "Exec 45/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 3s 7ms/step - loss: 2.0939\n",
            "Exec 45/120 MSE: 2.093871831893921\n",
            "Exec 46/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 4s 9ms/step - loss: 2.0939\n",
            "Exec 46/120 MSE: 2.093871831893921\n",
            "Exec 47/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 4s 9ms/step - loss: 2.0939\n",
            "Exec 47/120 MSE: 2.093871831893921\n",
            "Exec 48/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 5s 11ms/step - loss: 1.8764\n",
            "Exec 48/120 MSE: 1.8763854503631592\n",
            "Exec 49/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "345/345 [==============================] - 5s 12ms/step - loss: 1.4910\n",
            "Exec 49/120 MSE: 1.4910328388214111\n",
            "Exec 50/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 3s 7ms/step - loss: 2.0968\n",
            "Exec 50/120 MSE: 2.0968329906463623\n",
            "Exec 51/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 3s 7ms/step - loss: 2.0968\n",
            "Exec 51/120 MSE: 2.0968329906463623\n",
            "Exec 52/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 12ms/step - loss: 2.0968\n",
            "Exec 52/120 MSE: 2.0968329906463623\n",
            "Exec 53/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 12ms/step - loss: 2.0968\n",
            "Exec 53/120 MSE: 2.0968329906463623\n",
            "Exec 54/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 10ms/step - loss: 2.0968\n",
            "Exec 54/120 MSE: 2.0968329906463623\n",
            "Exec 55/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 4s 11ms/step - loss: 2.0968\n",
            "Exec 55/120 MSE: 2.0968329906463623\n",
            "Exec 56/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 6s 15ms/step - loss: 2.0968\n",
            "Exec 56/120 MSE: 2.0968329906463623\n",
            "Exec 57/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 6s 16ms/step - loss: 2.0968\n",
            "Exec 57/120 MSE: 2.0968329906463623\n",
            "Exec 58/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "WARNING:tensorflow:Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 7s 19ms/step - loss: 2.0967\n",
            "Exec 58/120 MSE: 2.096686363220215\n",
            "Exec 59/120:\n",
            "{'activation_func': 'relu', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "WARNING:tensorflow:Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_580 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "338/338 [==============================] - 8s 20ms/step - loss: 2.0968\n",
            "Exec 59/120 MSE: 2.0968329906463623\n",
            "Exec 60/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.5402\n",
            "Exec 60/120 MSE: 1.5402345657348633\n",
            "Exec 61/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 2s 3ms/step - loss: 1.5961\n",
            "Exec 61/120 MSE: 1.5961151123046875\n",
            "Exec 62/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4737\n",
            "Exec 62/120 MSE: 1.4736560583114624\n",
            "Exec 63/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4578\n",
            "Exec 63/120 MSE: 1.4578115940093994\n",
            "Exec 64/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 3s 5ms/step - loss: 1.4546\n",
            "Exec 64/120 MSE: 1.454561471939087\n",
            "Exec 65/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 3s 5ms/step - loss: 1.5429\n",
            "Exec 65/120 MSE: 1.5428905487060547\n",
            "Exec 66/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 4s 7ms/step - loss: 1.4204\n",
            "Exec 66/120 MSE: 1.4203535318374634\n",
            "Exec 67/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 4s 7ms/step - loss: 1.5212\n",
            "Exec 67/120 MSE: 1.521249532699585\n",
            "Exec 68/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 4s 6ms/step - loss: 1.4290\n",
            "Exec 68/120 MSE: 1.4290244579315186\n",
            "Exec 69/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 4s 6ms/step - loss: 1.5455\n",
            "Exec 69/120 MSE: 1.5454614162445068\n",
            "Exec 70/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 1.5583\n",
            "Exec 70/120 MSE: 1.5583405494689941\n",
            "Exec 71/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 1.6314\n",
            "Exec 71/120 MSE: 1.6313707828521729\n",
            "Exec 72/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.4505\n",
            "Exec 72/120 MSE: 1.450505256652832\n",
            "Exec 73/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.4582\n",
            "Exec 73/120 MSE: 1.4581910371780396\n",
            "Exec 74/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.4941\n",
            "Exec 74/120 MSE: 1.4941319227218628\n",
            "Exec 75/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 2s 5ms/step - loss: 1.6018\n",
            "Exec 75/120 MSE: 1.6018004417419434\n",
            "Exec 76/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 4s 7ms/step - loss: 1.4767\n",
            "Exec 76/120 MSE: 1.4767143726348877\n",
            "Exec 77/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 4s 8ms/step - loss: 1.5401\n",
            "Exec 77/120 MSE: 1.5401334762573242\n",
            "Exec 78/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 5s 7ms/step - loss: 1.4701\n",
            "Exec 78/120 MSE: 1.4700517654418945\n",
            "Exec 79/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 4s 7ms/step - loss: 1.5638\n",
            "Exec 79/120 MSE: 1.563765287399292\n",
            "Exec 80/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 2s 4ms/step - loss: 1.5740\n",
            "Exec 80/120 MSE: 1.5740060806274414\n",
            "Exec 81/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 2s 4ms/step - loss: 1.5974\n",
            "Exec 81/120 MSE: 1.5973570346832275\n",
            "Exec 82/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 3s 5ms/step - loss: 1.4473\n",
            "Exec 82/120 MSE: 1.447308897972107\n",
            "Exec 83/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 3s 5ms/step - loss: 1.4874\n",
            "Exec 83/120 MSE: 1.4873719215393066\n",
            "Exec 84/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 4s 6ms/step - loss: 1.4653\n",
            "Exec 84/120 MSE: 1.4653215408325195\n",
            "Exec 85/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 3s 5ms/step - loss: 1.5935\n",
            "Exec 85/120 MSE: 1.5934919118881226\n",
            "Exec 86/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 5s 10ms/step - loss: 1.4777\n",
            "Exec 86/120 MSE: 1.4776973724365234\n",
            "Exec 87/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 5s 10ms/step - loss: 1.5824\n",
            "Exec 87/120 MSE: 1.5823640823364258\n",
            "Exec 88/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 5s 9ms/step - loss: 1.4724\n",
            "Exec 88/120 MSE: 1.4724279642105103\n",
            "Exec 89/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 5s 9ms/step - loss: 1.5279\n",
            "Exec 89/120 MSE: 1.5278629064559937\n",
            "Exec 90/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.4830\n",
            "Exec 90/120 MSE: 1.483010172843933\n",
            "Exec 91/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.5192\n",
            "Exec 91/120 MSE: 1.519179344177246\n",
            "Exec 92/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 2s 5ms/step - loss: 1.4130\n",
            "Exec 92/120 MSE: 1.4130374193191528\n",
            "Exec 93/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 2s 4ms/step - loss: 1.5190\n",
            "Exec 93/120 MSE: 1.5190351009368896\n",
            "Exec 94/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 3s 5ms/step - loss: 1.4839\n",
            "Exec 94/120 MSE: 1.4839295148849487\n",
            "Exec 95/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 3s 5ms/step - loss: 1.5536\n",
            "Exec 95/120 MSE: 1.5535584688186646\n",
            "Exec 96/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 4s 8ms/step - loss: 1.4838\n",
            "Exec 96/120 MSE: 1.4837815761566162\n",
            "Exec 97/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 4s 7ms/step - loss: 1.5281\n",
            "Exec 97/120 MSE: 1.52809476852417\n",
            "Exec 98/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "349/349 [==============================] - 4s 6ms/step - loss: 1.4689\n",
            "Exec 98/120 MSE: 1.4689031839370728\n",
            "Exec 99/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 3, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "349/349 [==============================] - 5s 6ms/step - loss: 1.4798\n",
            "Exec 99/120 MSE: 1.4798412322998047\n",
            "Exec 100/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 1.4868\n",
            "Exec 100/120 MSE: 1.4867764711380005\n",
            "Exec 101/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 2s 4ms/step - loss: 1.5346\n",
            "Exec 101/120 MSE: 1.5346229076385498\n",
            "Exec 102/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.4698\n",
            "Exec 102/120 MSE: 1.4698097705841064\n",
            "Exec 103/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.5291\n",
            "Exec 103/120 MSE: 1.5291045904159546\n",
            "Exec 104/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.4865\n",
            "Exec 104/120 MSE: 1.4865124225616455\n",
            "Exec 105/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 3s 5ms/step - loss: 1.5779\n",
            "Exec 105/120 MSE: 1.5778651237487793\n",
            "Exec 106/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 4s 8ms/step - loss: 1.4858\n",
            "Exec 106/120 MSE: 1.4857608079910278\n",
            "Exec 107/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 4s 7ms/step - loss: 1.5336\n",
            "Exec 107/120 MSE: 1.5336387157440186\n",
            "Exec 108/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "345/345 [==============================] - 5s 8ms/step - loss: 1.4692\n",
            "Exec 108/120 MSE: 1.4691652059555054\n",
            "Exec 109/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 7, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "345/345 [==============================] - 5s 8ms/step - loss: 1.4710\n",
            "Exec 109/120 MSE: 1.4709683656692505\n",
            "Exec 110/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 2s 5ms/step - loss: 1.4631\n",
            "Exec 110/120 MSE: 1.463104248046875\n",
            "Exec 111/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200,), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 2s 5ms/step - loss: 1.5475\n",
            "Exec 111/120 MSE: 1.5474568605422974\n",
            "Exec 112/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 3s 6ms/step - loss: 1.4727\n",
            "Exec 112/120 MSE: 1.472681999206543\n",
            "Exec 113/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (64, 64), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 3s 6ms/step - loss: 1.4339\n",
            "Exec 113/120 MSE: 1.4338858127593994\n",
            "Exec 114/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 3s 6ms/step - loss: 1.4842\n",
            "Exec 114/120 MSE: 1.4842157363891602\n",
            "Exec 115/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 200), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 3s 5ms/step - loss: 1.5553\n",
            "Exec 115/120 MSE: 1.5553269386291504\n",
            "Exec 116/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 5s 9ms/step - loss: 1.4861\n",
            "Exec 116/120 MSE: 1.486073613166809\n",
            "Exec 117/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (400, 200, 100), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 4s 8ms/step - loss: 1.5371\n",
            "Exec 117/120 MSE: 1.5371448993682861\n",
            "Exec 118/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': True}\n",
            "338/338 [==============================] - 5s 9ms/step - loss: 1.4708\n",
            "Exec 118/120 MSE: 1.4707748889923096\n",
            "Exec 119/120:\n",
            "{'activation_func': 'tanh', 'dense_layers': (64,), 'lookback': 14, 'lr': 0.01, 'lstm_layers': (200, 100, 50, 10), 'optimizer': <class 'keras.optimizer_v2.adam.Adam'>, 'use_dropout': False}\n",
            "338/338 [==============================] - 5s 9ms/step - loss: 1.5208\n",
            "Exec 119/120 MSE: 1.520775556564331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el dataframe con los resultados del grid-search"
      ],
      "metadata": {
        "id": "inW6_EPRK72E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results_df = pd.read_csv(f\"{ROOT_PATH}/results/19-03-2022-21:37:08/results_all.txt\")\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "BzwoZ7hxobtJ",
        "outputId": "f7052c72-8290-4c8f-98ba-a4919df20e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0  lookback         lstm_layers dense_layers  use_dropout  \\\n",
              "0             0         3              (200,)           ()         True   \n",
              "1             1         3              (200,)           ()        False   \n",
              "2             2         3            (64, 64)           ()         True   \n",
              "3             3         3            (64, 64)           ()        False   \n",
              "4             4         3          (200, 200)           ()         True   \n",
              "..          ...       ...                 ...          ...          ...   \n",
              "115         115        14          (200, 200)        (64,)        False   \n",
              "116         116        14     (400, 200, 100)        (64,)         True   \n",
              "117         117        14     (400, 200, 100)        (64,)        False   \n",
              "118         118        14  (200, 100, 50, 10)        (64,)         True   \n",
              "119         119        14  (200, 100, 50, 10)        (64,)        False   \n",
              "\n",
              "    activation_func                               optimizer       mse  \n",
              "0              relu  <class 'keras.optimizer_v2.adam.Adam'>  1.536622  \n",
              "1              relu  <class 'keras.optimizer_v2.adam.Adam'>  1.880441  \n",
              "2              relu  <class 'keras.optimizer_v2.adam.Adam'>  1.397885  \n",
              "3              relu  <class 'keras.optimizer_v2.adam.Adam'>  1.549279  \n",
              "4              relu  <class 'keras.optimizer_v2.adam.Adam'>  1.439455  \n",
              "..              ...                                     ...       ...  \n",
              "115            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.555327  \n",
              "116            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.486074  \n",
              "117            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.537145  \n",
              "118            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.470775  \n",
              "119            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.520776  \n",
              "\n",
              "[120 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aaa24153-c094-4431-a497-50207b381403\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>lookback</th>\n",
              "      <th>lstm_layers</th>\n",
              "      <th>dense_layers</th>\n",
              "      <th>use_dropout</th>\n",
              "      <th>activation_func</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>mse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>(200,)</td>\n",
              "      <td>()</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.536622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>(200,)</td>\n",
              "      <td>()</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.880441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>()</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.397885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>()</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.549279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>(200, 200)</td>\n",
              "      <td>()</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.439455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>115</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 200)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.555327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>116</td>\n",
              "      <td>14</td>\n",
              "      <td>(400, 200, 100)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.486074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>117</td>\n",
              "      <td>14</td>\n",
              "      <td>(400, 200, 100)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.537145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>118</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 100, 50, 10)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.470775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>119</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 100, 50, 10)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.520776</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaa24153-c094-4431-a497-50207b381403')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aaa24153-c094-4431-a497-50207b381403 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aaa24153-c094-4431-a497-50207b381403');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y ordenamos por el valor, ascendente, de MSE."
      ],
      "metadata": {
        "id": "HqNZFlpsK_t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.sort_values(\"mse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QMu2Oheps_ad",
        "outputId": "45975402-1293-47e6-83b3-cc3ad1eb623a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0  lookback         lstm_layers dense_layers  use_dropout  \\\n",
              "2            2         3            (64, 64)           ()         True   \n",
              "32          32         3            (64, 64)        (64,)         True   \n",
              "92          92         3            (64, 64)        (64,)         True   \n",
              "30          30         3              (200,)        (64,)         True   \n",
              "66          66         3     (400, 200, 100)           ()         True   \n",
              "..         ...       ...                 ...          ...          ...   \n",
              "53          53        14            (64, 64)        (64,)        False   \n",
              "54          54        14          (200, 200)        (64,)         True   \n",
              "55          55        14          (200, 200)        (64,)        False   \n",
              "21          21        14              (200,)           ()        False   \n",
              "59          59        14  (200, 100, 50, 10)        (64,)        False   \n",
              "\n",
              "   activation_func                               optimizer       mse  \n",
              "2             relu  <class 'keras.optimizer_v2.adam.Adam'>  1.397885  \n",
              "32            relu  <class 'keras.optimizer_v2.adam.Adam'>  1.406834  \n",
              "92            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.413037  \n",
              "30            relu  <class 'keras.optimizer_v2.adam.Adam'>  1.416481  \n",
              "66            tanh  <class 'keras.optimizer_v2.adam.Adam'>  1.420354  \n",
              "..             ...                                     ...       ...  \n",
              "53            relu  <class 'keras.optimizer_v2.adam.Adam'>  2.096833  \n",
              "54            relu  <class 'keras.optimizer_v2.adam.Adam'>  2.096833  \n",
              "55            relu  <class 'keras.optimizer_v2.adam.Adam'>  2.096833  \n",
              "21            relu  <class 'keras.optimizer_v2.adam.Adam'>  2.096833  \n",
              "59            relu  <class 'keras.optimizer_v2.adam.Adam'>  2.096833  \n",
              "\n",
              "[120 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ae00fd2-9af4-4f99-8cba-837f58545341\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>lookback</th>\n",
              "      <th>lstm_layers</th>\n",
              "      <th>dense_layers</th>\n",
              "      <th>use_dropout</th>\n",
              "      <th>activation_func</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>mse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>()</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.397885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.406834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.413037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>(200,)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.416481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>3</td>\n",
              "      <td>(400, 200, 100)</td>\n",
              "      <td>()</td>\n",
              "      <td>True</td>\n",
              "      <td>tanh</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>1.420354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>53</td>\n",
              "      <td>14</td>\n",
              "      <td>(64, 64)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>2.096833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 200)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>True</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>2.096833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 200)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>2.096833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>14</td>\n",
              "      <td>(200,)</td>\n",
              "      <td>()</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>2.096833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>14</td>\n",
              "      <td>(200, 100, 50, 10)</td>\n",
              "      <td>(64,)</td>\n",
              "      <td>False</td>\n",
              "      <td>relu</td>\n",
              "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>2.096833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ae00fd2-9af4-4f99-8cba-837f58545341')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ae00fd2-9af4-4f99-8cba-837f58545341 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ae00fd2-9af4-4f99-8cba-837f58545341');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos comprobar, la red que mejores resultados ha obtenido tiene los siguientes hiperparámetros:\n",
        "\n",
        "* Loopback = 3\n",
        "* 2 capas LSTM con 64 neuronas cada una\n",
        "* Ninguna capa densa\n",
        "* Utiliza dropout\n",
        "* Función de activación relu\n",
        "* Optmizador Adam\n",
        "* Learning rate = 0.01\n",
        "\n",
        "En un primer momento, hemos vuelto a entrenar esta red neuronal, pero nos ha parecido conveniente probar a realizar otro modelo con una tercera capa LSTM de 64 neuronas, y ha mejorado el resultado obtenido anteriormente. También hemos aumentado el número de épocas."
      ],
      "metadata": {
        "id": "SxwAcPh7LG6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = split_sequences(data_np_scaled, 3)\n",
        "\n",
        "lookback = X.shape[1]\n",
        "n_features = X.shape[2]\n",
        "n_steps = y.shape[1]\n",
        "n_series = y.shape[2]\n",
        "\n",
        "final_model = create_model(lookback=lookback,\n",
        "                           n_features=n_features,\n",
        "                           n_steps=n_steps,\n",
        "                           n_series=n_series,\n",
        "                           lstm_layers=(64, 64, 64),\n",
        "                           dense_layers=(),\n",
        "                           use_dropout=True,\n",
        "                           activation_func=\"relu\",\n",
        "                           optimizer=Adam)\n",
        "    \n",
        "final_model.fit(X, y, epochs=400, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEoR1WlUtPfg",
        "outputId": "ebd4787c-3c2e-4d61-9800-0a5f7726b1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "11/11 [==============================] - 5s 36ms/step - loss: 2.1016\n",
            "Epoch 2/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.5843\n",
            "Epoch 3/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.5239\n",
            "Epoch 4/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4999\n",
            "Epoch 5/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4961\n",
            "Epoch 6/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4899\n",
            "Epoch 7/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4863\n",
            "Epoch 8/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4795\n",
            "Epoch 9/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4768\n",
            "Epoch 10/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4706\n",
            "Epoch 11/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4655\n",
            "Epoch 12/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4646\n",
            "Epoch 13/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4608\n",
            "Epoch 14/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4607\n",
            "Epoch 15/400\n",
            "11/11 [==============================] - 0s 33ms/step - loss: 1.4558\n",
            "Epoch 16/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4538\n",
            "Epoch 17/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4504\n",
            "Epoch 18/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4470\n",
            "Epoch 19/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4454\n",
            "Epoch 20/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4468\n",
            "Epoch 21/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4444\n",
            "Epoch 22/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4414\n",
            "Epoch 23/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4419\n",
            "Epoch 24/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4401\n",
            "Epoch 25/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4395\n",
            "Epoch 26/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4399\n",
            "Epoch 27/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4390\n",
            "Epoch 28/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4373\n",
            "Epoch 29/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4352\n",
            "Epoch 30/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4357\n",
            "Epoch 31/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4358\n",
            "Epoch 32/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4344\n",
            "Epoch 33/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4350\n",
            "Epoch 34/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4327\n",
            "Epoch 35/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4324\n",
            "Epoch 36/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4310\n",
            "Epoch 37/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4298\n",
            "Epoch 38/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4311\n",
            "Epoch 39/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4286\n",
            "Epoch 40/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4287\n",
            "Epoch 41/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4285\n",
            "Epoch 42/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4269\n",
            "Epoch 43/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4275\n",
            "Epoch 44/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4256\n",
            "Epoch 45/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4276\n",
            "Epoch 46/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4284\n",
            "Epoch 47/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4260\n",
            "Epoch 48/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4269\n",
            "Epoch 49/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4239\n",
            "Epoch 50/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4238\n",
            "Epoch 51/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4235\n",
            "Epoch 52/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4215\n",
            "Epoch 53/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4213\n",
            "Epoch 54/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4205\n",
            "Epoch 55/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4211\n",
            "Epoch 56/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4207\n",
            "Epoch 57/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4194\n",
            "Epoch 58/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4194\n",
            "Epoch 59/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4196\n",
            "Epoch 60/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4191\n",
            "Epoch 61/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4178\n",
            "Epoch 62/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4195\n",
            "Epoch 63/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4197\n",
            "Epoch 64/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4190\n",
            "Epoch 65/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4182\n",
            "Epoch 66/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4188\n",
            "Epoch 67/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4176\n",
            "Epoch 68/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4176\n",
            "Epoch 69/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4164\n",
            "Epoch 70/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4159\n",
            "Epoch 71/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4163\n",
            "Epoch 72/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4163\n",
            "Epoch 73/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4153\n",
            "Epoch 74/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4151\n",
            "Epoch 75/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4136\n",
            "Epoch 76/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4142\n",
            "Epoch 77/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4151\n",
            "Epoch 78/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4131\n",
            "Epoch 79/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4148\n",
            "Epoch 80/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4138\n",
            "Epoch 81/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4135\n",
            "Epoch 82/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4132\n",
            "Epoch 83/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4131\n",
            "Epoch 84/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4133\n",
            "Epoch 85/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4131\n",
            "Epoch 86/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4127\n",
            "Epoch 87/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4116\n",
            "Epoch 88/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4118\n",
            "Epoch 89/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4105\n",
            "Epoch 90/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4102\n",
            "Epoch 91/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4101\n",
            "Epoch 92/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4112\n",
            "Epoch 93/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4106\n",
            "Epoch 94/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4089\n",
            "Epoch 95/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4108\n",
            "Epoch 96/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4104\n",
            "Epoch 97/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4100\n",
            "Epoch 98/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4111\n",
            "Epoch 99/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4089\n",
            "Epoch 100/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4102\n",
            "Epoch 101/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4098\n",
            "Epoch 102/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4105\n",
            "Epoch 103/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4081\n",
            "Epoch 104/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4088\n",
            "Epoch 105/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4105\n",
            "Epoch 106/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4103\n",
            "Epoch 107/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4093\n",
            "Epoch 108/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4097\n",
            "Epoch 109/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4109\n",
            "Epoch 110/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4090\n",
            "Epoch 111/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4088\n",
            "Epoch 112/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4084\n",
            "Epoch 113/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4075\n",
            "Epoch 114/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4069\n",
            "Epoch 115/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4086\n",
            "Epoch 116/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4063\n",
            "Epoch 117/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4070\n",
            "Epoch 118/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4077\n",
            "Epoch 119/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4077\n",
            "Epoch 120/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4067\n",
            "Epoch 121/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4060\n",
            "Epoch 122/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4068\n",
            "Epoch 123/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4056\n",
            "Epoch 124/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4054\n",
            "Epoch 125/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4066\n",
            "Epoch 126/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4053\n",
            "Epoch 127/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4069\n",
            "Epoch 128/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4073\n",
            "Epoch 129/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4072\n",
            "Epoch 130/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4059\n",
            "Epoch 131/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4039\n",
            "Epoch 132/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4049\n",
            "Epoch 133/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4056\n",
            "Epoch 134/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4060\n",
            "Epoch 135/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.4049\n",
            "Epoch 136/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4044\n",
            "Epoch 137/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4055\n",
            "Epoch 138/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4037\n",
            "Epoch 139/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4056\n",
            "Epoch 140/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4044\n",
            "Epoch 141/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4061\n",
            "Epoch 142/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4100\n",
            "Epoch 143/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4078\n",
            "Epoch 144/400\n",
            "11/11 [==============================] - 0s 33ms/step - loss: 1.4046\n",
            "Epoch 145/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4079\n",
            "Epoch 146/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4041\n",
            "Epoch 147/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4057\n",
            "Epoch 148/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4035\n",
            "Epoch 149/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4033\n",
            "Epoch 150/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4053\n",
            "Epoch 151/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4025\n",
            "Epoch 152/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4020\n",
            "Epoch 153/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4018\n",
            "Epoch 154/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4026\n",
            "Epoch 155/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4038\n",
            "Epoch 156/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4027\n",
            "Epoch 157/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4027\n",
            "Epoch 158/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4031\n",
            "Epoch 159/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4030\n",
            "Epoch 160/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4033\n",
            "Epoch 161/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4015\n",
            "Epoch 162/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4005\n",
            "Epoch 163/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4024\n",
            "Epoch 164/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4019\n",
            "Epoch 165/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4010\n",
            "Epoch 166/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4019\n",
            "Epoch 167/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4025\n",
            "Epoch 168/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4027\n",
            "Epoch 169/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4030\n",
            "Epoch 170/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4019\n",
            "Epoch 171/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4014\n",
            "Epoch 172/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4015\n",
            "Epoch 173/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4006\n",
            "Epoch 174/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4007\n",
            "Epoch 175/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4006\n",
            "Epoch 176/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4018\n",
            "Epoch 177/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3991\n",
            "Epoch 178/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4017\n",
            "Epoch 179/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4011\n",
            "Epoch 180/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4010\n",
            "Epoch 181/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3997\n",
            "Epoch 182/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4004\n",
            "Epoch 183/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3998\n",
            "Epoch 184/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4007\n",
            "Epoch 185/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4007\n",
            "Epoch 186/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4015\n",
            "Epoch 187/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4021\n",
            "Epoch 188/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4020\n",
            "Epoch 189/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4006\n",
            "Epoch 190/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4024\n",
            "Epoch 191/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4015\n",
            "Epoch 192/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.4010\n",
            "Epoch 193/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.4001\n",
            "Epoch 194/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4006\n",
            "Epoch 195/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4000\n",
            "Epoch 196/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3997\n",
            "Epoch 197/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3995\n",
            "Epoch 198/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3999\n",
            "Epoch 199/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4007\n",
            "Epoch 200/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3999\n",
            "Epoch 201/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4005\n",
            "Epoch 202/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3989\n",
            "Epoch 203/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3998\n",
            "Epoch 204/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3999\n",
            "Epoch 205/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3986\n",
            "Epoch 206/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3996\n",
            "Epoch 207/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4002\n",
            "Epoch 208/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3997\n",
            "Epoch 209/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3986\n",
            "Epoch 210/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3985\n",
            "Epoch 211/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3981\n",
            "Epoch 212/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3979\n",
            "Epoch 213/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3989\n",
            "Epoch 214/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3977\n",
            "Epoch 215/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3982\n",
            "Epoch 216/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3978\n",
            "Epoch 217/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3988\n",
            "Epoch 218/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4013\n",
            "Epoch 219/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4024\n",
            "Epoch 220/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3998\n",
            "Epoch 221/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4001\n",
            "Epoch 222/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4029\n",
            "Epoch 223/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.4033\n",
            "Epoch 224/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4004\n",
            "Epoch 225/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.4008\n",
            "Epoch 226/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3997\n",
            "Epoch 227/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3985\n",
            "Epoch 228/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3982\n",
            "Epoch 229/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3987\n",
            "Epoch 230/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3982\n",
            "Epoch 231/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3992\n",
            "Epoch 232/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3998\n",
            "Epoch 233/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3986\n",
            "Epoch 234/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3992\n",
            "Epoch 235/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3982\n",
            "Epoch 236/400\n",
            "11/11 [==============================] - 0s 33ms/step - loss: 1.3980\n",
            "Epoch 237/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3977\n",
            "Epoch 238/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3976\n",
            "Epoch 239/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3979\n",
            "Epoch 240/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3987\n",
            "Epoch 241/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3967\n",
            "Epoch 242/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3984\n",
            "Epoch 243/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3976\n",
            "Epoch 244/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3977\n",
            "Epoch 245/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3981\n",
            "Epoch 246/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3981\n",
            "Epoch 247/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3973\n",
            "Epoch 248/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3982\n",
            "Epoch 249/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3981\n",
            "Epoch 250/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3973\n",
            "Epoch 251/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3966\n",
            "Epoch 252/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3958\n",
            "Epoch 253/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3965\n",
            "Epoch 254/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3978\n",
            "Epoch 255/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3979\n",
            "Epoch 256/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3970\n",
            "Epoch 257/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3968\n",
            "Epoch 258/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3963\n",
            "Epoch 259/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3979\n",
            "Epoch 260/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3961\n",
            "Epoch 261/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3988\n",
            "Epoch 262/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3984\n",
            "Epoch 263/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3975\n",
            "Epoch 264/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3967\n",
            "Epoch 265/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3972\n",
            "Epoch 266/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3975\n",
            "Epoch 267/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3981\n",
            "Epoch 268/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3975\n",
            "Epoch 269/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3961\n",
            "Epoch 270/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3955\n",
            "Epoch 271/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3972\n",
            "Epoch 272/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3953\n",
            "Epoch 273/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3956\n",
            "Epoch 274/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3965\n",
            "Epoch 275/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3970\n",
            "Epoch 276/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3962\n",
            "Epoch 277/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3968\n",
            "Epoch 278/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3980\n",
            "Epoch 279/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3971\n",
            "Epoch 280/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3968\n",
            "Epoch 281/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3970\n",
            "Epoch 282/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3965\n",
            "Epoch 283/400\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.3976\n",
            "Epoch 284/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3957\n",
            "Epoch 285/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3981\n",
            "Epoch 286/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3963\n",
            "Epoch 287/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3950\n",
            "Epoch 288/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3972\n",
            "Epoch 289/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3966\n",
            "Epoch 290/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3964\n",
            "Epoch 291/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3946\n",
            "Epoch 292/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3946\n",
            "Epoch 293/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3948\n",
            "Epoch 294/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3952\n",
            "Epoch 295/400\n",
            "11/11 [==============================] - 0s 33ms/step - loss: 1.3949\n",
            "Epoch 296/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3945\n",
            "Epoch 297/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3971\n",
            "Epoch 298/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3977\n",
            "Epoch 299/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3967\n",
            "Epoch 300/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3970\n",
            "Epoch 301/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3977\n",
            "Epoch 302/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3972\n",
            "Epoch 303/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3983\n",
            "Epoch 304/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3963\n",
            "Epoch 305/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3951\n",
            "Epoch 306/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3954\n",
            "Epoch 307/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3959\n",
            "Epoch 308/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3974\n",
            "Epoch 309/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3972\n",
            "Epoch 310/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3967\n",
            "Epoch 311/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3968\n",
            "Epoch 312/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3968\n",
            "Epoch 313/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3976\n",
            "Epoch 314/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3984\n",
            "Epoch 315/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3973\n",
            "Epoch 316/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3963\n",
            "Epoch 317/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3960\n",
            "Epoch 318/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3953\n",
            "Epoch 319/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3973\n",
            "Epoch 320/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3966\n",
            "Epoch 321/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3967\n",
            "Epoch 322/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3981\n",
            "Epoch 323/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3987\n",
            "Epoch 324/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3981\n",
            "Epoch 325/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3967\n",
            "Epoch 326/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3962\n",
            "Epoch 327/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3950\n",
            "Epoch 328/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3952\n",
            "Epoch 329/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3959\n",
            "Epoch 330/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3949\n",
            "Epoch 331/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3944\n",
            "Epoch 332/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3951\n",
            "Epoch 333/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3955\n",
            "Epoch 334/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3954\n",
            "Epoch 335/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3946\n",
            "Epoch 336/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3947\n",
            "Epoch 337/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3957\n",
            "Epoch 338/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3984\n",
            "Epoch 339/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3957\n",
            "Epoch 340/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3969\n",
            "Epoch 341/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3971\n",
            "Epoch 342/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3979\n",
            "Epoch 343/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3972\n",
            "Epoch 344/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3961\n",
            "Epoch 345/400\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.3966\n",
            "Epoch 346/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3954\n",
            "Epoch 347/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3942\n",
            "Epoch 348/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3941\n",
            "Epoch 349/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3961\n",
            "Epoch 350/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3953\n",
            "Epoch 351/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3957\n",
            "Epoch 352/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3964\n",
            "Epoch 353/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3972\n",
            "Epoch 354/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3987\n",
            "Epoch 355/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3975\n",
            "Epoch 356/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3964\n",
            "Epoch 357/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3952\n",
            "Epoch 358/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3954\n",
            "Epoch 359/400\n",
            "11/11 [==============================] - 0s 34ms/step - loss: 1.3948\n",
            "Epoch 360/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3933\n",
            "Epoch 361/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3939\n",
            "Epoch 362/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3943\n",
            "Epoch 363/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3946\n",
            "Epoch 364/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3944\n",
            "Epoch 365/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3950\n",
            "Epoch 366/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3948\n",
            "Epoch 367/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3947\n",
            "Epoch 368/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3947\n",
            "Epoch 369/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3934\n",
            "Epoch 370/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3963\n",
            "Epoch 371/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3967\n",
            "Epoch 372/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3959\n",
            "Epoch 373/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3958\n",
            "Epoch 374/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3939\n",
            "Epoch 375/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3940\n",
            "Epoch 376/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3944\n",
            "Epoch 377/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3952\n",
            "Epoch 378/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3990\n",
            "Epoch 379/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3971\n",
            "Epoch 380/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3958\n",
            "Epoch 381/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3952\n",
            "Epoch 382/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3955\n",
            "Epoch 383/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3964\n",
            "Epoch 384/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3953\n",
            "Epoch 385/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3939\n",
            "Epoch 386/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3957\n",
            "Epoch 387/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3959\n",
            "Epoch 388/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3954\n",
            "Epoch 389/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3955\n",
            "Epoch 390/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3954\n",
            "Epoch 391/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3944\n",
            "Epoch 392/400\n",
            "11/11 [==============================] - 0s 37ms/step - loss: 1.3956\n",
            "Epoch 393/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3948\n",
            "Epoch 394/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3952\n",
            "Epoch 395/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3949\n",
            "Epoch 396/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3941\n",
            "Epoch 397/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3945\n",
            "Epoch 398/400\n",
            "11/11 [==============================] - 0s 36ms/step - loss: 1.3947\n",
            "Epoch 399/400\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 1.3946\n",
            "Epoch 400/400\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6108eb6250>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = final_model.evaluate(X, y, batch_size=1)\n",
        "print(f\"MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlmvW3y6uBXH",
        "outputId": "fb60f996-16a0-4ec3-d5af-f1465d590131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "349/349 [==============================] - 2s 3ms/step - loss: 1.3958\n",
            "MSE: 1.3958357572555542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este modelo, vamos a obtener la predicción de consumo de las dos primeras semanas de febrero:"
      ],
      "metadata": {
        "id": "yolXpSxOMKIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ySubmission = GetSubmissionResults(final_model, X)"
      ],
      "metadata": {
        "id": "l2R1fGosud-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ySubmission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "femKZejHyjK2",
        "outputId": "a6ade328-be98-4710-c6ab-dd27dff779cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[893.4806 , 893.4806 , 893.4806 , ..., 893.4806 , 893.4806 ,\n",
              "        893.4806 ],\n",
              "       [ 40.84889,  40.84889,  40.84889, ...,  40.84889,  40.84889,\n",
              "         40.84889],\n",
              "       [100.7303 , 100.7303 , 100.7303 , ..., 100.7303 , 100.7303 ,\n",
              "        100.7303 ],\n",
              "       ...,\n",
              "       [  0.     ,   0.     ,   0.     , ...,   0.     ,   0.     ,\n",
              "          0.     ],\n",
              "       [  0.     ,   0.     ,   0.     , ...,   0.     ,   0.     ,\n",
              "          0.     ],\n",
              "       [  0.     ,   0.     ,   0.     , ...,   0.     ,   0.     ,\n",
              "          0.     ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ySubmission.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvJ4BUg9y_iZ",
        "outputId": "f9d9eba0-da34-44ec-bc95-5ba88f670558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2747, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS760owNykAz",
        "outputId": "2b9bb81e-ee60-4966-8cc5-c7bb0bb49b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2748"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, añadimos el nombre de las columnas, y el *ID* de los contadores."
      ],
      "metadata": {
        "id": "5CM2DWRsMUcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ySubmissionDF = pd.DataFrame(ySubmission, columns=[\"Dia_1\", \"Dia_2\", \"Dia_3\", \"Dia_4\", \"Dia_5\", \"Dia_6\", \"Dia_7\", \"Semana_1\", \"Semana_2\"])\n",
        "ySubmissionDF = ySubmissionDF.set_index(data.columns[1:])\n",
        "ySubmissionDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a_Nu4OVmzlyf",
        "outputId": "39d03d2d-b1e9-4786-e7ca-8f8d1f71f456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Dia_1        Dia_2        Dia_3        Dia_4        Dia_5  \\\n",
              "0      893.480591   893.480591   893.480591   893.480591   893.480591   \n",
              "1       40.848888    40.848888    40.848888    40.848888    40.848888   \n",
              "2      100.730301   100.730301   100.730301   100.730301   100.730301   \n",
              "3     1007.130127  1007.130127  1007.130127  1007.130127  1007.130127   \n",
              "4      930.518005   930.518005   930.518005   930.518005   930.518005   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "2746     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "2747     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "2748     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "2749     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "2756     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "\n",
              "            Dia_6        Dia_7     Semana_1     Semana_2  \n",
              "0      893.480591   893.480591   893.480591   893.480591  \n",
              "1       40.848888    40.848888    40.848888    40.848888  \n",
              "2      100.730301   100.730301   100.730301   100.730301  \n",
              "3     1007.130127  1007.130127  1007.130127  1007.130127  \n",
              "4      930.518005   930.518005   930.518005   930.518005  \n",
              "...           ...          ...          ...          ...  \n",
              "2746     0.000000     0.000000     0.000000     0.000000  \n",
              "2747     0.000000     0.000000     0.000000     0.000000  \n",
              "2748     0.000000     0.000000     0.000000     0.000000  \n",
              "2749     0.000000     0.000000     0.000000     0.000000  \n",
              "2756     0.000000     0.000000     0.000000     0.000000  \n",
              "\n",
              "[2747 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7cd85294-92fd-4a28-a4c1-727d9338af49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dia_1</th>\n",
              "      <th>Dia_2</th>\n",
              "      <th>Dia_3</th>\n",
              "      <th>Dia_4</th>\n",
              "      <th>Dia_5</th>\n",
              "      <th>Dia_6</th>\n",
              "      <th>Dia_7</th>\n",
              "      <th>Semana_1</th>\n",
              "      <th>Semana_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "      <td>893.480591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "      <td>40.848888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "      <td>100.730301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "      <td>1007.130127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "      <td>930.518005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2746</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2747</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2748</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2749</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2756</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2747 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cd85294-92fd-4a28-a4c1-727d9338af49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7cd85294-92fd-4a28-a4c1-727d9338af49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7cd85294-92fd-4a28-a4c1-727d9338af49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y lo guardamos en el archivo final con los resultados de nuestro experimento."
      ],
      "metadata": {
        "id": "wiJ5wDnmMgOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "ts = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
        "\n",
        "output_dir = Path(f\"{ROOT_PATH}/submissions/{ts}\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ySubmissionDF.to_csv(f\"{output_dir}/Lando_UH2022.txt\", sep=\"|\", header=False)"
      ],
      "metadata": {
        "id": "GofvQAwW0D8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Original.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}